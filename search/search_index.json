{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"COVID-19 Pandemic: impact on UBELIX operations The University of Bern switched to emergency operations (minimal presence). The HPC team put in all effort to slow the spread of COVID-19 by working remotly. The UBELIX system stays in production and the HPC team do their best to continue the services, even if problems may take longer to solve. For any questions or issue please write to: hpc@id.unibe.ch The UniBE HPC team would like to contribute to the research in the fight against COVID-19. Any related studies may get special UBELIX priorities as well as special software support on request ( hpc@id.unibe.ch ). Welcome to the High Performance Computing (HPC) documentation of the University of Bern Introduction Official documentation site of the high performance computing and the HPC cluster UBELIX. Currently, the UBELIX cluster runs about 330 compute nodes featuring almost 6\u2018300 CPU cores and about 300\u2018000 GPU cores. The infrastructure is available to all University personnel for their scientific work. The cluster can also be used by students within a scope of a thesis or a course. If your campus account is not yet activated for UBELIX, the Getting Started Guide might be a good place to get you started. UBELIX features a plethora of software and applications, which is outlined on the page Software, but the users are free to compile and install their own software within their home directories. If you are wondering\u2026 UBELIX is an acronym and stands for U niversity of Be rn Li nu x Cluster (Naming similarities to known Gauls are purely coincidental and not intended in any way). Job Monitoroing See what is currently running on UBELIX on the Job Monitoring pages. Acknowledging UBELIX When you present results generated using our cluster UBELIX, we kindly ask you to acknowledge the usage of the cluster. We would also highly appreciate if you could send us a copy of your papers, posters and presentations mentioning the UBELIX cluster. Public visibility of our cluster and documenting results are important for us to ensure long-term funding of UBELIX. Whenever the UBELIX infrastructure has been used to produce results used in a publication or poster, we kindly request citing the service in the acknowledgements: \"Calculations were performed on UBELIX (http://www.id.unibe.ch/hpc), the HPC cluster at the University of Bern.\" Press Kit Occasionally we are asked for images like diagrams illustrating the structure of UBELIX or even pictures of machines/storage or the like. Often this is due to the need to describe UBELIX within a research proposal. To support you with this, we provide a short text about UBELIX and some files to download. You can use all of this within your research proposal. COMING SOON IN 2020","title":"Home"},{"location":"index.html#welcome-to-the-high-performance-computing-hpc-documentation-of-the-university-of-bern","text":"","title":"Welcome to the High Performance Computing (HPC) documentation of the University of Bern"},{"location":"index.html#introduction","text":"Official documentation site of the high performance computing and the HPC cluster UBELIX. Currently, the UBELIX cluster runs about 330 compute nodes featuring almost 6\u2018300 CPU cores and about 300\u2018000 GPU cores. The infrastructure is available to all University personnel for their scientific work. The cluster can also be used by students within a scope of a thesis or a course. If your campus account is not yet activated for UBELIX, the Getting Started Guide might be a good place to get you started. UBELIX features a plethora of software and applications, which is outlined on the page Software, but the users are free to compile and install their own software within their home directories. If you are wondering\u2026 UBELIX is an acronym and stands for U niversity of Be rn Li nu x Cluster (Naming similarities to known Gauls are purely coincidental and not intended in any way). Job Monitoroing See what is currently running on UBELIX on the Job Monitoring pages.","title":"Introduction"},{"location":"index.html#acknowledging-ubelix","text":"When you present results generated using our cluster UBELIX, we kindly ask you to acknowledge the usage of the cluster. We would also highly appreciate if you could send us a copy of your papers, posters and presentations mentioning the UBELIX cluster. Public visibility of our cluster and documenting results are important for us to ensure long-term funding of UBELIX. Whenever the UBELIX infrastructure has been used to produce results used in a publication or poster, we kindly request citing the service in the acknowledgements: \"Calculations were performed on UBELIX (http://www.id.unibe.ch/hpc), the HPC cluster at the University of Bern.\"","title":"Acknowledging UBELIX"},{"location":"index.html#press-kit","text":"Occasionally we are asked for images like diagrams illustrating the structure of UBELIX or even pictures of machines/storage or the like. Often this is due to the need to describe UBELIX within a research proposal. To support you with this, we provide a short text about UBELIX and some files to download. You can use all of this within your research proposal. COMING SOON IN 2020","title":"Press Kit"},{"location":"code-of-conduct.html","text":"Code of Conduct On this page we list some expectations from our side and recommended practice that is crucial for maintaining a good and professional working relationship between the user and the system administrators. Most of those contents are quite self-explanatory while others help to reduce the amount of support time needed to allocate. General We assume that you are familiar with some basic knowledge about Linux command line (shell) navigation and shell scripting. If you never worked on the command line, consider some Linux tutorials on the subject first. We expect you to exploit this valuable documentation before asking for help. All that is needed to get some simple jobs done on UBELIX is documented here. Account Staff accounts are preferred over student accounts! If you currently use your student Campus Account to access UBELIX, but you also possess a staff Campus Account, get in contact with us so we can activate your staff Campus Account, migrate all your user data to the new account and deactivate your student account for UBELIX. Mailing List We communicate upcoming events (e.g. maintenance downtimes) on our mailing list. Make sure that you are subscribed to this list, otherwise you will miss important announcements. Security Do not share your account If using public key authentication, do not share your private key General Communication with the Cluster Administrators Use the support email address hpc@id.unibe.ch for question regarding UBELIX. Do not use the personal email address of a cluster administrator. This is important because it keeps all administrators informed about the ongoing problem-solving process, and if one administrator is on vacation, another administrator can help you with your question For each new problem start a new conversation with a new subject. Avoid to write to us by replying to an old answer mail from the last problem that you received from us or even worse by replying to mailing list email you received from us. The point here is that though it looks like an ordinary email, you actually are opening a new ticket in our ticket system (or reopening an old ticket if replying to an old email). State your Campus Account username when communicating with the administrators. This helps us to identify you on the cluster faster. Use your official UniBE email address for the communication with the cluster administrators, not any *@hotmail.com addresses or the like. Problem-Solving Process Exploit resources provided by your institute/research group before asking the UBELIX staff about domain-specific problems. We make an effort to help you, but we are no experts in your field, hence a colleague from your group who uses the cluster to solve a similar problem like you do might be a better first contact Ask Google for help before contacting us. We often also just \u201cgoogle\u201d for an answer, and then forward the outcome to you. Do not ask for/expect step-by-step solutions to a problem. Sometimes we give step-by-step instructions, but generally you should use our answers to do some refined research on the problem. If you still stuck, we are happy to provide further support Always give an exact as possible description of the problem. Provide your username, error messages, the path to the job script, the id of the job, and other hints that make the problem-solving process as economic as possible. Housekeeping Clean up your home directory frequently, in particular before asking for an increase of your quota limit Do not save thousands of files in a single directory. Distribute the files to subdirectories Job Submission Before submitting the same job a hundred times, please verify that the job finishes successfully. We often experience that hundreds of jobs getting killed due to an invalid path referenced in the job script, which generates hundreds of notification mails in our system. Cluster Performance DO NOT run resource-intensive computations directly on the front-end server (submit host). This will have a negative impact on the performance of the whole cluster. Instead, submit such computations as a job to the cluster DO NOT run server applications (PostgreSQL server, web server, \u2026) on the front-end server (submit host). Such a program usually run as a background process (daemon) rather than being under the direct control of an interactive user. We will immediately kill such processes","title":"Code of Conduct"},{"location":"code-of-conduct.html#code-of-conduct","text":"On this page we list some expectations from our side and recommended practice that is crucial for maintaining a good and professional working relationship between the user and the system administrators. Most of those contents are quite self-explanatory while others help to reduce the amount of support time needed to allocate.","title":"Code of Conduct"},{"location":"code-of-conduct.html#general","text":"We assume that you are familiar with some basic knowledge about Linux command line (shell) navigation and shell scripting. If you never worked on the command line, consider some Linux tutorials on the subject first. We expect you to exploit this valuable documentation before asking for help. All that is needed to get some simple jobs done on UBELIX is documented here.","title":"General"},{"location":"code-of-conduct.html#account","text":"Staff accounts are preferred over student accounts! If you currently use your student Campus Account to access UBELIX, but you also possess a staff Campus Account, get in contact with us so we can activate your staff Campus Account, migrate all your user data to the new account and deactivate your student account for UBELIX.","title":"Account"},{"location":"code-of-conduct.html#mailing-list","text":"We communicate upcoming events (e.g. maintenance downtimes) on our mailing list. Make sure that you are subscribed to this list, otherwise you will miss important announcements.","title":"Mailing List"},{"location":"code-of-conduct.html#security","text":"Do not share your account If using public key authentication, do not share your private key","title":"Security"},{"location":"code-of-conduct.html#general-communication-with-the-cluster-administrators","text":"Use the support email address hpc@id.unibe.ch for question regarding UBELIX. Do not use the personal email address of a cluster administrator. This is important because it keeps all administrators informed about the ongoing problem-solving process, and if one administrator is on vacation, another administrator can help you with your question For each new problem start a new conversation with a new subject. Avoid to write to us by replying to an old answer mail from the last problem that you received from us or even worse by replying to mailing list email you received from us. The point here is that though it looks like an ordinary email, you actually are opening a new ticket in our ticket system (or reopening an old ticket if replying to an old email). State your Campus Account username when communicating with the administrators. This helps us to identify you on the cluster faster. Use your official UniBE email address for the communication with the cluster administrators, not any *@hotmail.com addresses or the like.","title":"General Communication with the Cluster Administrators"},{"location":"code-of-conduct.html#problem-solving-process","text":"Exploit resources provided by your institute/research group before asking the UBELIX staff about domain-specific problems. We make an effort to help you, but we are no experts in your field, hence a colleague from your group who uses the cluster to solve a similar problem like you do might be a better first contact Ask Google for help before contacting us. We often also just \u201cgoogle\u201d for an answer, and then forward the outcome to you. Do not ask for/expect step-by-step solutions to a problem. Sometimes we give step-by-step instructions, but generally you should use our answers to do some refined research on the problem. If you still stuck, we are happy to provide further support Always give an exact as possible description of the problem. Provide your username, error messages, the path to the job script, the id of the job, and other hints that make the problem-solving process as economic as possible.","title":"Problem-Solving Process"},{"location":"code-of-conduct.html#housekeeping","text":"Clean up your home directory frequently, in particular before asking for an increase of your quota limit Do not save thousands of files in a single directory. Distribute the files to subdirectories","title":"Housekeeping"},{"location":"code-of-conduct.html#job-submission","text":"Before submitting the same job a hundred times, please verify that the job finishes successfully. We often experience that hundreds of jobs getting killed due to an invalid path referenced in the job script, which generates hundreds of notification mails in our system.","title":"Job Submission"},{"location":"code-of-conduct.html#cluster-performance","text":"DO NOT run resource-intensive computations directly on the front-end server (submit host). This will have a negative impact on the performance of the whole cluster. Instead, submit such computations as a job to the cluster DO NOT run server applications (PostgreSQL server, web server, \u2026) on the front-end server (submit host). Such a program usually run as a background process (daemon) rather than being under the direct control of an interactive user. We will immediately kill such processes","title":"Cluster Performance"},{"location":"contributing.html","text":"Contributing You can support the UBELIX cluster in different ways: Investments Documentation Improvements Investments Some text about money\u2026 Documentation Improvements Some text aboutn contirbuting to the user guide.","title":"Contributing"},{"location":"contributing.html#contributing","text":"You can support the UBELIX cluster in different ways: Investments Documentation Improvements","title":"Contributing"},{"location":"contributing.html#investments","text":"Some text about money\u2026","title":"Investments"},{"location":"contributing.html#documentation-improvements","text":"Some text aboutn contirbuting to the user guide.","title":"Documentation Improvements"},{"location":"halloffame.html","text":"Hall of Fame If you previously used UBELIX to do your computational work and you acknowledged this in your publication and want to your publication listed here, please drop us a note at hpc@id.unibe.ch. If you are wondering how you can acknowledge the usage of UBELIX in your publication, have a look at the homepage of this documentation, where you will find a text recommendation acknoowledging the use of our cluster. Papers and Articles Authors Title Journal Boris DOI 2020 Riou J, Althaus C Pattern of early human-to-human transmission of Wuhan 2019 novel coronavirus (2019-nCoV), December 2019 to January 2020 Euro Surveillance Direct Link 2019 Counotte M, Althaus C et al. Impact of age-specific immunity on the timing and burden of the next Zika virus outbreak PLOS NeglectedTropical Diseases Details Direct Link Brugger J, Althaus C Transmission of and susceptibility to seasonal influenza in Switzerland from 2003 to 2015 Epidemics, Elsevier Details Direct Link 2018 Horton P, Br\u00f6nnimann S Impact of global atmospheric reanalyses on statistical precipitation downscaling Climate Dynamics Details Direct Link Vonr\u00fcti N, Aschauer U Epitaxial strain dependence of band gaps in perovskite oxynitrides compared to perovskite oxides American Physical Society Details Direct Link Aschauer U Ultrafast Relaxation Dynamics of the Antiferrodistortive Phase in Ca Doped SrTiO\u2083 American Physical Society Details Direct Link Vonr\u00fcti N, Aschauer U, et al. Elucidation of Li x Ni 0.8 Co 0.15 Al 0.05 O 2 Redox Chemistry by Operando Raman Spectroscopy American Chemical Society Details Direct Link Ouhbi H, Aschauer U Water oxidation chemistry of oxynitrides and oxides: Comparing NaTaO 3 and SrTaO 2 N Surface Science Details Direct Link Aschauer U Surface and Defect Chemistry of Oxide Material CHIMIA Details Direct Link Kasper, C, Hebert, F, Aubin-Horth N, Taborsky B Divergent brain gene expression profiles between alternative behavioural helper types in a cooperative breeder Wiley Molecular Ecology Direct Link Panyasantisuk J, Dall\u2019Ara E, Pretterklieber M, Pahr D.H., Zysset P.K. Mapping anisotropy improves QCT-based finite element estimation of hip strength in pooled stance and side-fall load configurations Medical Engineering & Physics, Elsevier Direct Link Vonr\u00fcti, N, Aschauer U Anion Order and Spontaneous Polarization in LaTiO 2 N Oxynitride Thin Films American Physical Society Details Direct Link Bouri M, Aschauer U Bulk and surface properties of the Ruddlesden-Popper oxynitride Sr 2 TaO 3 N Physical Chemistry Chemical Physics Details Direct Link 2017 Aschauer, U et al. Surface Structure of TiO2 Rutile (011) Exposed to Liquid Water Journal of Physical Chemistry Details Direct Link Kasper, C, K\u00f6lliker, M, Pstma, E, Taborsky B Consistent cooperation in a cichlid fish is caused by maternal and developmental effects rather than heritable genetic variation Proceedings of the Royal Society, Biological Sciences Direct Link Riesen M, Garcia V, Low N, Althaus C Modeling the consequences of regional heterogeneity in human papillomavirus (HPV) vaccination uptake on transmission in Switzerland Vaccine, Elsevier Details Direct Link Kilic C, Raible C, Stocker T Multiple climate States of Habitable Exoplanets: The Rolf of Obliquity and Irradiance The Astrophysical Journal Details Direct Link Kilic C, Raible C, Kirk Impact of variations of gravitational acceleration on the general circulation of the planetary atmosphere Planetary and Space Science Details Direct Link Mueller S, Fix M et al. Simultaneous optimization of photons and electrons for mixed beam radiotherapy al. Physics in Medicine & Biology Direct Link Ninova S, Aschauer U Surface structure and anion order of the oynitride LaTiO 2 N Journal of Materials Chemistry A Details Direct Link Ninova S, Aschauer U et al. LaTiOxNy Thin Film Model Systems for Photocatalytic Water Splitting: Physicochemical Evolution of the Solid-Liquid Interface and the Role of the Crystallographic Orientation Advanced functional materials Details Direct Link Struchen R, Vial F, Andersson M. G. Value of evidence from syndromic surveillance with cumulative evidence from multiple data stream with delayed reporting Scientific Reports Direct Link 2013 Leichtle A, Fiedler G et al. Pancreatic carcinoma, pancreatitis, and healthy controls: metabolite models in a three-class diagnostic dilemma Metabolomics, Springer Details Direct Link Posters Newspapers Title Newspaper Year of Publication Link Berner Forscher entdecken neue Klimazust\u00e4nde, in denen Leben m\u00f6glich ist. Der Bund Direct Link","title":"Hall of Fame"},{"location":"halloffame.html#hall-of-fame","text":"If you previously used UBELIX to do your computational work and you acknowledged this in your publication and want to your publication listed here, please drop us a note at hpc@id.unibe.ch. If you are wondering how you can acknowledge the usage of UBELIX in your publication, have a look at the homepage of this documentation, where you will find a text recommendation acknoowledging the use of our cluster.","title":"Hall of Fame"},{"location":"halloffame.html#papers-and-articles","text":"Authors Title Journal Boris DOI 2020 Riou J, Althaus C Pattern of early human-to-human transmission of Wuhan 2019 novel coronavirus (2019-nCoV), December 2019 to January 2020 Euro Surveillance Direct Link 2019 Counotte M, Althaus C et al. Impact of age-specific immunity on the timing and burden of the next Zika virus outbreak PLOS NeglectedTropical Diseases Details Direct Link Brugger J, Althaus C Transmission of and susceptibility to seasonal influenza in Switzerland from 2003 to 2015 Epidemics, Elsevier Details Direct Link 2018 Horton P, Br\u00f6nnimann S Impact of global atmospheric reanalyses on statistical precipitation downscaling Climate Dynamics Details Direct Link Vonr\u00fcti N, Aschauer U Epitaxial strain dependence of band gaps in perovskite oxynitrides compared to perovskite oxides American Physical Society Details Direct Link Aschauer U Ultrafast Relaxation Dynamics of the Antiferrodistortive Phase in Ca Doped SrTiO\u2083 American Physical Society Details Direct Link Vonr\u00fcti N, Aschauer U, et al. Elucidation of Li x Ni 0.8 Co 0.15 Al 0.05 O 2 Redox Chemistry by Operando Raman Spectroscopy American Chemical Society Details Direct Link Ouhbi H, Aschauer U Water oxidation chemistry of oxynitrides and oxides: Comparing NaTaO 3 and SrTaO 2 N Surface Science Details Direct Link Aschauer U Surface and Defect Chemistry of Oxide Material CHIMIA Details Direct Link Kasper, C, Hebert, F, Aubin-Horth N, Taborsky B Divergent brain gene expression profiles between alternative behavioural helper types in a cooperative breeder Wiley Molecular Ecology Direct Link Panyasantisuk J, Dall\u2019Ara E, Pretterklieber M, Pahr D.H., Zysset P.K. Mapping anisotropy improves QCT-based finite element estimation of hip strength in pooled stance and side-fall load configurations Medical Engineering & Physics, Elsevier Direct Link Vonr\u00fcti, N, Aschauer U Anion Order and Spontaneous Polarization in LaTiO 2 N Oxynitride Thin Films American Physical Society Details Direct Link Bouri M, Aschauer U Bulk and surface properties of the Ruddlesden-Popper oxynitride Sr 2 TaO 3 N Physical Chemistry Chemical Physics Details Direct Link 2017 Aschauer, U et al. Surface Structure of TiO2 Rutile (011) Exposed to Liquid Water Journal of Physical Chemistry Details Direct Link Kasper, C, K\u00f6lliker, M, Pstma, E, Taborsky B Consistent cooperation in a cichlid fish is caused by maternal and developmental effects rather than heritable genetic variation Proceedings of the Royal Society, Biological Sciences Direct Link Riesen M, Garcia V, Low N, Althaus C Modeling the consequences of regional heterogeneity in human papillomavirus (HPV) vaccination uptake on transmission in Switzerland Vaccine, Elsevier Details Direct Link Kilic C, Raible C, Stocker T Multiple climate States of Habitable Exoplanets: The Rolf of Obliquity and Irradiance The Astrophysical Journal Details Direct Link Kilic C, Raible C, Kirk Impact of variations of gravitational acceleration on the general circulation of the planetary atmosphere Planetary and Space Science Details Direct Link Mueller S, Fix M et al. Simultaneous optimization of photons and electrons for mixed beam radiotherapy al. Physics in Medicine & Biology Direct Link Ninova S, Aschauer U Surface structure and anion order of the oynitride LaTiO 2 N Journal of Materials Chemistry A Details Direct Link Ninova S, Aschauer U et al. LaTiOxNy Thin Film Model Systems for Photocatalytic Water Splitting: Physicochemical Evolution of the Solid-Liquid Interface and the Role of the Crystallographic Orientation Advanced functional materials Details Direct Link Struchen R, Vial F, Andersson M. G. Value of evidence from syndromic surveillance with cumulative evidence from multiple data stream with delayed reporting Scientific Reports Direct Link 2013 Leichtle A, Fiedler G et al. Pancreatic carcinoma, pancreatitis, and healthy controls: metabolite models in a three-class diagnostic dilemma Metabolomics, Springer Details Direct Link","title":"Papers and Articles"},{"location":"halloffame.html#posters","text":"","title":"Posters"},{"location":"halloffame.html#newspapers","text":"Title Newspaper Year of Publication Link Berner Forscher entdecken neue Klimazust\u00e4nde, in denen Leben m\u00f6glich ist. Der Bund Direct Link","title":"Newspapers"},{"location":"mdcheat.html","text":"Markdown Cheatsheet This page outlines all stuff available by installing the base Python-Markdown (comes with MkDocs) and the additional bundle PyMdown Extensions . Markdown Cheatsheet Headings The 3rd level The 4th level The 5th level The 6th level Headings with secondary text The 3rd level with secondary text The 4th level with secondary text The 5th level with secondary text The 6th level with secondary text Emphasis Lists Links Images Code and Syntax Highlighting Tables Blockquotes Inline HTML Horizontal Rule Line Breaks YouTube Videos Admonition Abbreviations Definition Lists Footnotes Headings ### The 3rd level #### The 4th level ##### The 5th level ###### The 6th level ## Headings <small>with secondary text</small> ### The 3rd level <small>with secondary text</small> #### The 4th level <small>with secondary text</small> ##### The 5th level <small>with secondary text</small> ###### The 6th level <small>with secondary text</small> The 3rd level The 4th level The 5th level The 6th level Headings with secondary text The 3rd level with secondary text The 4th level with secondary text The 5th level with secondary text The 6th level with secondary text Emphasis Emphasis, aka italics, with *asterisks* or _underscores_. Strong emphasis, aka bold, with **asterisks** or __underscores__. Combined emphasis with **asterisks and _underscores_**. Strikethrough uses two tildes. ~~Scratch this.~~ Emphasis, aka italics, with asterisks or underscores . Strong emphasis, aka bold, with asterisks or underscores . Combined emphasis with asterisks and underscores . Strikethrough uses two tildes. ~~Scratch this.~~ Lists (In this example, leading and trailing spaces are shown with with dots: \u22c5) 1. First ordered list item 2. Another item \u22c5\u22c5\u22c5\u22c5* Unordered sub-list. \u22c5\u22c5\u22c5\u22c5* Item 2 \u22c5\u22c5\u22c5\u22c5* Item 3 1. Actual numbers don't matter, just that it's a number \u22c5\u22c5\u22c5\u22c51. Ordered sub-list \u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c51. Ordered subsub-list \u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c51. Item 2 \u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c51. Item 3 \u22c5\u22c5\u22c5\u22c51. Item 2 \u22c5\u22c5\u22c5\u22c51. Item 3 4. And another item. \u22c5\u22c5\u22c5\u22c5You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown). \u22c5\u22c5\u22c5\u22c5To have a line break without a paragraph, you will need to use two trailing spaces.\u22c5\u22c5 \u22c5\u22c5\u22c5\u22c5Note that this line is separate, but within the same paragraph.\u22c5\u22c5 \u22c5\u22c5\u22c5\u22c5(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.) * Unordered list can use asterisks - Or minuses + Or pluses First ordered list item Another item Unordered sub-list. Item 2 Item 3 Actual numbers don\u2019t matter, just that it\u2019s a number Ordered sub-list Ordered subsub-list Item 2 Item 3 Item 2 Item 3 And another item. You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we\u2019ll use three here to also align the raw Markdown). To have a line break without a paragraph, you will need to use two trailing spaces. Note that this line is separate, but within the same paragraph. (This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.) Unordered list can use asterisks Or minuses Or pluses Links There are two ways to create links. [I'm an inline-style link](https://www.google.com) [I'm an inline-style link with title](https://www.google.com \"Google's Homepage\") [I'm a reference-style link][Arbitrary case-insensitive reference text] [I'm a relative reference to a repository file](../blob/master/LICENSE) [You can use numbers for reference-style link definitions][1] Or leave it empty and use the [link text itself]. URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <http://www.example.com> and sometimes example.com (but not on Github, for example). Some text to show that the reference links can follow later. [arbitrary case-insensitive reference text]: https://www.mozilla.org [1]: http://slashdot.org [link text itself]: http://www.reddit.com I\u2019m an inline-style link I\u2019m an inline-style link with title I\u2019m a reference-style link I\u2019m a relative reference to a repository file You can use numbers for reference-style link definitions Or leave it empty and use the link text itself . URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example). Some text to show that the reference links can follow later. Images Here's our logo (hover to see the title text): Inline-style: ![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 1\") Reference-style: ![alt text][logo] [logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 2\" Here\u2019s our logo (hover to see the title text): Inline-style: Reference-style: Code and Syntax Highlighting Code blocks are part of the Markdown spec, but syntax highlighting isn\u2019t. However, many renderers \u2013 like Github\u2019s and Markdown Here \u2013 support syntax highlighting. Which languages are supported and how those language names should be written will vary from renderer to renderer. Markdown Here supports highlighting for dozens of languages (and not-really-languages, like diffs and HTTP headers); to see the complete list, and how to write the language names, see the highlight.js demo page . Inline `code` has `back-ticks around` it. Inline code has back-ticks around it. Blocks of code are either fenced by lines with three back-ticks ``` , or are indented with four spaces. I recommend only using the fenced code blocks \u2013 they\u2019re easier and only they support syntax highlighting. var s = \"JavaScript syntax highlighting\" ; alert ( s ); s = \"Python syntax highlighting\" print s No language indicated, so no syntax highlighting in Markdown Here (varies on Github). But let's throw in a <b>tag</b>. Even tabbed code example for different language are possible: Bash #!/bin/bash STR = \"Hello World!\" echo $STR C #include int main(void) { printf ( \"hello, world \\n \" ); } C++ #include <iostream> int main () { std :: cout << \"Hello, world! \\n \" ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello, world!\" ); } } Tables Tables aren\u2019t part of the core Markdown spec, but they are part of GFM and Markdown Here supports them. They are an easy way of adding tables to your email \u2013 a task that would otherwise require copy-pasting from another application. Colons can be used to align columns. | Tables | Are | Cool | | ------------- |:-------------:| -----:| | col 3 is | right-aligned | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 | There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown | Less | Pretty --- | --- | --- *Still* | `renders` | **nicely** 1 | 2 | 3 Colons can be used to align columns. Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u2019t need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown Less Pretty Still renders nicely 1 2 3 Blockquotes > Blockquotes are very handy in email to emulate reply text. > This line is part of the same quote. Quote break. > This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can *put* **Markdown** into a blockquote. Blockquotes are very handy in email to emulate reply text. This line is part of the same quote. Quote break. This is a very long line that will still be quoted properly when it wraps. Oh boy let\u2019s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote. Blockquote nesting is possible: > **Sed aliquet**, neque at rutrum mollis, neque nisi tincidunt nibh, vitae faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem [libero fermentum](#) urna, ut efficitur elit ligula et nunc. > > Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla. Ut sit amet placerat ante. Proin sed **elementum** __nulla__. Nunc vitae sem odio. Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum. eu odio. > > > `Suspendisse rutrum facilisis risus`, eu posuere neque commodo a. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo bibendum, sodales mauris ut, tincidunt massa. Sed aliquet , neque at rutrum mollis, neque nisi tincidunt nibh, vitae faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem libero fermentum urna, ut efficitur elit ligula et nunc. Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla. Ut sit amet placerat ante. Proin sed elementum nulla. Nunc vitae sem odio. Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum. eu odio. Suspendisse rutrum facilisis risus , eu posuere neque commodo a. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo bibendum, sodales mauris ut, tincidunt massa. Other content blocks within a blockquote Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; Praesent at :::js return target , sodales nibh vel, tempor felis. Fusce vel lacinia lacus. Suspendisse rhoncus nunc non nisi iaculis ultrices. Donec consectetur mauris non neque imperdiet, eget volutpat libero. Inline HTML You can also use raw HTML in your Markdown, and it\u2019ll mostly work pretty well. Horizontal Rule Three or more... --- Hyphens *** Asterisks ___ Underscores Three or more\u2026 Hyphens Asterisks Underscores Line Breaks My basic recommendation for learning how line breaks work is to experiment and discover \u2013 hit <Enter> once (i.e., insert one newline), then hit it twice (i.e., insert two newlines), see what happens. You\u2019ll soon learn to get what you want. \u201cMarkdown Toggle\u201d is your friend. Here are some things to try out: Here's a line for us to start with. This line is separated from the one above by two newlines, so it will be a *separate paragraph*. This line is also a separate paragraph, but... This line is only separated by a single newline, so it's a separate line in the *same paragraph*. Here\u2019s a line for us to start with. This line is separated from the one above by two newlines, so it will be a separate paragraph . This line is also begins a separate paragraph, but\u2026 This line is only separated by a single newline, so it\u2019s a separate line in the same paragraph . YouTube Videos They can\u2019t be added directly but you can add an image with a link to the video like this: <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=YOUTUBE_VIDEO_ID_HERE \" target=\"_blank\"><img src=\"http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg\" alt=\"IMAGE ALT TEXT HERE\" width=\"240\" height=\"180\" border=\"10\" /></a> Or, in pure Markdown, but losing the image sizing and border: [![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg)](http://www.youtube.com/watch?v=YOUTUBE_VIDEO_ID_HERE) Referencing a bug by #bugID in your git commit links it to the slip. For example #1. Admonition !!! type \"optional explicit title within double quotes\" Any number of other indented markdown elements. This is the second paragraph. Some title Any number of other indented markdown elements. This is the second paragraph. And this is outside the admonition again. If you don\u2019t want a title, use a blank string \u201c\u201d. Don\u2019t do this at home! rST suggests the following \u201ctypes\u201d: attention, caution, danger, error, hint, important, note, tip, and warning: Some title This is type note Some title This is type hint Some title This is type tip Some title This is type important Some title This is type attention Some title This is type caution Some title This is type warning Some title This is type danger Some title This is type error Abbreviations The HTML specification is maintained by the W3C. *[HTML]: Hyper Text Markup Language *[W3C]: World Wide Web Consortium The HTML specification is maintained by the W3C . Definition Lists Apple : Pomaceous fruit of plants of the genus Malus in the family Rosaceae. Orange : The fruit of an evergreen tree of the genus Citrus. Apple Pomaceous fruit of plants of the genus Malus in the family Rosaceae. Orange The fruit of an evergreen tree of the genus Citrus. Footnotes Footnotes 1 have a label 2 and the footnote\u2019s content. Another Footnote 3 License: CC-BY This is a footnote content. \u21a9 A footnote on the label: \u201c@#$%\u201d. \u21a9 Another content \u21a9","title":"Markdown Cheatsheet"},{"location":"mdcheat.html#markdown-cheatsheet","text":"This page outlines all stuff available by installing the base Python-Markdown (comes with MkDocs) and the additional bundle PyMdown Extensions . Markdown Cheatsheet Headings The 3rd level The 4th level The 5th level The 6th level Headings with secondary text The 3rd level with secondary text The 4th level with secondary text The 5th level with secondary text The 6th level with secondary text Emphasis Lists Links Images Code and Syntax Highlighting Tables Blockquotes Inline HTML Horizontal Rule Line Breaks YouTube Videos Admonition Abbreviations Definition Lists Footnotes","title":"Markdown Cheatsheet"},{"location":"mdcheat.html#headings","text":"### The 3rd level #### The 4th level ##### The 5th level ###### The 6th level ## Headings <small>with secondary text</small> ### The 3rd level <small>with secondary text</small> #### The 4th level <small>with secondary text</small> ##### The 5th level <small>with secondary text</small> ###### The 6th level <small>with secondary text</small>","title":"Headings"},{"location":"mdcheat.html#the-3rd-level","text":"","title":"The 3rd level"},{"location":"mdcheat.html#the-4th-level","text":"","title":"The 4th level"},{"location":"mdcheat.html#the-5th-level","text":"","title":"The 5th level"},{"location":"mdcheat.html#the-6th-level","text":"","title":"The 6th level"},{"location":"mdcheat.html#headings-with-secondary-text","text":"","title":"Headings with secondary text"},{"location":"mdcheat.html#the-3rd-level-with-secondary-text","text":"","title":"The 3rd level with secondary text"},{"location":"mdcheat.html#the-4th-level-with-secondary-text","text":"","title":"The 4th level with secondary text"},{"location":"mdcheat.html#the-5th-level-with-secondary-text","text":"","title":"The 5th level with secondary text"},{"location":"mdcheat.html#the-6th-level-with-secondary-text","text":"","title":"The 6th level with secondary text"},{"location":"mdcheat.html#emphasis","text":"Emphasis, aka italics, with *asterisks* or _underscores_. Strong emphasis, aka bold, with **asterisks** or __underscores__. Combined emphasis with **asterisks and _underscores_**. Strikethrough uses two tildes. ~~Scratch this.~~ Emphasis, aka italics, with asterisks or underscores . Strong emphasis, aka bold, with asterisks or underscores . Combined emphasis with asterisks and underscores . Strikethrough uses two tildes. ~~Scratch this.~~","title":"Emphasis"},{"location":"mdcheat.html#lists","text":"(In this example, leading and trailing spaces are shown with with dots: \u22c5) 1. First ordered list item 2. Another item \u22c5\u22c5\u22c5\u22c5* Unordered sub-list. \u22c5\u22c5\u22c5\u22c5* Item 2 \u22c5\u22c5\u22c5\u22c5* Item 3 1. Actual numbers don't matter, just that it's a number \u22c5\u22c5\u22c5\u22c51. Ordered sub-list \u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c51. Ordered subsub-list \u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c51. Item 2 \u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c51. Item 3 \u22c5\u22c5\u22c5\u22c51. Item 2 \u22c5\u22c5\u22c5\u22c51. Item 3 4. And another item. \u22c5\u22c5\u22c5\u22c5You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown). \u22c5\u22c5\u22c5\u22c5To have a line break without a paragraph, you will need to use two trailing spaces.\u22c5\u22c5 \u22c5\u22c5\u22c5\u22c5Note that this line is separate, but within the same paragraph.\u22c5\u22c5 \u22c5\u22c5\u22c5\u22c5(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.) * Unordered list can use asterisks - Or minuses + Or pluses First ordered list item Another item Unordered sub-list. Item 2 Item 3 Actual numbers don\u2019t matter, just that it\u2019s a number Ordered sub-list Ordered subsub-list Item 2 Item 3 Item 2 Item 3 And another item. You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we\u2019ll use three here to also align the raw Markdown). To have a line break without a paragraph, you will need to use two trailing spaces. Note that this line is separate, but within the same paragraph. (This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.) Unordered list can use asterisks Or minuses Or pluses","title":"Lists"},{"location":"mdcheat.html#links","text":"There are two ways to create links. [I'm an inline-style link](https://www.google.com) [I'm an inline-style link with title](https://www.google.com \"Google's Homepage\") [I'm a reference-style link][Arbitrary case-insensitive reference text] [I'm a relative reference to a repository file](../blob/master/LICENSE) [You can use numbers for reference-style link definitions][1] Or leave it empty and use the [link text itself]. URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <http://www.example.com> and sometimes example.com (but not on Github, for example). Some text to show that the reference links can follow later. [arbitrary case-insensitive reference text]: https://www.mozilla.org [1]: http://slashdot.org [link text itself]: http://www.reddit.com I\u2019m an inline-style link I\u2019m an inline-style link with title I\u2019m a reference-style link I\u2019m a relative reference to a repository file You can use numbers for reference-style link definitions Or leave it empty and use the link text itself . URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example). Some text to show that the reference links can follow later.","title":"Links"},{"location":"mdcheat.html#images","text":"Here's our logo (hover to see the title text): Inline-style: ![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 1\") Reference-style: ![alt text][logo] [logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 2\" Here\u2019s our logo (hover to see the title text): Inline-style: Reference-style:","title":"Images"},{"location":"mdcheat.html#code-and-syntax-highlighting","text":"Code blocks are part of the Markdown spec, but syntax highlighting isn\u2019t. However, many renderers \u2013 like Github\u2019s and Markdown Here \u2013 support syntax highlighting. Which languages are supported and how those language names should be written will vary from renderer to renderer. Markdown Here supports highlighting for dozens of languages (and not-really-languages, like diffs and HTTP headers); to see the complete list, and how to write the language names, see the highlight.js demo page . Inline `code` has `back-ticks around` it. Inline code has back-ticks around it. Blocks of code are either fenced by lines with three back-ticks ``` , or are indented with four spaces. I recommend only using the fenced code blocks \u2013 they\u2019re easier and only they support syntax highlighting. var s = \"JavaScript syntax highlighting\" ; alert ( s ); s = \"Python syntax highlighting\" print s No language indicated, so no syntax highlighting in Markdown Here (varies on Github). But let's throw in a <b>tag</b>. Even tabbed code example for different language are possible: Bash #!/bin/bash STR = \"Hello World!\" echo $STR C #include int main(void) { printf ( \"hello, world \\n \" ); } C++ #include <iostream> int main () { std :: cout << \"Hello, world! \\n \" ; return 0 ; } C# using System ; class Program { static void Main ( string [] args ) { Console . WriteLine ( \"Hello, world!\" ); } }","title":"Code and Syntax Highlighting"},{"location":"mdcheat.html#tables","text":"Tables aren\u2019t part of the core Markdown spec, but they are part of GFM and Markdown Here supports them. They are an easy way of adding tables to your email \u2013 a task that would otherwise require copy-pasting from another application. Colons can be used to align columns. | Tables | Are | Cool | | ------------- |:-------------:| -----:| | col 3 is | right-aligned | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 | There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown | Less | Pretty --- | --- | --- *Still* | `renders` | **nicely** 1 | 2 | 3 Colons can be used to align columns. Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u2019t need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown Less Pretty Still renders nicely 1 2 3","title":"Tables"},{"location":"mdcheat.html#blockquotes","text":"> Blockquotes are very handy in email to emulate reply text. > This line is part of the same quote. Quote break. > This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can *put* **Markdown** into a blockquote. Blockquotes are very handy in email to emulate reply text. This line is part of the same quote. Quote break. This is a very long line that will still be quoted properly when it wraps. Oh boy let\u2019s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote. Blockquote nesting is possible: > **Sed aliquet**, neque at rutrum mollis, neque nisi tincidunt nibh, vitae faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem [libero fermentum](#) urna, ut efficitur elit ligula et nunc. > > Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla. Ut sit amet placerat ante. Proin sed **elementum** __nulla__. Nunc vitae sem odio. Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum. eu odio. > > > `Suspendisse rutrum facilisis risus`, eu posuere neque commodo a. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo bibendum, sodales mauris ut, tincidunt massa. Sed aliquet , neque at rutrum mollis, neque nisi tincidunt nibh, vitae faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem libero fermentum urna, ut efficitur elit ligula et nunc. Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla. Ut sit amet placerat ante. Proin sed elementum nulla. Nunc vitae sem odio. Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum. eu odio. Suspendisse rutrum facilisis risus , eu posuere neque commodo a. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo bibendum, sodales mauris ut, tincidunt massa. Other content blocks within a blockquote Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; Praesent at :::js return target , sodales nibh vel, tempor felis. Fusce vel lacinia lacus. Suspendisse rhoncus nunc non nisi iaculis ultrices. Donec consectetur mauris non neque imperdiet, eget volutpat libero.","title":"Blockquotes"},{"location":"mdcheat.html#inline-html","text":"You can also use raw HTML in your Markdown, and it\u2019ll mostly work pretty well.","title":"Inline HTML"},{"location":"mdcheat.html#horizontal-rule","text":"Three or more... --- Hyphens *** Asterisks ___ Underscores Three or more\u2026 Hyphens Asterisks Underscores","title":"Horizontal Rule"},{"location":"mdcheat.html#line-breaks","text":"My basic recommendation for learning how line breaks work is to experiment and discover \u2013 hit <Enter> once (i.e., insert one newline), then hit it twice (i.e., insert two newlines), see what happens. You\u2019ll soon learn to get what you want. \u201cMarkdown Toggle\u201d is your friend. Here are some things to try out: Here's a line for us to start with. This line is separated from the one above by two newlines, so it will be a *separate paragraph*. This line is also a separate paragraph, but... This line is only separated by a single newline, so it's a separate line in the *same paragraph*. Here\u2019s a line for us to start with. This line is separated from the one above by two newlines, so it will be a separate paragraph . This line is also begins a separate paragraph, but\u2026 This line is only separated by a single newline, so it\u2019s a separate line in the same paragraph .","title":"Line Breaks"},{"location":"mdcheat.html#youtube-videos","text":"They can\u2019t be added directly but you can add an image with a link to the video like this: <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=YOUTUBE_VIDEO_ID_HERE \" target=\"_blank\"><img src=\"http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg\" alt=\"IMAGE ALT TEXT HERE\" width=\"240\" height=\"180\" border=\"10\" /></a> Or, in pure Markdown, but losing the image sizing and border: [![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg)](http://www.youtube.com/watch?v=YOUTUBE_VIDEO_ID_HERE) Referencing a bug by #bugID in your git commit links it to the slip. For example #1.","title":"YouTube Videos"},{"location":"mdcheat.html#admonition","text":"!!! type \"optional explicit title within double quotes\" Any number of other indented markdown elements. This is the second paragraph. Some title Any number of other indented markdown elements. This is the second paragraph. And this is outside the admonition again. If you don\u2019t want a title, use a blank string \u201c\u201d. Don\u2019t do this at home! rST suggests the following \u201ctypes\u201d: attention, caution, danger, error, hint, important, note, tip, and warning: Some title This is type note Some title This is type hint Some title This is type tip Some title This is type important Some title This is type attention Some title This is type caution Some title This is type warning Some title This is type danger Some title This is type error","title":"Admonition"},{"location":"mdcheat.html#abbreviations","text":"The HTML specification is maintained by the W3C. *[HTML]: Hyper Text Markup Language *[W3C]: World Wide Web Consortium The HTML specification is maintained by the W3C .","title":"Abbreviations"},{"location":"mdcheat.html#definition-lists","text":"Apple : Pomaceous fruit of plants of the genus Malus in the family Rosaceae. Orange : The fruit of an evergreen tree of the genus Citrus. Apple Pomaceous fruit of plants of the genus Malus in the family Rosaceae. Orange The fruit of an evergreen tree of the genus Citrus.","title":"Definition Lists"},{"location":"mdcheat.html#footnotes","text":"Footnotes 1 have a label 2 and the footnote\u2019s content. Another Footnote 3 License: CC-BY This is a footnote content. \u21a9 A footnote on the label: \u201c@#$%\u201d. \u21a9 Another content \u21a9","title":"Footnotes"},{"location":"overview.html","text":"UBELIX - Overview Description This page provides a high-level system overview of a HPC cluster such as UBELIX. It describes the different hardware components that constitute the cluster and gives a quantitative list of the different generations of compute nodes in UBELIX. UBELIX (University of Bern Linux Cluster) is a HPC cluster that currently consists of about 330 compute nodes featuring almost 6\u2018300 CPU cores and 300\u2018000 GPU cores and a software-defined storage infrastructure providing ~580 TB of disk storage net. UBELIX is a heterogeneous cluster, meaning UBELIX consists of different generations of compute nodes with different instruction sets. Compute nodes, front-end servers and the storage are interconnected through a high speed Infiniband network. The front-end servers also provide a link to the outside world. UBELIX is used by various institutes and research groups within chemistry, biology, physics, astronomy, computer science, geography, medical radiology and others for scientific research and by students working on their thesis. High-level system overview Login Server AKA Submit Server A user connects to the cluster by logging into the submit host via SSH. You can use this host for medium-performance tasks, e.g. to edit files or to compile programs. Resource-demanding/high-performance tasks must be submitted to the batch queuing system as jobs, and will finally run on one or multiple compute nodes. Even long running compile tasks could fit as a job on a compute instead of running it on the submit host Batch-Queueing System On UBELIX we use the open-source batch-queueing system Slurm for executing jobs on a pool of cooperating compute nodes. Slurm manages the distributed resources provided by the compute nodes and is responsible for accepting, scheduling, dispatching, and managing the remote and distributed execution of sequential, parallel or interactive user jobs. Cluster Partitions (Queues) and their Compute Nodes Partitions group nodes into logical sets. Nodes in a partition share the same limits. You can choose a partition depending on your jobs requirements (CPUs, memory, runtime). UBELIX provides several different partitions as shown in the following table including the corresponding hardware details of the compute nodes each partition is comprised of: Partition name max runtime (wall clock time) in h max memory per node max cores/node GPU Node Group CPU Generation #Nodes #Cores RAM Local Scratch all 96h 243GB 20 cores - hnodes[23-42] jnodes knodes anodes[145-216] sandybridge sandybridge ivybridge broadwell 20 21 36 72 16 16 16 20 70GB 243GB 117GB 117GB 250GB 500GB 850GB 850GB empi 24h 117GB 20 cores - anodes[003-144] broadwell 142 20 117GB 850GB long 1 360h 86GB 24 - hnode[43-49] ivybridge 7 24 86GB 500Gb debug 10m 117GB 20 cores - anodes[001-002] broadwell 2 20 117GB 850 gpu 2 24h 243GB 24 GTX 1080Ti Tesla P100 RTX 2080Ti gnode[01-06] gnode[07-08] gnode[09-10] broadwell 6 2 2 24 243 GB 850GB gpu-debug 10m 243GB 24 GTX 1080Ti gnode[11-12] broadwell 12 24 243 GB 850GB The all partition is the default partition if you do not specify one explicitly. Storage Infrastructure A modular, software-defined storage system (IBM Spectrum Scale) provides a shared, parallel file system that is mounted on all frontend servers and compute nodes. Ubelix also provides a limited amount of storage space on the Campus Storage. The different storage locations are summarized in the table below. For more information about the storage infrastructure see here. Path Connection Availability Backup Quota /home/ubelix/ / Network global no yes 3 /home/storage/ / Network submit host yes yes 4 Due to the limited resources and the potentially long job runtime, access to the long partition must be requested explicitly once. \u21a9 The gpu partition is closed by definition. If you need GPU resources, you have request access to this partition. Write an email to hpc@id.unibe.ch to do so. \u21a9 Default: 3TB/user, 15TB/group \u21a9 Default: 50GB/user \u21a9","title":"Overview"},{"location":"overview.html#ubelix-overview","text":"","title":"UBELIX - Overview"},{"location":"overview.html#description","text":"This page provides a high-level system overview of a HPC cluster such as UBELIX. It describes the different hardware components that constitute the cluster and gives a quantitative list of the different generations of compute nodes in UBELIX. UBELIX (University of Bern Linux Cluster) is a HPC cluster that currently consists of about 330 compute nodes featuring almost 6\u2018300 CPU cores and 300\u2018000 GPU cores and a software-defined storage infrastructure providing ~580 TB of disk storage net. UBELIX is a heterogeneous cluster, meaning UBELIX consists of different generations of compute nodes with different instruction sets. Compute nodes, front-end servers and the storage are interconnected through a high speed Infiniband network. The front-end servers also provide a link to the outside world. UBELIX is used by various institutes and research groups within chemistry, biology, physics, astronomy, computer science, geography, medical radiology and others for scientific research and by students working on their thesis.","title":"Description"},{"location":"overview.html#high-level-system-overview","text":"","title":"High-level system overview"},{"location":"overview.html#login-server-aka-submit-server","text":"A user connects to the cluster by logging into the submit host via SSH. You can use this host for medium-performance tasks, e.g. to edit files or to compile programs. Resource-demanding/high-performance tasks must be submitted to the batch queuing system as jobs, and will finally run on one or multiple compute nodes. Even long running compile tasks could fit as a job on a compute instead of running it on the submit host","title":"Login Server AKA Submit Server"},{"location":"overview.html#batch-queueing-system","text":"On UBELIX we use the open-source batch-queueing system Slurm for executing jobs on a pool of cooperating compute nodes. Slurm manages the distributed resources provided by the compute nodes and is responsible for accepting, scheduling, dispatching, and managing the remote and distributed execution of sequential, parallel or interactive user jobs.","title":"Batch-Queueing System"},{"location":"overview.html#cluster-partitions-queues-and-their-compute-nodes","text":"Partitions group nodes into logical sets. Nodes in a partition share the same limits. You can choose a partition depending on your jobs requirements (CPUs, memory, runtime). UBELIX provides several different partitions as shown in the following table including the corresponding hardware details of the compute nodes each partition is comprised of: Partition name max runtime (wall clock time) in h max memory per node max cores/node GPU Node Group CPU Generation #Nodes #Cores RAM Local Scratch all 96h 243GB 20 cores - hnodes[23-42] jnodes knodes anodes[145-216] sandybridge sandybridge ivybridge broadwell 20 21 36 72 16 16 16 20 70GB 243GB 117GB 117GB 250GB 500GB 850GB 850GB empi 24h 117GB 20 cores - anodes[003-144] broadwell 142 20 117GB 850GB long 1 360h 86GB 24 - hnode[43-49] ivybridge 7 24 86GB 500Gb debug 10m 117GB 20 cores - anodes[001-002] broadwell 2 20 117GB 850 gpu 2 24h 243GB 24 GTX 1080Ti Tesla P100 RTX 2080Ti gnode[01-06] gnode[07-08] gnode[09-10] broadwell 6 2 2 24 243 GB 850GB gpu-debug 10m 243GB 24 GTX 1080Ti gnode[11-12] broadwell 12 24 243 GB 850GB The all partition is the default partition if you do not specify one explicitly.","title":"Cluster Partitions (Queues) and their Compute Nodes"},{"location":"overview.html#storage-infrastructure","text":"A modular, software-defined storage system (IBM Spectrum Scale) provides a shared, parallel file system that is mounted on all frontend servers and compute nodes. Ubelix also provides a limited amount of storage space on the Campus Storage. The different storage locations are summarized in the table below. For more information about the storage infrastructure see here. Path Connection Availability Backup Quota /home/ubelix/ / Network global no yes 3 /home/storage/ / Network submit host yes yes 4 Due to the limited resources and the potentially long job runtime, access to the long partition must be requested explicitly once. \u21a9 The gpu partition is closed by definition. If you need GPU resources, you have request access to this partition. Write an email to hpc@id.unibe.ch to do so. \u21a9 Default: 3TB/user, 15TB/group \u21a9 Default: 50GB/user \u21a9","title":"Storage Infrastructure"},{"location":"getting-Started/getting-Started.html","text":"Getting Started This section provides an intoduction to UniBE High Performance Computing (HPC), explaining systems, accounts, and access.","title":"Getting Started"},{"location":"getting-Started/getting-Started.html#getting-started","text":"This section provides an intoduction to UniBE High Performance Computing (HPC), explaining systems, accounts, and access.","title":"Getting Started"},{"location":"user-guide/account-activation-login.html","text":"Account Activation and Login Description UBELIX is available to everybody with a valid Campus Account (CA) of the University of Bern. The cluster is meant to be used for research related to the University of Bern. Before you can use this service we have to activate your CA for UBELIX . On this page you will find useful information regarding the activation of your CA and the login procedure. Additionally, this page contains information on how to configure your SSH environment for a simplified login procedure and information regarding the application of a CA for external researchers. Account activation Request for activation To request the activation of your Campus Account, please send an email to hpc@id.unibe.ch including: a brief description of what you want to use the cluster for your Campus Account username Students must additionally provide: Students must additionally provide: the name of the institute (e.g. Mathematical Institute) if available, the name of the research group (e.g. Numerical Analysis) If you possess multiple Campus Accounts (staff and student) use your staff account since this one is more specific. As soon as we get your email we will activate your account for UBELIX. Once activated, you will receive a confirmation email containing initial instructions You cannot choose a new username for UBELIX. The username/password combination will be the same as for your Campus Account that you also use to access other services provided by the University of Bern (e.g: email, Ilias). Apply for a Campus Account for external coworkers If you do not have a Campus Account of the University of Bern, but you need access to the cluster for your cooperative scientific research with an UniBE institute, the account manager of the institute has to request a Campus Account from the IT department of the University of Bern. Please ask your coworker at the institute to arrange this for you. The responsible account manager at your institute can be found from the following link: Account managers Mailing List The official channel for informing the UBELIX community about upcoming events (e.g. maintenance) and other important news is our mailing list. Sign up to receive information on what\u2019s going on on the cluster: https://listserv.unibe.ch/mailman/listinfo/hpc-users Log in to UBELIX Before proceeding make sure that: you have your Campus Account activated for UBELIX (see above) you have a working SSH client you are operating on a Linux/Mac environment. If you are running Microsoft Windows you can use PuTTY, but we strongly encourage you to familiarize with a Unix-based operating system, if necessary by installing a flavor of Linux using virtualization software (e.g VirtualBox) Log in to UBELIX is only possible from within the UniBE network. If you want to connect from outside, you must first establish a VPN connection. For VPN profiles and instructions see the official tutorial . Mac/Linux/Unix Run the following commands in a terminal. Open an SSH connection to the submit host: $ ssh <username>@submit.unibe.ch At the password prompt enter your Campus Account password: $ ssh <username>@submit.unibe.ch Password: Usually there is no indication of typing when entering your password (not even asterisks or bullets). That\u2019s intended. Just enter your password and press \u2018enter\u2019. After log in successfully you will see the welcome message and the command prompt: Last login: Tue Apr 21 16 :17:26 2020 CentOS 7 .7.1908.x86_64 FQDN: submit01.ubelix.unibe.ch ( 10 .1.129.21 ) Processor: 24x Intel ( R ) Xeon ( R ) CPU E5-2630 v2 @ 2 .60GHz Kernel: 3 .10.0-1062.9.1.el7.x86_64 Memory: 62 .73 GiB Congratulations, you just logged in to the cluster! You can immediately start using UBELIX. Microsoft Windows We use PuTTY to illustrate how to establish a SSH connection from a Windows client to UBELIX. There are of course other SSH clients for Windows available that serve the same purpose. Download You can download PuTTY from http://www.putty.org In category \u201cSession\u201d specify: Connection type: SSH Host name (or IP address): submit.unibe.ch Port: 22 In category \u201cConnection\u201d/\u201dData\u201d specify: Auto-login username: (Enter your username here) To save your session, in category \u201cSession\u201d specify: Saved Session: ubelix (you can choose your own session name) and click the \u201cSave\u201d button Click \u201cOpen\u201d to establish a new connection. At the password prompt enter your password: Usually there is no indication of typing when entering your password (not even asterisks or bullets). That\u2019s intended. Just enter your password and press \u2018enter\u2019. After log in successfully you will see the welcome message and the command prompt: Congratulations, you just logged in to the cluster! You can immediately start using UBELIX. Customize your SSH environment Create a SSH alias Mac/Linux/Unix To simplify the login procedure you can define an alias for the user-/hostname combination. Add a host declaration to ~/.ssh/config (substitute your own alias and username): ~/.ssh/config Host <alias> Hostname submit.unibe.ch User <username> From now on you can log in to the cluster by using the specified alias: $ ssh <alias> You still have to provide your password! SSH session timeout Mac/Linux/Unix If a SSH connection goes idle for a specific amount of time (default 10 minutes), you may be confronted with a \u201cWrite failed: Broken pipe\u201d error message or the connection is simply frozen, and you are forced to log in again. To prevent this from happening, configure the client to periodically (e.g. every 60 seconds) send a message to trigger a response from the remote server. To do so, add the following line to the SSH configuration file: ServerAliveInterval 60 The host declaration may now look like this: ~/.ssh/config Host <alias> Hostname submit.unibe.ch User <username> ServerAliveInterval 60 SSH key pairs Mac/Linux/Unix SSH keys serve as a means of identifying a user to a SSH server. When using SSH keys your password will never be send over the network. Remember to always keep your private key private! Only share your public key, never share your private key. If you already have a valid private/public key pair that you also want to use for UBELIX, you can omit the rest of this section and continue with \u201cAdding a public key to your UBELIX account\u201d. First, generate a private/public key pair. You can substitute your own comment (-C). To accept the default name/location simply press Enter, otherwise specify a different name/location: $ ssh-keygen -t rsa -b 4096 -C \"ubelix\" Generating public/private rsa key pair. Enter file in which to save the key ( /Users/faerber/.ssh/id_rsa ) : Enter and confirm a secure passphrase: If you do not specify a passphrase and someone else gets a copy of your private key, then he will be able to login with your identity on any account that uses the corresponding public key! Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Adding a public key to your UBELIX account If you have specified a custom name/location for your SSH keys, you can tell your SSH client to use this key for connecting to UBELIX by specifying the private key on the command line: $ ssh -i ~/.ssh/id_rsa_ubelix <alias> or even better, add the key to your host declaration in your ssh configuration: ~/.ssh/config Host <alias> Hostname submit.unibe.ch User <username> ServerAliveInterval 60 IdentityFile ~/.ssh/id_rsa_ubelix Now, login to UBELIX and append your public key (content of id_rsa.pub) to the file ~/.ssh/authorized_keys. This step can also be done by simply issuing ssh-copy-id -i ~/.ssh/id_rsa_ubelix.pub . If everything was correct, you will now be able to login without providing you Campus Account password upon your next login attempt. However, if you have secured your key with a passphrase, you will get prompted for your passphrase instead. You can use ssh-agent to securely save your passphrase, so you do not have to re-enter it all the time. Adding your Key to SSH-Agent The behavior of ssh-agent depends on the flavor and version of your operating system. On OS X Leopard or later your keys can be saved in the system\u2019s keychain. Most Linux installations will automatically start ssh-agent when you log in. Add the key to ssh-agent: $ ssh-add ~/.ssh/id_rsa_ubelix","title":"Account Activation and Login"},{"location":"user-guide/account-activation-login.html#account-activation-and-login","text":"","title":"Account Activation and Login"},{"location":"user-guide/account-activation-login.html#description","text":"UBELIX is available to everybody with a valid Campus Account (CA) of the University of Bern. The cluster is meant to be used for research related to the University of Bern. Before you can use this service we have to activate your CA for UBELIX . On this page you will find useful information regarding the activation of your CA and the login procedure. Additionally, this page contains information on how to configure your SSH environment for a simplified login procedure and information regarding the application of a CA for external researchers.","title":"Description"},{"location":"user-guide/account-activation-login.html#account-activation","text":"Request for activation To request the activation of your Campus Account, please send an email to hpc@id.unibe.ch including: a brief description of what you want to use the cluster for your Campus Account username Students must additionally provide: Students must additionally provide: the name of the institute (e.g. Mathematical Institute) if available, the name of the research group (e.g. Numerical Analysis) If you possess multiple Campus Accounts (staff and student) use your staff account since this one is more specific. As soon as we get your email we will activate your account for UBELIX. Once activated, you will receive a confirmation email containing initial instructions You cannot choose a new username for UBELIX. The username/password combination will be the same as for your Campus Account that you also use to access other services provided by the University of Bern (e.g: email, Ilias).","title":"Account activation"},{"location":"user-guide/account-activation-login.html#apply-for-a-campus-account-for-external-coworkers","text":"If you do not have a Campus Account of the University of Bern, but you need access to the cluster for your cooperative scientific research with an UniBE institute, the account manager of the institute has to request a Campus Account from the IT department of the University of Bern. Please ask your coworker at the institute to arrange this for you. The responsible account manager at your institute can be found from the following link: Account managers","title":"Apply for a Campus Account for external coworkers"},{"location":"user-guide/account-activation-login.html#mailing-list","text":"The official channel for informing the UBELIX community about upcoming events (e.g. maintenance) and other important news is our mailing list. Sign up to receive information on what\u2019s going on on the cluster: https://listserv.unibe.ch/mailman/listinfo/hpc-users","title":"Mailing List"},{"location":"user-guide/account-activation-login.html#log-in-to-ubelix","text":"Before proceeding make sure that: you have your Campus Account activated for UBELIX (see above) you have a working SSH client you are operating on a Linux/Mac environment. If you are running Microsoft Windows you can use PuTTY, but we strongly encourage you to familiarize with a Unix-based operating system, if necessary by installing a flavor of Linux using virtualization software (e.g VirtualBox) Log in to UBELIX is only possible from within the UniBE network. If you want to connect from outside, you must first establish a VPN connection. For VPN profiles and instructions see the official tutorial .","title":"Log in to UBELIX"},{"location":"user-guide/account-activation-login.html#maclinuxunix","text":"Run the following commands in a terminal. Open an SSH connection to the submit host: $ ssh <username>@submit.unibe.ch At the password prompt enter your Campus Account password: $ ssh <username>@submit.unibe.ch Password: Usually there is no indication of typing when entering your password (not even asterisks or bullets). That\u2019s intended. Just enter your password and press \u2018enter\u2019. After log in successfully you will see the welcome message and the command prompt: Last login: Tue Apr 21 16 :17:26 2020 CentOS 7 .7.1908.x86_64 FQDN: submit01.ubelix.unibe.ch ( 10 .1.129.21 ) Processor: 24x Intel ( R ) Xeon ( R ) CPU E5-2630 v2 @ 2 .60GHz Kernel: 3 .10.0-1062.9.1.el7.x86_64 Memory: 62 .73 GiB Congratulations, you just logged in to the cluster! You can immediately start using UBELIX.","title":"Mac/Linux/Unix"},{"location":"user-guide/account-activation-login.html#microsoft-windows","text":"We use PuTTY to illustrate how to establish a SSH connection from a Windows client to UBELIX. There are of course other SSH clients for Windows available that serve the same purpose. Download You can download PuTTY from http://www.putty.org In category \u201cSession\u201d specify: Connection type: SSH Host name (or IP address): submit.unibe.ch Port: 22 In category \u201cConnection\u201d/\u201dData\u201d specify: Auto-login username: (Enter your username here) To save your session, in category \u201cSession\u201d specify: Saved Session: ubelix (you can choose your own session name) and click the \u201cSave\u201d button Click \u201cOpen\u201d to establish a new connection. At the password prompt enter your password: Usually there is no indication of typing when entering your password (not even asterisks or bullets). That\u2019s intended. Just enter your password and press \u2018enter\u2019. After log in successfully you will see the welcome message and the command prompt: Congratulations, you just logged in to the cluster! You can immediately start using UBELIX.","title":"Microsoft Windows"},{"location":"user-guide/account-activation-login.html#customize-your-ssh-environment","text":"","title":"Customize your SSH environment"},{"location":"user-guide/account-activation-login.html#create-a-ssh-alias","text":"Mac/Linux/Unix To simplify the login procedure you can define an alias for the user-/hostname combination. Add a host declaration to ~/.ssh/config (substitute your own alias and username): ~/.ssh/config Host <alias> Hostname submit.unibe.ch User <username> From now on you can log in to the cluster by using the specified alias: $ ssh <alias> You still have to provide your password!","title":"Create a SSH alias"},{"location":"user-guide/account-activation-login.html#ssh-session-timeout","text":"Mac/Linux/Unix If a SSH connection goes idle for a specific amount of time (default 10 minutes), you may be confronted with a \u201cWrite failed: Broken pipe\u201d error message or the connection is simply frozen, and you are forced to log in again. To prevent this from happening, configure the client to periodically (e.g. every 60 seconds) send a message to trigger a response from the remote server. To do so, add the following line to the SSH configuration file: ServerAliveInterval 60 The host declaration may now look like this: ~/.ssh/config Host <alias> Hostname submit.unibe.ch User <username> ServerAliveInterval 60","title":"SSH session timeout"},{"location":"user-guide/account-activation-login.html#ssh-key-pairs","text":"Mac/Linux/Unix SSH keys serve as a means of identifying a user to a SSH server. When using SSH keys your password will never be send over the network. Remember to always keep your private key private! Only share your public key, never share your private key. If you already have a valid private/public key pair that you also want to use for UBELIX, you can omit the rest of this section and continue with \u201cAdding a public key to your UBELIX account\u201d. First, generate a private/public key pair. You can substitute your own comment (-C). To accept the default name/location simply press Enter, otherwise specify a different name/location: $ ssh-keygen -t rsa -b 4096 -C \"ubelix\" Generating public/private rsa key pair. Enter file in which to save the key ( /Users/faerber/.ssh/id_rsa ) : Enter and confirm a secure passphrase: If you do not specify a passphrase and someone else gets a copy of your private key, then he will be able to login with your identity on any account that uses the corresponding public key! Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Adding a public key to your UBELIX account If you have specified a custom name/location for your SSH keys, you can tell your SSH client to use this key for connecting to UBELIX by specifying the private key on the command line: $ ssh -i ~/.ssh/id_rsa_ubelix <alias> or even better, add the key to your host declaration in your ssh configuration: ~/.ssh/config Host <alias> Hostname submit.unibe.ch User <username> ServerAliveInterval 60 IdentityFile ~/.ssh/id_rsa_ubelix Now, login to UBELIX and append your public key (content of id_rsa.pub) to the file ~/.ssh/authorized_keys. This step can also be done by simply issuing ssh-copy-id -i ~/.ssh/id_rsa_ubelix.pub . If everything was correct, you will now be able to login without providing you Campus Account password upon your next login attempt. However, if you have secured your key with a passphrase, you will get prompted for your passphrase instead. You can use ssh-agent to securely save your passphrase, so you do not have to re-enter it all the time. Adding your Key to SSH-Agent The behavior of ssh-agent depends on the flavor and version of your operating system. On OS X Leopard or later your keys can be saved in the system\u2019s keychain. Most Linux installations will automatically start ssh-agent when you log in. Add the key to ssh-agent: $ ssh-add ~/.ssh/id_rsa_ubelix","title":"SSH key pairs"},{"location":"user-guide/faq.html","text":"FAQ Description This page provides a collection of frequently asked questions. Why is my job still pending? The REASON column of the squeue output gives you a hint why your job is not running. (Resources) The job is waiting for resources to become available so that the jobs resource request can be fulfilled. (Priority) The job is not allowed to run because at least one higher prioritized job is waiting for resources. (Dependency) The job is waiting for another job to finish first (\u2013dependency=\u2026 option). (DependencyNeverSatisfied) The job is waiting for a dependency that can never be satisfied. Such a job will remain pending forever. Please cancel such jobs. (QOSMaxCpuPerUserLimit) The job is not allowed to start because your currently running jobs consume all allowed CPU resources for your user in a specific partition. Wait for jobs to finish. (AssocGrpCpuLimit) dito. (AssocGrpJobsLimit) The job is not allowed to start because you have reached the maximum of allowed running jobs for your user in a specific partition. Wait for jobs to finish. (ReqNodeNotAvail, UnavailableNodes:\u2026) Some node required by the job is currently not available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Most probably there is an active reservation for all nodes due to an upcoming maintenance downtime (see output of scontrol show reservation) and your job is not able to finish before the start of the downtime. Another reason why you should specify the duration of a job (\u2013time) as accurately as possible. Your job will start after the downtime has finished. You can list all active reservations using scontrol show reservation . Why can\u2019t I submit further jobs? sbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user\u2019s size and/or time limits) \u2026 means that you have reached the maximum of allowed jobs to be submitted to a specific partition. Job in state FAILED although job completed successfully Slurm captures the return value of the batch script/last command and reports this value as the completion status of the job/job step. Slurm indicates status FAILED if the value captured is non-zero. The following simplified example illustrates the issue: simple.c #include <unistd.h> #include <stdio.h> int main ( int argc, char *argv []) { char hostname [ 128 ] ; gethostname ( hostname, sizeof ( hostname )) ; printf ( \"%s says: Hello World.\\n\" , hostname ) ; } job.sh #!/bin/bash # Slurm options #SBATCH --mail-user=foo@bar.unibe.ch #SBATCH --mail-type=END #SBATCH --job-name=\"Simple Hello World\" #SBATCH --time=00:05:00 #SBATCH --nodes=1 # Put your code below this line ./simple bash$ sbatch job.sh Submitted batch job 104 Although the job finished successfully\u2026 slurm-104.out knlnode02.ubelix.unibe.ch says: Hello World. \u2026Slurm reports job FAILED: bash$ sacct -j 104 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 104 Simple He+ all 1 FAILED 45 :0 104 .batch batch 1 FAILED 45 :0 Problem: The exit code of the job is the exit status of batch script (job.sh) which in turn returns the exit status of the last command executed (simple) which in turn returns the return value of the last statement (printf()). Since printf() returns the number of characters printed (45), the exit code of the batch script is non-zero and consequently Slurm reports job FAILED although the job produces the desired output. Solution: Explicitly return a value: #include <unistd.h> #include <stdio.h> int main ( int argc, char *argv []) { char hostname [ 128 ] ; int n ; gethostname ( hostname, sizeof ( hostname )) ; // If successful, the total number of characters written is returned. On failure, a negative number is returned. n = printf ( \"%s says: Hello World.\\n\" , hostname ) ; if ( n < 0 ) return 1 ; return 0 ; } bash$ sacct -j 105 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 105 Simple He+ all 1 COMPLETED 0 :0 105 .batch batch 1 COMPLETED 0 :0","title":"FAQ"},{"location":"user-guide/faq.html#faq","text":"","title":"FAQ"},{"location":"user-guide/faq.html#description","text":"This page provides a collection of frequently asked questions.","title":"Description"},{"location":"user-guide/faq.html#why-is-my-job-still-pending","text":"The REASON column of the squeue output gives you a hint why your job is not running. (Resources) The job is waiting for resources to become available so that the jobs resource request can be fulfilled. (Priority) The job is not allowed to run because at least one higher prioritized job is waiting for resources. (Dependency) The job is waiting for another job to finish first (\u2013dependency=\u2026 option). (DependencyNeverSatisfied) The job is waiting for a dependency that can never be satisfied. Such a job will remain pending forever. Please cancel such jobs. (QOSMaxCpuPerUserLimit) The job is not allowed to start because your currently running jobs consume all allowed CPU resources for your user in a specific partition. Wait for jobs to finish. (AssocGrpCpuLimit) dito. (AssocGrpJobsLimit) The job is not allowed to start because you have reached the maximum of allowed running jobs for your user in a specific partition. Wait for jobs to finish. (ReqNodeNotAvail, UnavailableNodes:\u2026) Some node required by the job is currently not available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Most probably there is an active reservation for all nodes due to an upcoming maintenance downtime (see output of scontrol show reservation) and your job is not able to finish before the start of the downtime. Another reason why you should specify the duration of a job (\u2013time) as accurately as possible. Your job will start after the downtime has finished. You can list all active reservations using scontrol show reservation .","title":"Why is my job still pending?"},{"location":"user-guide/faq.html#why-cant-i-submit-further-jobs","text":"sbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user\u2019s size and/or time limits) \u2026 means that you have reached the maximum of allowed jobs to be submitted to a specific partition.","title":"Why can't I submit further jobs?"},{"location":"user-guide/faq.html#job-in-state-failed-although-job-completed-successfully","text":"Slurm captures the return value of the batch script/last command and reports this value as the completion status of the job/job step. Slurm indicates status FAILED if the value captured is non-zero. The following simplified example illustrates the issue: simple.c #include <unistd.h> #include <stdio.h> int main ( int argc, char *argv []) { char hostname [ 128 ] ; gethostname ( hostname, sizeof ( hostname )) ; printf ( \"%s says: Hello World.\\n\" , hostname ) ; } job.sh #!/bin/bash # Slurm options #SBATCH --mail-user=foo@bar.unibe.ch #SBATCH --mail-type=END #SBATCH --job-name=\"Simple Hello World\" #SBATCH --time=00:05:00 #SBATCH --nodes=1 # Put your code below this line ./simple bash$ sbatch job.sh Submitted batch job 104 Although the job finished successfully\u2026 slurm-104.out knlnode02.ubelix.unibe.ch says: Hello World. \u2026Slurm reports job FAILED: bash$ sacct -j 104 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 104 Simple He+ all 1 FAILED 45 :0 104 .batch batch 1 FAILED 45 :0 Problem: The exit code of the job is the exit status of batch script (job.sh) which in turn returns the exit status of the last command executed (simple) which in turn returns the return value of the last statement (printf()). Since printf() returns the number of characters printed (45), the exit code of the batch script is non-zero and consequently Slurm reports job FAILED although the job produces the desired output. Solution: Explicitly return a value: #include <unistd.h> #include <stdio.h> int main ( int argc, char *argv []) { char hostname [ 128 ] ; int n ; gethostname ( hostname, sizeof ( hostname )) ; // If successful, the total number of characters written is returned. On failure, a negative number is returned. n = printf ( \"%s says: Hello World.\\n\" , hostname ) ; if ( n < 0 ) return 1 ; return 0 ; } bash$ sacct -j 105 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 105 Simple He+ all 1 COMPLETED 0 :0 105 .batch batch 1 COMPLETED 0 :0","title":"Job in state FAILED although job completed successfully"},{"location":"user-guide/file-system-quota.html","text":"File System Quota Description This page contains information about quota limits on the GPFS. Quotas are enabled to control the file system usage. Default Quota Limits For each user, the default block quota limit is 3 TB, and the default file quota limit is 2 million files User Quota Job abortion Jobs will fail if no more disk space can be allocated, or if no more files can be created because the respective quota hard limit is exceeded You can display the user quota information with the mmlsquota command. It will inform you about quota limits (block quota and file quota) and the current usage on the filesystem: $ mmlsquota Block Limits | File Limits Filesystem type KB quota limit in_doubt grace | files quota limit in_doubt grace Remarks gpfs USR 28292096 3221225472 4294967296 0 none | 136704 2000000 3000000 0 none Add the \u2013block-size option to specify the unit (K , M, G, T) in which the number of blocks is displayed: $ mmlsquota --block-size = G Block Limits | File Limits Filesystem type GB quota limit in_doubt grace | files quota limit in_doubt grace Remarks gpfs USR 27 3072 4096 0 none | 136704 2000000 3000000 0 none The output shows the quotas in the file system GPFS for the user who issued the command. The quotas are set to a soft limit of 3072 GB, and a hard limit of 4096 GB. 27 GB is currently allocated to the user. An in_doubt value greater than zero means that the quota system has not yet been updated as to whether the space that is in doubt is still available or not. If the user exceeds the soft limit, the grace period will be set to one week. If usage is not reduced to a level below the soft limit during that time, the quota system interprets the soft limit as the hard limit and no further allocation is allowed. The user can reset this condition by reducing usage enough to fall below the soft limit. The maximum amount of disk space the user can accumulate during the grace period is defined by the hard limit. The same information is also displayed for the file limits (number of files). Group Quota You can display the the group quota of your own group using the -g option: $ mmlsquota -g id --block-size T Disk quotas for group id ( gid 1001 ) : Block Limits | File Limits Filesystem type TB quota limit in_doubt grace | files quota limit in_doubt grace Remarks gpfs GRP 1 15 16 0 none | 2232666 10000000 12000000 0 none Request Higher Quota Limits Make sure to clean up your home directory before requesting additional storage space","title":"File System Quota"},{"location":"user-guide/file-system-quota.html#file-system-quota","text":"","title":"File System Quota"},{"location":"user-guide/file-system-quota.html#description","text":"This page contains information about quota limits on the GPFS. Quotas are enabled to control the file system usage. Default Quota Limits For each user, the default block quota limit is 3 TB, and the default file quota limit is 2 million files","title":"Description"},{"location":"user-guide/file-system-quota.html#user-quota","text":"Job abortion Jobs will fail if no more disk space can be allocated, or if no more files can be created because the respective quota hard limit is exceeded You can display the user quota information with the mmlsquota command. It will inform you about quota limits (block quota and file quota) and the current usage on the filesystem: $ mmlsquota Block Limits | File Limits Filesystem type KB quota limit in_doubt grace | files quota limit in_doubt grace Remarks gpfs USR 28292096 3221225472 4294967296 0 none | 136704 2000000 3000000 0 none Add the \u2013block-size option to specify the unit (K , M, G, T) in which the number of blocks is displayed: $ mmlsquota --block-size = G Block Limits | File Limits Filesystem type GB quota limit in_doubt grace | files quota limit in_doubt grace Remarks gpfs USR 27 3072 4096 0 none | 136704 2000000 3000000 0 none The output shows the quotas in the file system GPFS for the user who issued the command. The quotas are set to a soft limit of 3072 GB, and a hard limit of 4096 GB. 27 GB is currently allocated to the user. An in_doubt value greater than zero means that the quota system has not yet been updated as to whether the space that is in doubt is still available or not. If the user exceeds the soft limit, the grace period will be set to one week. If usage is not reduced to a level below the soft limit during that time, the quota system interprets the soft limit as the hard limit and no further allocation is allowed. The user can reset this condition by reducing usage enough to fall below the soft limit. The maximum amount of disk space the user can accumulate during the grace period is defined by the hard limit. The same information is also displayed for the file limits (number of files).","title":"User Quota"},{"location":"user-guide/file-system-quota.html#group-quota","text":"You can display the the group quota of your own group using the -g option: $ mmlsquota -g id --block-size T Disk quotas for group id ( gid 1001 ) : Block Limits | File Limits Filesystem type TB quota limit in_doubt grace | files quota limit in_doubt grace Remarks gpfs GRP 1 15 16 0 none | 2232666 10000000 12000000 0 none","title":"Group Quota"},{"location":"user-guide/file-system-quota.html#request-higher-quota-limits","text":"Make sure to clean up your home directory before requesting additional storage space","title":"Request Higher Quota Limits"},{"location":"user-guide/file-transfer.html","text":"File Transfer from/to UBELIX Description This page contains some basic information about moving files between your local workstation and the cluster. Mac/Linux/Windows You can use different protocols/programs for transferring files from/to the cluster, depending on your need: Sftp, SCP, Rsync, Wget, and others. The following commands are from on your local workstation as indicated by \u201clocal$\u201d If you have customized your SSH environment as described here, you can substitute your host alias for @submit.unibe.ch in the following commands Secure Copy (SCP) - Mac/Linux Secure Copy is a program (also a protocol) that allows you to securely transfer files between local and remote hosts. SCP uses SSH for transferring data and managing authentication. SCP performs a plain linear copy of the specified files, while replacing already existing files with the same name. If you need more sophisticated control over your copy process, consider Rsync . Syntax scp [options] source destination Some common options -r : copy directories recursively (Note that SCP follows symbolic links encountered in the tree traversal) -p : preserve modification time, access time, and modes from the original file -v : verbose mode Copying Files from Your Local Workstation to UBELIX Copy the file ~/dir/file01 to your remote home directory: $ scp ~/dir/file01 <username>@submit.unibe.ch: Copy multiple files to the remote directory ~/bar : The destination directory must already exist. You can create a directory from remote with: ssh @submit.unibe.ch \u2018mkdir -p ~/bar\u2019 $ scp ~/dir/file01 ~/dir/file02 ~/dir/file03 <username>@submit.unibe.ch:bar Copy all files within directory ~/dir to the remote directory ~/bar : Add the -r option (recursive) to also copy all subdirectories of ~/dir $ scp -r ~/dir/* <username>@submit.unibe.ch:bar Copy the directory ~/dir to your remote home directory: This will create a new directory ~/dir on the remote host. If the directory ~/dir already exists, the following command adds the content of the source directory to the destination directory $ scp -r ~/dir <username>@submit.unibe.ch: Copying Files from UBELIX to Your Local Workstation Copy the remote file ~/bar/file01 to the current working directory on your local workstation: $ scp <username>@submit.unibe.ch:bar/file01 . Copy multiple remote files to the local directory ~/dir : The local directory ~/dir will be automatically created if it does not already exist $ scp <username>@submit.unibe.ch:bar/ \\{ file02,file03,file04 \\} ~/dir Copy the remote directory ~/bar to the current working directory on your local workstation: $ scp -r <username>@submit.unibe.ch:bar . Remote Sync (Rsync) - Mac/Linux Rsync implements a sophisticated algorithm that allows to transfer only missing/non-matching parts of a source file to update a target file. With this the process of transferring data may be significantly faster than simply replacing all data. Among other things, Rsync also allows you to specify complex filter rules to exclude certain files or directories located inside a directory that you want to sync. Syntax rsync [options] source destination Some common options -r : copy directories recursively (does not preserve timestamps and permissions) -a : archive mode (like -r, but also preserves timestamps, permissions, ownership, and copies symlinks as symlinks) -z : compress data -v : verbose mode (additional v\u2019s will increase verbosity level) -n : dry-run -h : output numbers in a human readable format Copying Files from Your Local Workstation to UBELIX Copy the file ~/dir/file01 to your remote home directory: $ rsync ~/dir/file01 <username>@submit.unibe.ch: Copy multiple files to your remote home directory: $ rsync file01 file02 file03 <username>@submit.unibe.ch: Copy the local directory ~/dir to the remote directory ~/bar: With a trailing slash (/) after the source directory only the content of the source directory is copied to the destination directory. Without a trailing slash both the source directory and the content of the directory are copied to the destination directory $ rsync -az ~/dir/ <username>@submit.unibe.ch:bar Copying Files from UBELIX to Your Local Workstation Copy the remote file ~/foo/file01 to your current working directory: $ rsync <username>@submit.unibe.ch:foo/file01 . Copy the remote files ~/foo/file01 and ~/bar/file02 to your the local directory ~/dir : $ rsync <username>@submit.unibe.ch: \\{ foo/file01,bar/file02 \\} ~/dir Copy the remote directory ~/foo to the local directory ~/dir : With a trailing slash (/) after the source directory only the content of the source directory is copied to the destination directory. Without a trailing slash both the source directory and the content of the directory are copied to the destination directory. $ rsync -az <username>@submit.unibe.ch:foo/ ~/dir Including/Excluding Files With the --include / --exclude options you can specify patterns, that describe which files are not excluded/excluded from the copy process. Use the -n option with the -v option to perform a dry-run while listing the files that would be copied Exclude a specific directory: rsync -av --exclude \"subdir1\" ~/dir/ <username>@submit.unibe.ch: Copy only files with suffix .txt and .m : rsync -av --include \"*.txt\" --include \"*.m\" --exclude \"*\" ~/dir/ <username>@submit.unibe.ch: Copy all files with suffix .m within the source directory ~/dir (including matching files within subdirectories) to the remote destination directory ~/foo : Use the --prune-empty-dirs option to omit copying empty directories $ rsync -av --prune-empty-dirs --include \"*/\" --include \"*.m\" --exclude \"*\" ~/dir/ <username>@submit.unibe.ch:foo Deleting Files None of the following commands will delete any files in your source folder This delete options can be dangerous if used incorrectly! Perform a dry-run ( -n option) first and verify that important files are not listed ( -v option) for deletion Use the --delete option to delete files/directories from the destination directory that are not/no more present in the source directory: $ rsync -av --delete ~/dir/ <username>@submit.unibe.ch:mfiles With the --delete-excluded option you can additionally delete files from the destination directory that are excluded from transferring/syncing (not in the generated file list): $ rsync -av --prune-empty-dirs --delete-excluded --include \"*/\" --include \"*.m\" --exclude \"*\" ~/dir/ <username>@submit.unibe.ch:foo WinSCP - Windows We use WinSCP to illustrate file transfers from Windows. There are of course other tools that serve the same purpose. Type You can download WinSCP from https://winscp.net/eng/index.php Open WinSCP and select \u201cNew Site\u201d in the left field to define the session parameters: File protocol: SFTP Host name: submit.unibe.ch Port number: 22 User name: (Enter your username here) Click the \u201cSave\u201d button to save your site Specify an alias for the site: Site name: (Choose your own alias, e.g. ubelix) Press the \u201cOK\u201d button to save the session as a site In the left field, double-click on your alias to open the connection, enter your Campus Account password and click \u201cOK\u201d: Now you can move (drag and drop) files between your work station and the submit host using the two views:","title":"File Transfer"},{"location":"user-guide/file-transfer.html#file-transfer-fromto-ubelix","text":"","title":"File Transfer from/to UBELIX"},{"location":"user-guide/file-transfer.html#description","text":"This page contains some basic information about moving files between your local workstation and the cluster.","title":"Description"},{"location":"user-guide/file-transfer.html#maclinuxwindows","text":"You can use different protocols/programs for transferring files from/to the cluster, depending on your need: Sftp, SCP, Rsync, Wget, and others. The following commands are from on your local workstation as indicated by \u201clocal$\u201d If you have customized your SSH environment as described here, you can substitute your host alias for @submit.unibe.ch in the following commands","title":"Mac/Linux/Windows"},{"location":"user-guide/file-transfer.html#secure-copy-scp-maclinux","text":"Secure Copy is a program (also a protocol) that allows you to securely transfer files between local and remote hosts. SCP uses SSH for transferring data and managing authentication. SCP performs a plain linear copy of the specified files, while replacing already existing files with the same name. If you need more sophisticated control over your copy process, consider Rsync . Syntax scp [options] source destination Some common options -r : copy directories recursively (Note that SCP follows symbolic links encountered in the tree traversal) -p : preserve modification time, access time, and modes from the original file -v : verbose mode","title":"Secure Copy (SCP) - Mac/Linux"},{"location":"user-guide/file-transfer.html#copying-files-from-your-local-workstation-to-ubelix","text":"Copy the file ~/dir/file01 to your remote home directory: $ scp ~/dir/file01 <username>@submit.unibe.ch: Copy multiple files to the remote directory ~/bar : The destination directory must already exist. You can create a directory from remote with: ssh @submit.unibe.ch \u2018mkdir -p ~/bar\u2019 $ scp ~/dir/file01 ~/dir/file02 ~/dir/file03 <username>@submit.unibe.ch:bar Copy all files within directory ~/dir to the remote directory ~/bar : Add the -r option (recursive) to also copy all subdirectories of ~/dir $ scp -r ~/dir/* <username>@submit.unibe.ch:bar Copy the directory ~/dir to your remote home directory: This will create a new directory ~/dir on the remote host. If the directory ~/dir already exists, the following command adds the content of the source directory to the destination directory $ scp -r ~/dir <username>@submit.unibe.ch:","title":"Copying Files from Your Local Workstation to UBELIX"},{"location":"user-guide/file-transfer.html#copying-files-from-ubelix-to-your-local-workstation","text":"Copy the remote file ~/bar/file01 to the current working directory on your local workstation: $ scp <username>@submit.unibe.ch:bar/file01 . Copy multiple remote files to the local directory ~/dir : The local directory ~/dir will be automatically created if it does not already exist $ scp <username>@submit.unibe.ch:bar/ \\{ file02,file03,file04 \\} ~/dir Copy the remote directory ~/bar to the current working directory on your local workstation: $ scp -r <username>@submit.unibe.ch:bar .","title":"Copying Files from UBELIX to Your Local Workstation"},{"location":"user-guide/file-transfer.html#remote-sync-rsync-maclinux","text":"Rsync implements a sophisticated algorithm that allows to transfer only missing/non-matching parts of a source file to update a target file. With this the process of transferring data may be significantly faster than simply replacing all data. Among other things, Rsync also allows you to specify complex filter rules to exclude certain files or directories located inside a directory that you want to sync. Syntax rsync [options] source destination Some common options -r : copy directories recursively (does not preserve timestamps and permissions) -a : archive mode (like -r, but also preserves timestamps, permissions, ownership, and copies symlinks as symlinks) -z : compress data -v : verbose mode (additional v\u2019s will increase verbosity level) -n : dry-run -h : output numbers in a human readable format","title":"Remote Sync (Rsync) - Mac/Linux"},{"location":"user-guide/file-transfer.html#copying-files-from-your-local-workstation-to-ubelix_1","text":"Copy the file ~/dir/file01 to your remote home directory: $ rsync ~/dir/file01 <username>@submit.unibe.ch: Copy multiple files to your remote home directory: $ rsync file01 file02 file03 <username>@submit.unibe.ch: Copy the local directory ~/dir to the remote directory ~/bar: With a trailing slash (/) after the source directory only the content of the source directory is copied to the destination directory. Without a trailing slash both the source directory and the content of the directory are copied to the destination directory $ rsync -az ~/dir/ <username>@submit.unibe.ch:bar","title":"Copying Files from Your Local Workstation to UBELIX"},{"location":"user-guide/file-transfer.html#copying-files-from-ubelix-to-your-local-workstation_1","text":"Copy the remote file ~/foo/file01 to your current working directory: $ rsync <username>@submit.unibe.ch:foo/file01 . Copy the remote files ~/foo/file01 and ~/bar/file02 to your the local directory ~/dir : $ rsync <username>@submit.unibe.ch: \\{ foo/file01,bar/file02 \\} ~/dir Copy the remote directory ~/foo to the local directory ~/dir : With a trailing slash (/) after the source directory only the content of the source directory is copied to the destination directory. Without a trailing slash both the source directory and the content of the directory are copied to the destination directory. $ rsync -az <username>@submit.unibe.ch:foo/ ~/dir","title":"Copying Files from UBELIX to Your Local Workstation"},{"location":"user-guide/file-transfer.html#includingexcluding-files","text":"With the --include / --exclude options you can specify patterns, that describe which files are not excluded/excluded from the copy process. Use the -n option with the -v option to perform a dry-run while listing the files that would be copied Exclude a specific directory: rsync -av --exclude \"subdir1\" ~/dir/ <username>@submit.unibe.ch: Copy only files with suffix .txt and .m : rsync -av --include \"*.txt\" --include \"*.m\" --exclude \"*\" ~/dir/ <username>@submit.unibe.ch: Copy all files with suffix .m within the source directory ~/dir (including matching files within subdirectories) to the remote destination directory ~/foo : Use the --prune-empty-dirs option to omit copying empty directories $ rsync -av --prune-empty-dirs --include \"*/\" --include \"*.m\" --exclude \"*\" ~/dir/ <username>@submit.unibe.ch:foo","title":"Including/Excluding Files"},{"location":"user-guide/file-transfer.html#deleting-files","text":"None of the following commands will delete any files in your source folder This delete options can be dangerous if used incorrectly! Perform a dry-run ( -n option) first and verify that important files are not listed ( -v option) for deletion Use the --delete option to delete files/directories from the destination directory that are not/no more present in the source directory: $ rsync -av --delete ~/dir/ <username>@submit.unibe.ch:mfiles With the --delete-excluded option you can additionally delete files from the destination directory that are excluded from transferring/syncing (not in the generated file list): $ rsync -av --prune-empty-dirs --delete-excluded --include \"*/\" --include \"*.m\" --exclude \"*\" ~/dir/ <username>@submit.unibe.ch:foo","title":"Deleting Files"},{"location":"user-guide/file-transfer.html#winscp-windows","text":"We use WinSCP to illustrate file transfers from Windows. There are of course other tools that serve the same purpose. Type You can download WinSCP from https://winscp.net/eng/index.php Open WinSCP and select \u201cNew Site\u201d in the left field to define the session parameters: File protocol: SFTP Host name: submit.unibe.ch Port number: 22 User name: (Enter your username here) Click the \u201cSave\u201d button to save your site Specify an alias for the site: Site name: (Choose your own alias, e.g. ubelix) Press the \u201cOK\u201d button to save the session as a site In the left field, double-click on your alias to open the connection, enter your Campus Account password and click \u201cOK\u201d: Now you can move (drag and drop) files between your work station and the submit host using the two views:","title":"WinSCP - Windows"},{"location":"user-guide/getting-started.html","text":"Getting Started This page is intended as a brief introduction into HPC and to get you up and running with Ubelix. This hands-on introduction targets primarily users without prior knowledge in high-performance computing. However, basic Linux knowledge is a prerequisite. If you are not familiar with basic Linux commands, there are many beginner tutorials available online. After reading this section you will have composed and submitted your first job successfully to the cluster. Links are provided throughout the text to point you to the user guide which contains more in-depth information on the topic. Request an Account Staff and students of the University of Bern must have their Campus Account (CA) unlocked for UBELIX. External researchers that collaborate with an institute of the University of Bern must apply for a CA through that institute. See here for more information getting access to UBELIX. Training Courses ScITS regularly conducts introductory and advanced courses on Linux, UBELIX and other topics. Details oultined on their pages . Cluster Rules As everywhere where people come together, rules are needed on Ubelix to allow for a good cooperation and to enable a positive HPC experience. Be always aware that you are working on a shared system where your behavior could have a negative impact on the workflow of other users. Please find the list of the most important rules and guidelines in our code of conduct . Login To connect to the cluster, you must log in to the submit host from inside the university network (e.g. from a workstation on the campus). If you want to connect from a remote location (e.g. from your computer at home) you must first establish a VPN connection to get access to the university network. To connect from a UNIX-like system (Linux, Mac OS X) simply use a secure shell (SSH) to log in to the submit host: ssh <username>@submit.unibe.ch # the following is equivalent ssh -l <username> submit.unibe.ch Welcome Home After successful login to the cluster, your will find yourself in the directory /home/ubelix/ / /. This is your home directory and serves as the repository for your personal files, direcotries and programs. You can reference your home directory by ~ or $HOME. Your home directory is located on a shared file system. Therefore, all your files, programs and directory structure are always available on all cluster nodes and must hence not be copied between those nodes. We provide no backup service for data in your home directory. It is your own responsibility to backup important data to a private location. Disk space is managed by quotas . By default, each user has 3TB of disk space available. Keep your home directory clean by regularly deleting old data or by moving data to a private storage. You can always print the current working directory using the pwd (print working directory) command: pwd /home/ubelix/test/testuser Move Data At some point, you will probably need to copy files between your local computer and the cluster. There are different ways to achieve this, depending on your local operating system (OS). To copy a file from your local computer running a UNIX-like OS use the secure copy command (scp) on your local workstation: scp /path/to/file <username>@submit.unibe.ch:/path/to/target_dir/ To copy a file from the cluster to your local computer running a UNIX-like OS also use the secure copy command (scp) on your local workstation: scp <username>@submit.unibe.ch:/path/to/file /path/to/target_dir/ More information about file transfer can be found on the page File Transfer to/from UBELIX . Use Software On Ubelix you can make use of already preinstalled software or you can compile and install your own software. We use a module system to manage different versions of the same software. This allows you to focus on getting your work done instead of compilling software. E.g. to get a list of all provided versions of the GNU Compiler Collection (GCC), use: module avail To load GCC version 4.9, use: module load gcc/4.9 Now, you are using this specific version of GCC: gcc --version gcc ( GCC ) 4 .9.3 Copyright ( C ) 2015 Free Software Foundation, Inc. This is free software ; see the source for copying conditions. There is NO warranty ; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Scope The loaded version of a software is only active in your current session. If you open a new shell you are again using the default version of the software. Therefore, it is crucial to load the required modules from within your job script. The Software is dedicated to this topic. More information can be found there. Hello World Finally, it\u2019s time for your first job. To do some work on the cluster, you require certain ressources (e.g. CPUs and memory) and a description of the computations to be done. A job consits of instructions to the scheduler in the form of option flags, and statements that describe the actual tasks. Let\u2019s start with the instructions to the scheduler: 1 2 3 4 5 6 7 8 #!/bin/bash #SBATCH --mail-user=<your_email_address> #SBATCH --mail-type=none #SBATCH --partition=all #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 # Put your code below this line The first line makes sure that the file is executed using the bash shell. The remaining lines are option flags used by the sbatch command. The page Jobs Submission outlines the most important options of sbatch . Now, let\u2019s write a simple \u201chello, world\u201d-task: 1 2 3 4 5 # Put your code below this line mkdir -p $HOME /my_first_job cd $HOME /my_first_job echo \"Hello, Ubelix from node $( hostname ) \" > hello.txt sleep 120 In the first line we create a new directory \u2018my_first_job\u2019 within our home directory. The variable $HOME expands to /home/ubelix/<your_group>/<your_username> . In the second line we change directory to the newly created directory. In the third line we print the line \u201cHello, Ubelix from node \u201d and redirect the output to a file named hello.txt . The expression $(hostname) means, run the command hostname and put its output here. In the forth line we wait (do nothing) for 120 seconds. This gives us some time to monitor our job later on. Save the content to a file named first.sh. The complete job script looks like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash #SBATCH --mail-user=<your_email_address> #SBATCH --mail-type=end,fail #SBATCH --job-name=\"job01\" #SBATCH --partition=all #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 # Put your code below this line mkdir -p $HOME /my_first_job cd $HOME /my_first_job echo \"Hello, Ubelix from node $( hostname ) \" > hello.txt sleep 120 Use Correct Emai Addresss! When using a mail-type other than \u2018none\u2019, make sure that you use a valid email address with the \u2013mail-user option! Schedule Your Job We can now submit our first job to the scheduler. The scheduler will then provide the requested resources to the job. If all requested resources are already available, then your job can start immediately. Otherwise your job will wait until enough resources are available. We submit our job to the scheduler using the sbatch command: sbatch first.sh Submitted batch job 32490640 If the job is submitted successfully, the command outputs a job-ID with which you can refer to your job later on. Monitor Your Job You can inspect the state of our active jobs (running or pending) with the squeue command: squeue --job = 32490640 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 32490640 all job01 testuser R 0:22 1 fnode23 Here you can see that the job \u2018job01\u2019 with job-ID 32490640 is in state RUNNING (R). The job is running in the \u2018all\u2019 partition (default partition) on fnode23 for 22 seconds. It is also possible that the job can not start immediately after submitting it to Slurm because the requested resources are not yet available. In this case, the output could look like this: squeue --job = 32490640 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 32490640 all job01 testuser PD 0:00 1 (Priority) Here you can see that the job is in state PENDING (PD) and a reason why the job is pending. In this example, the job has to wait for at least one higher priorised job to run. See here for a list of other reasons why a job might be pending. You can always list all your active (pending or running) jobs with squeue: squeue --user = testuser JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 34651451 all slurm.sh testuser PD 0:00 2 (Priority) 34651453 all slurm.sh testuser PD 0:00 2 (Priority) 29143227 empi Rjob testuser PD 0:00 4 (JobHeldUser) 37856328 empi mpi.sh testuser R 4:38 2 anode[041-042] 32634559 all fast.sh testuser R 2:52:37 1 hnode12 32634558 all fast.sh testuser R 3:00:54 1 hnode14 32634554 all fast.sh testuser R 4:11:26 1 jnode08 32633556 all fast.sh testuser R 4:36:10 1 jnode08","title":"Getting Started"},{"location":"user-guide/getting-started.html#getting-started","text":"This page is intended as a brief introduction into HPC and to get you up and running with Ubelix. This hands-on introduction targets primarily users without prior knowledge in high-performance computing. However, basic Linux knowledge is a prerequisite. If you are not familiar with basic Linux commands, there are many beginner tutorials available online. After reading this section you will have composed and submitted your first job successfully to the cluster. Links are provided throughout the text to point you to the user guide which contains more in-depth information on the topic.","title":"Getting Started"},{"location":"user-guide/getting-started.html#request-an-account","text":"Staff and students of the University of Bern must have their Campus Account (CA) unlocked for UBELIX. External researchers that collaborate with an institute of the University of Bern must apply for a CA through that institute. See here for more information getting access to UBELIX.","title":"Request an Account"},{"location":"user-guide/getting-started.html#training-courses","text":"ScITS regularly conducts introductory and advanced courses on Linux, UBELIX and other topics. Details oultined on their pages .","title":"Training Courses"},{"location":"user-guide/getting-started.html#cluster-rules","text":"As everywhere where people come together, rules are needed on Ubelix to allow for a good cooperation and to enable a positive HPC experience. Be always aware that you are working on a shared system where your behavior could have a negative impact on the workflow of other users. Please find the list of the most important rules and guidelines in our code of conduct .","title":"Cluster Rules"},{"location":"user-guide/getting-started.html#login","text":"To connect to the cluster, you must log in to the submit host from inside the university network (e.g. from a workstation on the campus). If you want to connect from a remote location (e.g. from your computer at home) you must first establish a VPN connection to get access to the university network. To connect from a UNIX-like system (Linux, Mac OS X) simply use a secure shell (SSH) to log in to the submit host: ssh <username>@submit.unibe.ch # the following is equivalent ssh -l <username> submit.unibe.ch","title":"Login"},{"location":"user-guide/getting-started.html#welcome-home","text":"After successful login to the cluster, your will find yourself in the directory /home/ubelix/ / /. This is your home directory and serves as the repository for your personal files, direcotries and programs. You can reference your home directory by ~ or $HOME. Your home directory is located on a shared file system. Therefore, all your files, programs and directory structure are always available on all cluster nodes and must hence not be copied between those nodes. We provide no backup service for data in your home directory. It is your own responsibility to backup important data to a private location. Disk space is managed by quotas . By default, each user has 3TB of disk space available. Keep your home directory clean by regularly deleting old data or by moving data to a private storage. You can always print the current working directory using the pwd (print working directory) command: pwd /home/ubelix/test/testuser","title":"Welcome Home"},{"location":"user-guide/getting-started.html#move-data","text":"At some point, you will probably need to copy files between your local computer and the cluster. There are different ways to achieve this, depending on your local operating system (OS). To copy a file from your local computer running a UNIX-like OS use the secure copy command (scp) on your local workstation: scp /path/to/file <username>@submit.unibe.ch:/path/to/target_dir/ To copy a file from the cluster to your local computer running a UNIX-like OS also use the secure copy command (scp) on your local workstation: scp <username>@submit.unibe.ch:/path/to/file /path/to/target_dir/ More information about file transfer can be found on the page File Transfer to/from UBELIX .","title":"Move Data"},{"location":"user-guide/getting-started.html#use-software","text":"On Ubelix you can make use of already preinstalled software or you can compile and install your own software. We use a module system to manage different versions of the same software. This allows you to focus on getting your work done instead of compilling software. E.g. to get a list of all provided versions of the GNU Compiler Collection (GCC), use: module avail To load GCC version 4.9, use: module load gcc/4.9 Now, you are using this specific version of GCC: gcc --version gcc ( GCC ) 4 .9.3 Copyright ( C ) 2015 Free Software Foundation, Inc. This is free software ; see the source for copying conditions. There is NO warranty ; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Scope The loaded version of a software is only active in your current session. If you open a new shell you are again using the default version of the software. Therefore, it is crucial to load the required modules from within your job script. The Software is dedicated to this topic. More information can be found there.","title":"Use Software"},{"location":"user-guide/getting-started.html#hello-world","text":"Finally, it\u2019s time for your first job. To do some work on the cluster, you require certain ressources (e.g. CPUs and memory) and a description of the computations to be done. A job consits of instructions to the scheduler in the form of option flags, and statements that describe the actual tasks. Let\u2019s start with the instructions to the scheduler: 1 2 3 4 5 6 7 8 #!/bin/bash #SBATCH --mail-user=<your_email_address> #SBATCH --mail-type=none #SBATCH --partition=all #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 # Put your code below this line The first line makes sure that the file is executed using the bash shell. The remaining lines are option flags used by the sbatch command. The page Jobs Submission outlines the most important options of sbatch . Now, let\u2019s write a simple \u201chello, world\u201d-task: 1 2 3 4 5 # Put your code below this line mkdir -p $HOME /my_first_job cd $HOME /my_first_job echo \"Hello, Ubelix from node $( hostname ) \" > hello.txt sleep 120 In the first line we create a new directory \u2018my_first_job\u2019 within our home directory. The variable $HOME expands to /home/ubelix/<your_group>/<your_username> . In the second line we change directory to the newly created directory. In the third line we print the line \u201cHello, Ubelix from node \u201d and redirect the output to a file named hello.txt . The expression $(hostname) means, run the command hostname and put its output here. In the forth line we wait (do nothing) for 120 seconds. This gives us some time to monitor our job later on. Save the content to a file named first.sh. The complete job script looks like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash #SBATCH --mail-user=<your_email_address> #SBATCH --mail-type=end,fail #SBATCH --job-name=\"job01\" #SBATCH --partition=all #SBATCH --nodes=1 #SBATCH --ntasks-per-node=1 # Put your code below this line mkdir -p $HOME /my_first_job cd $HOME /my_first_job echo \"Hello, Ubelix from node $( hostname ) \" > hello.txt sleep 120 Use Correct Emai Addresss! When using a mail-type other than \u2018none\u2019, make sure that you use a valid email address with the \u2013mail-user option!","title":"Hello World"},{"location":"user-guide/getting-started.html#schedule-your-job","text":"We can now submit our first job to the scheduler. The scheduler will then provide the requested resources to the job. If all requested resources are already available, then your job can start immediately. Otherwise your job will wait until enough resources are available. We submit our job to the scheduler using the sbatch command: sbatch first.sh Submitted batch job 32490640 If the job is submitted successfully, the command outputs a job-ID with which you can refer to your job later on.","title":"Schedule Your Job"},{"location":"user-guide/getting-started.html#monitor-your-job","text":"You can inspect the state of our active jobs (running or pending) with the squeue command: squeue --job = 32490640 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 32490640 all job01 testuser R 0:22 1 fnode23 Here you can see that the job \u2018job01\u2019 with job-ID 32490640 is in state RUNNING (R). The job is running in the \u2018all\u2019 partition (default partition) on fnode23 for 22 seconds. It is also possible that the job can not start immediately after submitting it to Slurm because the requested resources are not yet available. In this case, the output could look like this: squeue --job = 32490640 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 32490640 all job01 testuser PD 0:00 1 (Priority) Here you can see that the job is in state PENDING (PD) and a reason why the job is pending. In this example, the job has to wait for at least one higher priorised job to run. See here for a list of other reasons why a job might be pending. You can always list all your active (pending or running) jobs with squeue: squeue --user = testuser JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 34651451 all slurm.sh testuser PD 0:00 2 (Priority) 34651453 all slurm.sh testuser PD 0:00 2 (Priority) 29143227 empi Rjob testuser PD 0:00 4 (JobHeldUser) 37856328 empi mpi.sh testuser R 4:38 2 anode[041-042] 32634559 all fast.sh testuser R 2:52:37 1 hnode12 32634558 all fast.sh testuser R 3:00:54 1 hnode14 32634554 all fast.sh testuser R 4:11:26 1 jnode08 32633556 all fast.sh testuser R 4:36:10 1 jnode08","title":"Monitor Your Job"},{"location":"user-guide/job-management/array-jobs.html","text":"Array Jobs with Slurm Description You want to submit multiple jobs that are identical or differ only in some arguments. Instead of submitting N jobs independently, you can submit one array job unifying N tasks. Submitting an Array To submit an array job, specify the number of tasks as a range of task ids using the \u2013array option: #SBATCH --array=n[,k[,...]][-m[:s]] The task id range specified in the option argument may be a single number, a simple range of the form n-m, a range with a step size s, a comma separated list of values, or a combination thereof. The task ids will be exported to the job tasks via the environment variable SLURM_ARRAY_TASK_ID. Other variables available in the context of the job describing the task range are: SLURM_ARRAY_TASK_MAX, SLURM_ARRAY_TASK_MIN, SLURM_ARRAY_TASK_STEP. Specifying \u2013array=10 will not submit an array job with 10 tasks, but an array job with a single task with task id 10. To run an array job with multiple tasks you must specify a range or a comma separated list of task ids. Limit the Number of Concurrently Running Tasks You may want to limit the number of concurrently running tasks if the tasks are very resource demanding and too many of them running concurrently would lower the overall performance of the cluster. To limit the number of tasks that are allowed to run concurrently, use a \u201c%\u201d separator: #SBATCH --array=n[,k[,...]][-m[:s]]%<max_tasks> Canceling Individual Tasks You can cancel individual tasks of an array job by indicating tasks ids to the scancel command: $ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 79265_ [ 49 -99:2%20 ] test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_41 test Simple H foo R 0 :10 1 fnode03 79265_43 test Simple H foo R 0 :10 1 fnode03 79265_45 test Simple H foo R 0 :10 1 fnode03 79265_47 test Simple H foo R 0 :10 1 fnode03 Use the \u2013array option to the squeue command to display one tasks per line: $ squeue --array JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 79265_65 test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_67 test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_69 test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_97 test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_57 test Simple H foo R 0 :47 1 fnode03 79265_59 test Simple H foo R 0 :47 1 fnode03 79265_61 test Simple H foo R 0 :47 1 fnode03 79265_63 test Simple H foo R 0 :47 1 fnode03 Examples Use case 1: 1000 computations, same resource requirements, different input/output arguments Instead of submitting 1000 individual jobs, submit a single array jobs with 1000 tasks: #!/bin/bash #SBATCH --mail-type=NONE #SBATCH --partition=all #SBATCH --time=00:30:00 # Each task takes max 30 minutes #SBATCH --mem-per-cpu=2G # Each task uses max 2G of memory #SBATCH --array=1-1000 # Submit 1000 tasks with task ID 1,2,...,1000. #SBATCH --output=/dev/null #SBATCH --error=/dev/null # The name of the input files must reflect the task ID! ./foo input_data_ ${ SLURM_ARRAY_TASK_ID } .txt > output_ ${ SLURM_ARRAY_TASK_ID } .txt Task with ID 20 will run the program foo with the following arguments: ./foo input_data_20.txt > output_20.txt Use case 2: Read arguments from file Submit an array job with 1000 tasks. Each task executes the program foo with different arguments: #!/bin/bash #SBATCH --mail-type=NONE #SBATCH --partition=all #SBATCH --time=00:30:00 # Each task takes max 30 minutes #SBATCH --mem-per-cpu=2G # Each task uses max 2G of memory #SBATCH --array=1-1000%20 # Submit 1000 tasks with task ID 1,2,...,1000. Run max 20 tasks concurrently #SBATCH --output=/dev/null #SBATCH --error=/dev/null param_store = $HOME /projects/example/args.txt # args.txt contains 1000 lines with 2 arguments per line. # Line <i> contains arguments for run <i> data_dir = $HOME /projects/example/input_data # Input files are named input_run_0001.txt,...input_run_1000.txt result_dir = $HOME /projects/example/results param_a = $( cat $param_store | awk -v var = $SLURM_ARRAY_TASK_ID 'NR==var {print $1}' ) # Get first argument param_b = $( cat $param_store | awk -v var = $SLURM_ARRAY_TASK_ID 'NR==var {print $2}' ) # Get second argument n = $( printf \"%04d\" $SLURM_ARRAY_TASK_ID ) # Zero pad the task ID to match the numbering of the input files ./foo -c $param_a -p $param_b -i ${ data_dir } /input_run_ ${ n } .txt -o ${ result_dir } /result_run_ ${ n } .txt exit Related pages:","title":"Array Jobs with Slurm"},{"location":"user-guide/job-management/array-jobs.html#array-jobs-with-slurm","text":"","title":"Array Jobs with Slurm"},{"location":"user-guide/job-management/array-jobs.html#description","text":"You want to submit multiple jobs that are identical or differ only in some arguments. Instead of submitting N jobs independently, you can submit one array job unifying N tasks.","title":"Description"},{"location":"user-guide/job-management/array-jobs.html#submitting-an-array","text":"To submit an array job, specify the number of tasks as a range of task ids using the \u2013array option: #SBATCH --array=n[,k[,...]][-m[:s]] The task id range specified in the option argument may be a single number, a simple range of the form n-m, a range with a step size s, a comma separated list of values, or a combination thereof. The task ids will be exported to the job tasks via the environment variable SLURM_ARRAY_TASK_ID. Other variables available in the context of the job describing the task range are: SLURM_ARRAY_TASK_MAX, SLURM_ARRAY_TASK_MIN, SLURM_ARRAY_TASK_STEP. Specifying \u2013array=10 will not submit an array job with 10 tasks, but an array job with a single task with task id 10. To run an array job with multiple tasks you must specify a range or a comma separated list of task ids.","title":"Submitting an Array"},{"location":"user-guide/job-management/array-jobs.html#limit-the-number-of-concurrently-running-tasks","text":"You may want to limit the number of concurrently running tasks if the tasks are very resource demanding and too many of them running concurrently would lower the overall performance of the cluster. To limit the number of tasks that are allowed to run concurrently, use a \u201c%\u201d separator: #SBATCH --array=n[,k[,...]][-m[:s]]%<max_tasks>","title":"Limit the Number of Concurrently Running Tasks"},{"location":"user-guide/job-management/array-jobs.html#canceling-individual-tasks","text":"You can cancel individual tasks of an array job by indicating tasks ids to the scancel command: $ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 79265_ [ 49 -99:2%20 ] test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_41 test Simple H foo R 0 :10 1 fnode03 79265_43 test Simple H foo R 0 :10 1 fnode03 79265_45 test Simple H foo R 0 :10 1 fnode03 79265_47 test Simple H foo R 0 :10 1 fnode03 Use the \u2013array option to the squeue command to display one tasks per line: $ squeue --array JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 79265_65 test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_67 test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_69 test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_97 test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_57 test Simple H foo R 0 :47 1 fnode03 79265_59 test Simple H foo R 0 :47 1 fnode03 79265_61 test Simple H foo R 0 :47 1 fnode03 79265_63 test Simple H foo R 0 :47 1 fnode03","title":"Canceling Individual Tasks"},{"location":"user-guide/job-management/array-jobs.html#examples","text":"","title":"Examples"},{"location":"user-guide/job-management/array-jobs.html#use-case-1-1000-computations-same-resource-requirements-different-inputoutput-arguments","text":"Instead of submitting 1000 individual jobs, submit a single array jobs with 1000 tasks: #!/bin/bash #SBATCH --mail-type=NONE #SBATCH --partition=all #SBATCH --time=00:30:00 # Each task takes max 30 minutes #SBATCH --mem-per-cpu=2G # Each task uses max 2G of memory #SBATCH --array=1-1000 # Submit 1000 tasks with task ID 1,2,...,1000. #SBATCH --output=/dev/null #SBATCH --error=/dev/null # The name of the input files must reflect the task ID! ./foo input_data_ ${ SLURM_ARRAY_TASK_ID } .txt > output_ ${ SLURM_ARRAY_TASK_ID } .txt Task with ID 20 will run the program foo with the following arguments: ./foo input_data_20.txt > output_20.txt","title":"Use case 1: 1000 computations, same resource requirements, different input/output arguments"},{"location":"user-guide/job-management/array-jobs.html#use-case-2-read-arguments-from-file","text":"Submit an array job with 1000 tasks. Each task executes the program foo with different arguments: #!/bin/bash #SBATCH --mail-type=NONE #SBATCH --partition=all #SBATCH --time=00:30:00 # Each task takes max 30 minutes #SBATCH --mem-per-cpu=2G # Each task uses max 2G of memory #SBATCH --array=1-1000%20 # Submit 1000 tasks with task ID 1,2,...,1000. Run max 20 tasks concurrently #SBATCH --output=/dev/null #SBATCH --error=/dev/null param_store = $HOME /projects/example/args.txt # args.txt contains 1000 lines with 2 arguments per line. # Line <i> contains arguments for run <i> data_dir = $HOME /projects/example/input_data # Input files are named input_run_0001.txt,...input_run_1000.txt result_dir = $HOME /projects/example/results param_a = $( cat $param_store | awk -v var = $SLURM_ARRAY_TASK_ID 'NR==var {print $1}' ) # Get first argument param_b = $( cat $param_store | awk -v var = $SLURM_ARRAY_TASK_ID 'NR==var {print $2}' ) # Get second argument n = $( printf \"%04d\" $SLURM_ARRAY_TASK_ID ) # Zero pad the task ID to match the numbering of the input files ./foo -c $param_a -p $param_b -i ${ data_dir } /input_run_ ${ n } .txt -o ${ result_dir } /result_run_ ${ n } .txt exit Related pages:","title":"Use case 2: Read arguments from file"},{"location":"user-guide/job-management/checkpointing.html","text":"Checkpointing Description Checkpointing: Saving the state of a computation so that it can be resumed later. On this page we provide some useful information for making your own code checkpoint-able. Currently, we do not provide and support programs and libraries (e.g. BLCR) that allow to checkpoint proprietary (closed source) software. Some applications provide built-in checkpoint/restart mechanisms: Gaussian, Quantum Espresso, CP2K and more. Why checkpointing Imagine a job is already running for several hours when an event occurs which leads to the abortion of the job. Such events can be: Exceeding the time limit Exceeding allocated memory Job gets preempted by another job (gpu partition only!) Node failure Checkpointing a job means that you frequently save the job state so that you can resume computation from the last checkpoint in case of a disastrous event. General recipe for checkpointing your own code Introducing checkpointing logic in your code consists of 3 steps 1. Look for a state file containing a previously saved state. 2. If a state file exists, then restore the state. Else, start from scratch. 3. Periodically save the state. Using UNIX signals You can save the state of your job at specific points in time, after certain iterations, or at whatever event you choose to trigger a state saving. You can also trap specific UNIX signals and act as soon as the signal occurs. The following table lists common signals that you might want to trap in your program: Signal Name Signal Number Description Default Disposition SIGTERM 15 SIGTERM initiates the termination of a process Term - Terminate the process SIGCONT 18 SIGCONT continues a stopped process Cont - Continue the process if stopped SIGUSR1 10 User-defined signals. SIGUSR1/SIGUSR2 are never sent by the kernel Term - Terminate the process SIGUSR2 12 User-defined signals. SIGUSR1/SIGUSR2 are never sent by the kernel Term - Terminate the process kill -l for a list of all supported signals. Note that some signals cannot be trapped, e.g SIGKILL Slurm sends SIGCONT followed by SIGTERM just before a job is canceled. Trapping the signal (e.g. SIGTERM) gives you 60 seconds for housekeeping tasks, e.g. save current state. At the latest after that your job is canceled with SIGKILL. This is true for jobs canceled by the owner using scancel and jobs canceled by Slurm, e.g. because of exceeding time limit. Register a signal handler for a UNIX signal The following examples show how to register a signal handler in different languages, but omit the logic for creating a checkpoint and restart a job from an existing checkpoint. We will provide a working example further down below on this page. Bash #!/bin/bash function signal_handler { # Save program state and exit ( ... ) exit } trap signal_handler TERM ( ... ) C/C++ #include <signal.h> // C #include <csignal> // C++ void signal_handler ( int signal ) { // Save program state and exit (...) exit ( 0 ); } // Register signal handler for SIGTERM signal ( SIGTERM , signal_handler ); // signal_handler: function to handle signal (...) Python #! /usr/bin/env python import signal import sys def signal_handler ( sig , frame ): # Save program state and exit ( ... ) sys . exit ( 0 ) signal . signal ( signal . SIGTERM , signal_handler ) ( ... ) Signaling checkpoint creation without canceling the job You can use a UNIX signal to trigger the creation of a checkpoint of a running job. For example, consider a job that traps SIGUSR1 and saves intermediate results as soon as the signal occurs. You can then create a checkpoint by signaling SIGUSR1 to the job using scancel : scancel --signal = USR1 <jobid> Use \u2013batch option to signal the batch step (shell script), but not any other associated job step (srun) or child processes of the shell script. Use \u2013full option to signal all steps associated with the job including the shell script and its child processes. Using scancel with the \u2013signal option won\u2019t terminate the job or job step.","title":"Checkpointing"},{"location":"user-guide/job-management/checkpointing.html#checkpointing","text":"","title":"Checkpointing"},{"location":"user-guide/job-management/checkpointing.html#description","text":"Checkpointing: Saving the state of a computation so that it can be resumed later. On this page we provide some useful information for making your own code checkpoint-able. Currently, we do not provide and support programs and libraries (e.g. BLCR) that allow to checkpoint proprietary (closed source) software. Some applications provide built-in checkpoint/restart mechanisms: Gaussian, Quantum Espresso, CP2K and more.","title":"Description"},{"location":"user-guide/job-management/checkpointing.html#why-checkpointing","text":"Imagine a job is already running for several hours when an event occurs which leads to the abortion of the job. Such events can be: Exceeding the time limit Exceeding allocated memory Job gets preempted by another job (gpu partition only!) Node failure Checkpointing a job means that you frequently save the job state so that you can resume computation from the last checkpoint in case of a disastrous event.","title":"Why checkpointing"},{"location":"user-guide/job-management/checkpointing.html#general-recipe-for-checkpointing-your-own-code","text":"Introducing checkpointing logic in your code consists of 3 steps 1. Look for a state file containing a previously saved state. 2. If a state file exists, then restore the state. Else, start from scratch. 3. Periodically save the state.","title":"General recipe for checkpointing your own code"},{"location":"user-guide/job-management/checkpointing.html#using-unix-signals","text":"You can save the state of your job at specific points in time, after certain iterations, or at whatever event you choose to trigger a state saving. You can also trap specific UNIX signals and act as soon as the signal occurs. The following table lists common signals that you might want to trap in your program: Signal Name Signal Number Description Default Disposition SIGTERM 15 SIGTERM initiates the termination of a process Term - Terminate the process SIGCONT 18 SIGCONT continues a stopped process Cont - Continue the process if stopped SIGUSR1 10 User-defined signals. SIGUSR1/SIGUSR2 are never sent by the kernel Term - Terminate the process SIGUSR2 12 User-defined signals. SIGUSR1/SIGUSR2 are never sent by the kernel Term - Terminate the process kill -l for a list of all supported signals. Note that some signals cannot be trapped, e.g SIGKILL Slurm sends SIGCONT followed by SIGTERM just before a job is canceled. Trapping the signal (e.g. SIGTERM) gives you 60 seconds for housekeeping tasks, e.g. save current state. At the latest after that your job is canceled with SIGKILL. This is true for jobs canceled by the owner using scancel and jobs canceled by Slurm, e.g. because of exceeding time limit.","title":"Using UNIX signals"},{"location":"user-guide/job-management/checkpointing.html#register-a-signal-handler-for-a-unix-signal","text":"The following examples show how to register a signal handler in different languages, but omit the logic for creating a checkpoint and restart a job from an existing checkpoint. We will provide a working example further down below on this page. Bash #!/bin/bash function signal_handler { # Save program state and exit ( ... ) exit } trap signal_handler TERM ( ... ) C/C++ #include <signal.h> // C #include <csignal> // C++ void signal_handler ( int signal ) { // Save program state and exit (...) exit ( 0 ); } // Register signal handler for SIGTERM signal ( SIGTERM , signal_handler ); // signal_handler: function to handle signal (...) Python #! /usr/bin/env python import signal import sys def signal_handler ( sig , frame ): # Save program state and exit ( ... ) sys . exit ( 0 ) signal . signal ( signal . SIGTERM , signal_handler ) ( ... )","title":"Register a signal handler for a UNIX signal"},{"location":"user-guide/job-management/checkpointing.html#signaling-checkpoint-creation-without-canceling-the-job","text":"You can use a UNIX signal to trigger the creation of a checkpoint of a running job. For example, consider a job that traps SIGUSR1 and saves intermediate results as soon as the signal occurs. You can then create a checkpoint by signaling SIGUSR1 to the job using scancel : scancel --signal = USR1 <jobid> Use \u2013batch option to signal the batch step (shell script), but not any other associated job step (srun) or child processes of the shell script. Use \u2013full option to signal all steps associated with the job including the shell script and its child processes. Using scancel with the \u2013signal option won\u2019t terminate the job or job step.","title":"Signaling checkpoint creation without canceling the job"},{"location":"user-guide/job-management/deleting-jobs.html","text":"Deleting Jobs Description This page provides information on how to delete jobs. scancel Use the scancel command to delete active jobs Syntax scancel [ options ] <jobid> ... Common options --account Restrict the scancel operation to jobs under this charge account --jobname Restrict the scancel operation to jobs with this job name --partition Restrict the scancel operation to jobs in this partition --state Restrict the scancel operation to jobs in this state Examples Delete specific job: scancel 12345678 Delete all running jobs: scancel --state = R","title":"Deleting Jobs"},{"location":"user-guide/job-management/deleting-jobs.html#deleting-jobs","text":"","title":"Deleting Jobs"},{"location":"user-guide/job-management/deleting-jobs.html#description","text":"This page provides information on how to delete jobs.","title":"Description"},{"location":"user-guide/job-management/deleting-jobs.html#scancel","text":"Use the scancel command to delete active jobs Syntax scancel [ options ] <jobid> ... Common options --account Restrict the scancel operation to jobs under this charge account --jobname Restrict the scancel operation to jobs with this job name --partition Restrict the scancel operation to jobs in this partition --state Restrict the scancel operation to jobs in this state","title":"scancel"},{"location":"user-guide/job-management/deleting-jobs.html#examples","text":"Delete specific job: scancel 12345678 Delete all running jobs: scancel --state = R","title":"Examples"},{"location":"user-guide/job-management/gpus.html","text":"GPUs Description This page contains all information you need to submit GPU-jobs successfully on Ubelix. Important Information on GPU Usage Code that runs on the CPU will not magically make use of GPUs by simply submitting a job to the \u2018gpu\u2019 partition! You have to explicitly adapt your code to run on the GPU. Also, code that runs on a GPU will not necessarily run faster than it runs on the CPU. For example, GPUs are not suited to handle tasks that are not highly parallelizable. In other words, you must understand the characteristics of your job, and make sure that you only submit jobs to the \u2018gpu\u2019 partition that can actually benefit from GPUs. Privileged vs. Regular Users We have two categories of users on Ubelix concerning GPU usage: privileged and regular users. Privileged users are users that have invested money into GPUs. Jobs of privileged users can preempt running jobs of regular users on a certain number of GPUs. Unless the option \u2013no-requeue was used when submitting the job, a preempted job is automatically requeued, or canceled otherwise. A requeued job can start on different resources. This behavior is enforced by job QOSs. Whether a job is privileged or not depends on the job QoS that was used to submit the job. Regular users submit their jobs always with the unprivileged QoS \u2018job_gpu\u2019, while privileged users submits their jobs by default with the privileged QoS \u2018job_gpu_ \u2018. Additionally, privileged users can also submit jobs with the unprivileged QoS. A privileged job will cancel a running unprivileged job when the following two criteria are met: There are no free GPU resources of the requested GPU type available. The QoS of the privileged user has not yet reached the maximum number of GPUs allowed to use with this QoS. If an unprivileged job needs to be preempted to make resources available for a privileged job, Slurm will always preempt the youngest running job in the partition. Because an unprivileged job can be preempted at any time, it is important that you checkpoint your jobs. This allows you to resubmit the job and continue execution from the last saved checkpoint. Access to the \u2018gpu\u2019 Partition While the \u2018gpu\u2019 partition is open for everybody, regular users must request access to this partition explicitly before they can submit jobs. You have to request access only once. To do so, simply write an email to hpc@id.unibe.ch and describe in a few words your application. GPU Type Ubelix currently features three types of GPUs Number of Cards GPU 48 Nvidia Geforce GTX 1080 Ti 24 Nvidia Geforce RTX 2080 Ti 16 Nvidia Tesla P100 You must request a GPU using the --gres option: Currently you can specify only two GRES types when requesting GPU resources: gtx1080ti and teslaP100. Requesting type gtx1080ti will allocate GTX or RTX cards to your job. To request a specific Geforce card you must use the \u2013constraint option (see below). --gres = gpu:gtx1080ti:<number_of_gpus> or --gres = gpu:teslaP100:<number_of_gpus> Use the --constraint option to differentiate between Geforce GTX and RTX cards: To request Geforce GTX cards: --gres = gpu:gtx1080ti:<number_of_gpus> --constraint = gtx1080 To request Geforce RTX cards: --gres = gpu:gtx1080ti:<number_of_gpus> --constraint = rtx2080 Job Submission Use the following options to submit a job to the gpu partition using the default job QoS: #SBATCH --partition=gpu #SBATCH --gres=gpu:<type>:<number_of_gpus> Privileged user only: Use the following options to submit a job using the non-privileged QoS: #SBATCH --partition=gpu #SBATCH --qos=job_gpu #SBATCH --gres=gpu:<type>:<number_of_gpus> Use the following option to ensure that the job, if preempted, won\u2019t be requeued but canceled instead: #SBATCH --no-requeue CUDA CUDA versions are now managed through module files. Run module avail to see which versions are available: module avail ( ... ) CUDA/9.0.176 help2man/1.47.4 ( D ) numactl/2.0.11-GCCcore-6.4.0 ( D ) CUDA/9.1.85 hwloc/1.11.3-GCC-5.4.0-2.26 OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1 CUDA/9.2.88 ( D ) hwloc/1.11.5-GCC-6.3.0-2.27 OpenBLAS/0.2.19-GCC-6.3.0-2.27-LAPACK-3.7.0 cuDNN/7.0.5-CUDA-9.0.176 hwloc/1.11.7-GCCcore-6.4.0 OpenBLAS/0.2.20-GCC-6.4.0-2.28 ( D ) cuDNN/7.0.5-CUDA-9.1.85 hwloc/1.11.8-GCCcore-6.4.0 ( D ) OpenMPI/1.10.3-GCC-5.4.0-2.26 cuDNN/7.1.4-CUDA-9.2.88 ( ... ) Run module load to load a specific version of CUDA: module load cuDNN/7.1.4-CUDA-9.2.88 If you need cuDNN you must load the cuDNN module. The appropriate CUDA version is then loaded as a dependency. Further Information CUDA: https://developer.nvidia.com/cuda-zone CUDA C/C++ Basics: http://www.nvidia.com/docs/IO/116711/sc11-cuda-c-basics.pdf Nvidia Geforce GTX 1080 Ti: https://www.nvidia.com/en-us/geforce/products/10series/geforce-gtx-1080-ti Nvidia Tesla P100: http://www.nvidia.com/object/tesla-p100.html","title":"GPUs"},{"location":"user-guide/job-management/gpus.html#gpus","text":"","title":"GPUs"},{"location":"user-guide/job-management/gpus.html#description","text":"This page contains all information you need to submit GPU-jobs successfully on Ubelix.","title":"Description"},{"location":"user-guide/job-management/gpus.html#important-information-on-gpu-usage","text":"Code that runs on the CPU will not magically make use of GPUs by simply submitting a job to the \u2018gpu\u2019 partition! You have to explicitly adapt your code to run on the GPU. Also, code that runs on a GPU will not necessarily run faster than it runs on the CPU. For example, GPUs are not suited to handle tasks that are not highly parallelizable. In other words, you must understand the characteristics of your job, and make sure that you only submit jobs to the \u2018gpu\u2019 partition that can actually benefit from GPUs.","title":"Important Information on GPU Usage"},{"location":"user-guide/job-management/gpus.html#privileged-vs-regular-users","text":"We have two categories of users on Ubelix concerning GPU usage: privileged and regular users. Privileged users are users that have invested money into GPUs. Jobs of privileged users can preempt running jobs of regular users on a certain number of GPUs. Unless the option \u2013no-requeue was used when submitting the job, a preempted job is automatically requeued, or canceled otherwise. A requeued job can start on different resources. This behavior is enforced by job QOSs. Whether a job is privileged or not depends on the job QoS that was used to submit the job. Regular users submit their jobs always with the unprivileged QoS \u2018job_gpu\u2019, while privileged users submits their jobs by default with the privileged QoS \u2018job_gpu_ \u2018. Additionally, privileged users can also submit jobs with the unprivileged QoS. A privileged job will cancel a running unprivileged job when the following two criteria are met: There are no free GPU resources of the requested GPU type available. The QoS of the privileged user has not yet reached the maximum number of GPUs allowed to use with this QoS. If an unprivileged job needs to be preempted to make resources available for a privileged job, Slurm will always preempt the youngest running job in the partition. Because an unprivileged job can be preempted at any time, it is important that you checkpoint your jobs. This allows you to resubmit the job and continue execution from the last saved checkpoint.","title":"Privileged vs. Regular Users"},{"location":"user-guide/job-management/gpus.html#access-to-the-gpu-partition","text":"While the \u2018gpu\u2019 partition is open for everybody, regular users must request access to this partition explicitly before they can submit jobs. You have to request access only once. To do so, simply write an email to hpc@id.unibe.ch and describe in a few words your application.","title":"Access to the 'gpu' Partition"},{"location":"user-guide/job-management/gpus.html#gpu-type","text":"Ubelix currently features three types of GPUs Number of Cards GPU 48 Nvidia Geforce GTX 1080 Ti 24 Nvidia Geforce RTX 2080 Ti 16 Nvidia Tesla P100 You must request a GPU using the --gres option: Currently you can specify only two GRES types when requesting GPU resources: gtx1080ti and teslaP100. Requesting type gtx1080ti will allocate GTX or RTX cards to your job. To request a specific Geforce card you must use the \u2013constraint option (see below). --gres = gpu:gtx1080ti:<number_of_gpus> or --gres = gpu:teslaP100:<number_of_gpus> Use the --constraint option to differentiate between Geforce GTX and RTX cards: To request Geforce GTX cards: --gres = gpu:gtx1080ti:<number_of_gpus> --constraint = gtx1080 To request Geforce RTX cards: --gres = gpu:gtx1080ti:<number_of_gpus> --constraint = rtx2080","title":"GPU Type"},{"location":"user-guide/job-management/gpus.html#job-submission","text":"Use the following options to submit a job to the gpu partition using the default job QoS: #SBATCH --partition=gpu #SBATCH --gres=gpu:<type>:<number_of_gpus> Privileged user only: Use the following options to submit a job using the non-privileged QoS: #SBATCH --partition=gpu #SBATCH --qos=job_gpu #SBATCH --gres=gpu:<type>:<number_of_gpus> Use the following option to ensure that the job, if preempted, won\u2019t be requeued but canceled instead: #SBATCH --no-requeue","title":"Job Submission"},{"location":"user-guide/job-management/gpus.html#cuda","text":"CUDA versions are now managed through module files. Run module avail to see which versions are available: module avail ( ... ) CUDA/9.0.176 help2man/1.47.4 ( D ) numactl/2.0.11-GCCcore-6.4.0 ( D ) CUDA/9.1.85 hwloc/1.11.3-GCC-5.4.0-2.26 OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1 CUDA/9.2.88 ( D ) hwloc/1.11.5-GCC-6.3.0-2.27 OpenBLAS/0.2.19-GCC-6.3.0-2.27-LAPACK-3.7.0 cuDNN/7.0.5-CUDA-9.0.176 hwloc/1.11.7-GCCcore-6.4.0 OpenBLAS/0.2.20-GCC-6.4.0-2.28 ( D ) cuDNN/7.0.5-CUDA-9.1.85 hwloc/1.11.8-GCCcore-6.4.0 ( D ) OpenMPI/1.10.3-GCC-5.4.0-2.26 cuDNN/7.1.4-CUDA-9.2.88 ( ... ) Run module load to load a specific version of CUDA: module load cuDNN/7.1.4-CUDA-9.2.88 If you need cuDNN you must load the cuDNN module. The appropriate CUDA version is then loaded as a dependency.","title":"CUDA"},{"location":"user-guide/job-management/gpus.html#further-information","text":"CUDA: https://developer.nvidia.com/cuda-zone CUDA C/C++ Basics: http://www.nvidia.com/docs/IO/116711/sc11-cuda-c-basics.pdf Nvidia Geforce GTX 1080 Ti: https://www.nvidia.com/en-us/geforce/products/10series/geforce-gtx-1080-ti Nvidia Tesla P100: http://www.nvidia.com/object/tesla-p100.html","title":"Further Information"},{"location":"user-guide/job-management/interactive-jobs.html","text":"Interactive Jobs Description This page describes how to request resources for interactive jobs and how to use the allocated resources when working interactively. Requesting resources Use salloc [options] to allocate resources for an interactive job. The command will block until sufficient resources are available. After the resources have been successfully allocated to your job your are still located on the submit host. Applications started now will not run within the allocation! bash-4.2$ salloc --nodes = 1 --ntasks-per-node = 4 --mem-per-cpu = 2G --time = 01 :00:00 salloc: Pending job allocation 63752579 salloc: job 63752579 queued and waiting for resources salloc: job 63752579 has been allocated resources salloc: Granted job allocation 63752579 bash-4.2$ hostname submit01.ubelix.unibe.ch Use an allocation Use srun [options] \u2013jobid= to start a job step under an existing job/allocation. bash-4.2$ srun -n1 --jobid = 63752579 hostname knode02 To work interactively on an allocated compute node: bash-4.2$ srun -n1 --jobid = 63752579 --pty bash bash-4.2$ hostname knode02 X11 Forwarding Requirements You must login to UBELIX with X11 forwarding enabled: ssh -X @submit.unibe.ch . Make this the default with ForwardX11 yes in ~/.ssh/config . Password-less communication between all nodes within UBELIX. In order to make this possible, generate a new SSH key (without passphrase) on the login node (submit) and add the public key to ~/.ssh/authorized_keys : ssh-keygen -t rsa -b 4096 cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys A X-Server on your local workstation, e.g. MAC: Xquartz (X11 no longer included in macOS) Windows: MobaXTerm or Xming DO NOT reuse an existing SSH key for this purpose, i.e. do not copy an existing private key from your local machine to UBELIX. With all the requirements in place you can now submit an interactive job and export an X11 display on the allocated compute node, e.g: srun -n 1 --pty --x11 xterm You can also use X11 forwarding with non interactive jobs adding the option #SBATCH --x11 in your job script and using again srun --x11 to launch your application.","title":"Interactive Jobs"},{"location":"user-guide/job-management/interactive-jobs.html#interactive-jobs","text":"","title":"Interactive Jobs"},{"location":"user-guide/job-management/interactive-jobs.html#description","text":"This page describes how to request resources for interactive jobs and how to use the allocated resources when working interactively.","title":"Description"},{"location":"user-guide/job-management/interactive-jobs.html#requesting-resources","text":"Use salloc [options] to allocate resources for an interactive job. The command will block until sufficient resources are available. After the resources have been successfully allocated to your job your are still located on the submit host. Applications started now will not run within the allocation! bash-4.2$ salloc --nodes = 1 --ntasks-per-node = 4 --mem-per-cpu = 2G --time = 01 :00:00 salloc: Pending job allocation 63752579 salloc: job 63752579 queued and waiting for resources salloc: job 63752579 has been allocated resources salloc: Granted job allocation 63752579 bash-4.2$ hostname submit01.ubelix.unibe.ch","title":"Requesting resources"},{"location":"user-guide/job-management/interactive-jobs.html#use-an-allocation","text":"Use srun [options] \u2013jobid= to start a job step under an existing job/allocation. bash-4.2$ srun -n1 --jobid = 63752579 hostname knode02 To work interactively on an allocated compute node: bash-4.2$ srun -n1 --jobid = 63752579 --pty bash bash-4.2$ hostname knode02","title":"Use an allocation"},{"location":"user-guide/job-management/interactive-jobs.html#x11-forwarding","text":"Requirements You must login to UBELIX with X11 forwarding enabled: ssh -X @submit.unibe.ch . Make this the default with ForwardX11 yes in ~/.ssh/config . Password-less communication between all nodes within UBELIX. In order to make this possible, generate a new SSH key (without passphrase) on the login node (submit) and add the public key to ~/.ssh/authorized_keys : ssh-keygen -t rsa -b 4096 cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys A X-Server on your local workstation, e.g. MAC: Xquartz (X11 no longer included in macOS) Windows: MobaXTerm or Xming DO NOT reuse an existing SSH key for this purpose, i.e. do not copy an existing private key from your local machine to UBELIX. With all the requirements in place you can now submit an interactive job and export an X11 display on the allocated compute node, e.g: srun -n 1 --pty --x11 xterm You can also use X11 forwarding with non interactive jobs adding the option #SBATCH --x11 in your job script and using again srun --x11 to launch your application.","title":"X11 Forwarding"},{"location":"user-guide/job-management/investigating-job-failure.html","text":"Investigating a Job Failure Description This page provides some useful information on investigating a job failure. It is important to collect error/output messages either by writing such information to the default location or by specifying specific locations using the \u2013error/\u2013output option. Do not redirect the error/output stream to /dev/null unless you know what you are doing. Error and output messages are the starting point for investigating a job failure. A job may fail due to a hardware failure on a node involved in the computation, a failure of a Slurm daemon, exceeding a resource limit, or a software specific error. The most common causes are exceeding resource limits and software-specific errors which we will discuss here. Exceeding Resource Limits Each partition limits the maximal allowed runtime of a job and provides default values for the estimated job runtime and memory usage per core. A job should request appropriate values for those resources using the \u2013time and \u2013mem-per-core options. A job is killed if one of these limits is exceeded. In both cases, the error file provides appropriate information: Time limit: ( ... ) slurmstepd: error: *** JOB 41239 ON fnode01 CANCELLED AT 2016 -11-30T11:22:57 DUE TO TIME LIMIT *** ( ... ) `````` Memory limit: ``` Bash ( ... ) slurmstepd: error: Job 41176 exceeded memory limit ( 3940736 > 2068480 ) , being killed slurmstepd: error: Exceeded job memory limit slurmstepd: error: *** JOB 41176 ON fnode01 CANCELLED AT 2016 -11-30T10:21:37 *** ( ... ) Software Errors The exit code of a job is captured by Slurm and saved as part of the job record. For sbatch jobs the exit code of the batch script is captured. For srun, the exit code will be the return value of the executed command. Any non-zero exit code is considered a job failure, and results in job state of FAILED. When a signal was responsible for a job/step termination, the signal number will also be captured, and displayed after the exit code (separated by a colon). Depending on the execution order of the commands in the batch script, it is possible that a specific command fails but the batch script will return zero indicating success. Consider the following simplified example: fail.R var<-sq ( 1 ,1000000000 ) job.sbatch #!/bin/bash # Slurm options #SBATCH --mail-user=nico.faerber@id.unibe.ch #SBATCH --mail-type=begin,end,fail #SBATCH --job-name=\"Simple Example\" #SBATCH --time=00:05:00 #SBATCH --mem-per-cpu=2G # Put your code below this line R CMD BATCH --vanilla fail.R echo \"Script finished\" The exit code and state wrongly indicates that the job finished successfully: $ sbatch job_slurm.sh Submitted batch job 41585 $ sacct -j 41585 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 41585 Simple E + all id 1 COMPLETED 0 :0 41585 .batch batch id 1 COMPLETED 0 :0 Only the R-specific output file shows the error: fail.Rout ( ... ) > var<-sq ( 1 ,1000000000 ) Error: could not find function \"sq\" Execution halted You can bypass this problem by exiting with a proper exit code as soon as the command failed: jobsbatch #!/bin/bash # Slurm options #SBATCH --mail-user=nico.faerber@id.unibe.ch #SBATCH --mail-type=begin,end,fail #SBATCH --job-name=\"Simple Example\" #SBATCH --time=00:05:00 #SBATCH --mem-per-cpu=2G # Put your code below this line R CMD BATCH --vanilla fail.R || exit 91 echo \"Script finished\" Now, the exit code and state matches the true outcome: $ sbatch job_slurm.sh Submitted batch job 41925 $ sacct -j 41925 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 41925 Simple E + all id 1 FAILED 91 :0 41925 .batch batch id 1 FAILED 91 :0 Always check application-specifc output files for error messages.","title":"Investigating a Job Failure"},{"location":"user-guide/job-management/investigating-job-failure.html#investigating-a-job-failure","text":"","title":"Investigating a Job Failure"},{"location":"user-guide/job-management/investigating-job-failure.html#description","text":"This page provides some useful information on investigating a job failure. It is important to collect error/output messages either by writing such information to the default location or by specifying specific locations using the \u2013error/\u2013output option. Do not redirect the error/output stream to /dev/null unless you know what you are doing. Error and output messages are the starting point for investigating a job failure. A job may fail due to a hardware failure on a node involved in the computation, a failure of a Slurm daemon, exceeding a resource limit, or a software specific error. The most common causes are exceeding resource limits and software-specific errors which we will discuss here.","title":"Description"},{"location":"user-guide/job-management/investigating-job-failure.html#exceeding-resource-limits","text":"Each partition limits the maximal allowed runtime of a job and provides default values for the estimated job runtime and memory usage per core. A job should request appropriate values for those resources using the \u2013time and \u2013mem-per-core options. A job is killed if one of these limits is exceeded. In both cases, the error file provides appropriate information: Time limit: ( ... ) slurmstepd: error: *** JOB 41239 ON fnode01 CANCELLED AT 2016 -11-30T11:22:57 DUE TO TIME LIMIT *** ( ... ) `````` Memory limit: ``` Bash ( ... ) slurmstepd: error: Job 41176 exceeded memory limit ( 3940736 > 2068480 ) , being killed slurmstepd: error: Exceeded job memory limit slurmstepd: error: *** JOB 41176 ON fnode01 CANCELLED AT 2016 -11-30T10:21:37 *** ( ... )","title":"Exceeding Resource Limits"},{"location":"user-guide/job-management/investigating-job-failure.html#software-errors","text":"The exit code of a job is captured by Slurm and saved as part of the job record. For sbatch jobs the exit code of the batch script is captured. For srun, the exit code will be the return value of the executed command. Any non-zero exit code is considered a job failure, and results in job state of FAILED. When a signal was responsible for a job/step termination, the signal number will also be captured, and displayed after the exit code (separated by a colon). Depending on the execution order of the commands in the batch script, it is possible that a specific command fails but the batch script will return zero indicating success. Consider the following simplified example: fail.R var<-sq ( 1 ,1000000000 ) job.sbatch #!/bin/bash # Slurm options #SBATCH --mail-user=nico.faerber@id.unibe.ch #SBATCH --mail-type=begin,end,fail #SBATCH --job-name=\"Simple Example\" #SBATCH --time=00:05:00 #SBATCH --mem-per-cpu=2G # Put your code below this line R CMD BATCH --vanilla fail.R echo \"Script finished\" The exit code and state wrongly indicates that the job finished successfully: $ sbatch job_slurm.sh Submitted batch job 41585 $ sacct -j 41585 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 41585 Simple E + all id 1 COMPLETED 0 :0 41585 .batch batch id 1 COMPLETED 0 :0 Only the R-specific output file shows the error: fail.Rout ( ... ) > var<-sq ( 1 ,1000000000 ) Error: could not find function \"sq\" Execution halted You can bypass this problem by exiting with a proper exit code as soon as the command failed: jobsbatch #!/bin/bash # Slurm options #SBATCH --mail-user=nico.faerber@id.unibe.ch #SBATCH --mail-type=begin,end,fail #SBATCH --job-name=\"Simple Example\" #SBATCH --time=00:05:00 #SBATCH --mem-per-cpu=2G # Put your code below this line R CMD BATCH --vanilla fail.R || exit 91 echo \"Script finished\" Now, the exit code and state matches the true outcome: $ sbatch job_slurm.sh Submitted batch job 41925 $ sacct -j 41925 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 41925 Simple E + all id 1 FAILED 91 :0 41925 .batch batch id 1 FAILED 91 :0 Always check application-specifc output files for error messages.","title":"Software Errors"},{"location":"user-guide/job-management/monitoring-jobs.html","text":"Monitoring Jobs Description This page provides information about monitoring user jobs. Different Slurm commands provide information about jobs/job steps on different levels. The command squeue provides high-level information about jobs in the Slurm scheduling queue (state information, allocated resources, runtime, \u2026). The command sstat provides detailed usage information about running jobs, and sacct provides accounting information about active and completed (past) jobs. The command scontrol provides even more detailed information about jobs and job steps. The output format of most commands is highly configurable to your needs. Look for the \u2013format or \u2013Format options. Most command options support a short form as well as a long form (e.g. -u , and \u2013user= ). Because few options only support the long form, we will consistently use the long form throughout this documentation. squeue Use the squeue command to get a high-level overview of all active (running and pending) jobs in the cluster. Syntax squeue [ options ] Common options --user = <user [ ,user [ ,... ]] > Request jobs from a comma separated list of users. --jobs = <job_id [ ,job_id [ ,... ]] > Request specific jobs to be displayed --partition = <part [ ,part [ ,... ]] > Request jobs to be displayed from a comma separated list of partitions --states = <state [ ,state [ ,... ]] > Display jobs in specific states. Comma separated list or \"all\" . Default: \"PD,R,CG\" The default output format is as follows: JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) where JOBID Job or step ID. For array jobs, the job ID format will be of the form <job_id>_<index> PARTITION Partition of the job/step NAME Name of the job/step USER Owner of the job/step ST State of the job/step. See below for a description of the most common states TIME Time used by the job/step. Format is days-hours:minutes:seconds ( days,hours only printed as needed ) NODES Number of nodes allocated to the job or the minimum amount of nodes required by a pending job NODELIST ( REASON ) For pending jobs: Reason why pending. For failed jobs: Reason why failed. For all other job states: List of allocated nodes. See below for a list of the most common reason codes You can easily tailor the output format of squeue to your own needs. Use the \u2013format (-o) or \u2013Format (-O) options to request a comma separated list of job information to be displayed. See the man page for more information: man squeue Job States During its lifetime, a job passes through several states. The most common states are PENDING, RUNNING, SUSPENDED, COMPLETING, and COMPLETED. PD Pending. Job is waiting for resource allocation R Running. Job has an allocation and is running S Suspended. Execution has been suspended and resources have been released for other jobs CA Cancelled. Job was explicitly cancelled by the user or the system administrator CG Completing. Job is in the process of completing. Some processes on some nodes may still be active CD Completed. Job has terminated all processes on all nodes with an exit code of zero F Failed. Job has terminated with non-zero exit code or other failure condition Why is my job still pending? The REASON column of the squeue output gives you a hint why your job is not running. (Resources) The job is waiting for resources to become available so that the jobs resource request can be fulfilled. (Priority) The job is not allowed to run because at least one higher prioritized job is waiting for resources. (Dependency) The job is waiting for another job to finish first (\u2013dependency=\u2026 option). (DependencyNeverSatisfied) The job is waiting for a dependency that can never be satisfied. Such a job will remain pending forever. Please cancel such jobs. (QOSMaxCpuPerUserLimit) The job is not allowed to start because your currently running jobs consume all allowed CPU resources for your user in a specific partition. Wait for jobs to finish. (AssocGrpCpuLimit) dito. (AssocGrpJobsLimit) The job is not allowed to start because you have reached the maximum of allowed running jobs for your user in a specific partition. Wait for jobs to finish. (ReqNodeNotAvail, UnavailableNodes:\u2026) Some node required by the job is currently not available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Most probably there is an active reservation for all nodes due to an upcoming maintenance downtime and your job is not able to finish before the start of the downtime. Another reason why you should specify the duration of a job (\u2013time) as accurately as possible. Your job will start after the downtime has finished. You can list all active reservations using scontrol show reservation . Why can\u2019t I submit further jobs? sbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user\u2019s size and/or time limits) \u2026 means that you have reached the maximum of allowed jobs to be submitted to a specific partition. Examples List all currently running jobs of user foo: squeue --user = foo --states = PD,R List all currently running jobs of user foo in partition bar : squeue --user = foo --partition = bar --states = R scontrol Use the scontrol command to show more detailed information about a job Syntax scontrol [ options ] [ command ] Examples Show detailed information about job with ID 500: scontrol show jobid 500 Show even more detailed information about job with ID 500 (including the jobscript): scontrol -dd show jobid 500 sacct Use the sacct command to query information about past jobs Syntax sacct [ options ] Common options --endtime = end_time Select jobs in any state before the specified time. --starttime = start_time Select jobs in any state after the specified time. --state = state [ ,state [ ,... ]] Select jobs based on their state during the time period given. By default, the start and end time will be the current time when the --state option is specified, and hence only currently running jobs will be displayed.","title":"Monitoring Jobs"},{"location":"user-guide/job-management/monitoring-jobs.html#monitoring-jobs","text":"","title":"Monitoring Jobs"},{"location":"user-guide/job-management/monitoring-jobs.html#description","text":"This page provides information about monitoring user jobs. Different Slurm commands provide information about jobs/job steps on different levels. The command squeue provides high-level information about jobs in the Slurm scheduling queue (state information, allocated resources, runtime, \u2026). The command sstat provides detailed usage information about running jobs, and sacct provides accounting information about active and completed (past) jobs. The command scontrol provides even more detailed information about jobs and job steps. The output format of most commands is highly configurable to your needs. Look for the \u2013format or \u2013Format options. Most command options support a short form as well as a long form (e.g. -u , and \u2013user= ). Because few options only support the long form, we will consistently use the long form throughout this documentation.","title":"Description"},{"location":"user-guide/job-management/monitoring-jobs.html#squeue","text":"Use the squeue command to get a high-level overview of all active (running and pending) jobs in the cluster. Syntax squeue [ options ] Common options --user = <user [ ,user [ ,... ]] > Request jobs from a comma separated list of users. --jobs = <job_id [ ,job_id [ ,... ]] > Request specific jobs to be displayed --partition = <part [ ,part [ ,... ]] > Request jobs to be displayed from a comma separated list of partitions --states = <state [ ,state [ ,... ]] > Display jobs in specific states. Comma separated list or \"all\" . Default: \"PD,R,CG\" The default output format is as follows: JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) where JOBID Job or step ID. For array jobs, the job ID format will be of the form <job_id>_<index> PARTITION Partition of the job/step NAME Name of the job/step USER Owner of the job/step ST State of the job/step. See below for a description of the most common states TIME Time used by the job/step. Format is days-hours:minutes:seconds ( days,hours only printed as needed ) NODES Number of nodes allocated to the job or the minimum amount of nodes required by a pending job NODELIST ( REASON ) For pending jobs: Reason why pending. For failed jobs: Reason why failed. For all other job states: List of allocated nodes. See below for a list of the most common reason codes You can easily tailor the output format of squeue to your own needs. Use the \u2013format (-o) or \u2013Format (-O) options to request a comma separated list of job information to be displayed. See the man page for more information: man squeue","title":"squeue"},{"location":"user-guide/job-management/monitoring-jobs.html#job-states","text":"During its lifetime, a job passes through several states. The most common states are PENDING, RUNNING, SUSPENDED, COMPLETING, and COMPLETED. PD Pending. Job is waiting for resource allocation R Running. Job has an allocation and is running S Suspended. Execution has been suspended and resources have been released for other jobs CA Cancelled. Job was explicitly cancelled by the user or the system administrator CG Completing. Job is in the process of completing. Some processes on some nodes may still be active CD Completed. Job has terminated all processes on all nodes with an exit code of zero F Failed. Job has terminated with non-zero exit code or other failure condition","title":"Job States"},{"location":"user-guide/job-management/monitoring-jobs.html#why-is-my-job-still-pending","text":"The REASON column of the squeue output gives you a hint why your job is not running. (Resources) The job is waiting for resources to become available so that the jobs resource request can be fulfilled. (Priority) The job is not allowed to run because at least one higher prioritized job is waiting for resources. (Dependency) The job is waiting for another job to finish first (\u2013dependency=\u2026 option). (DependencyNeverSatisfied) The job is waiting for a dependency that can never be satisfied. Such a job will remain pending forever. Please cancel such jobs. (QOSMaxCpuPerUserLimit) The job is not allowed to start because your currently running jobs consume all allowed CPU resources for your user in a specific partition. Wait for jobs to finish. (AssocGrpCpuLimit) dito. (AssocGrpJobsLimit) The job is not allowed to start because you have reached the maximum of allowed running jobs for your user in a specific partition. Wait for jobs to finish. (ReqNodeNotAvail, UnavailableNodes:\u2026) Some node required by the job is currently not available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Most probably there is an active reservation for all nodes due to an upcoming maintenance downtime and your job is not able to finish before the start of the downtime. Another reason why you should specify the duration of a job (\u2013time) as accurately as possible. Your job will start after the downtime has finished. You can list all active reservations using scontrol show reservation .","title":"Why is my job still pending?"},{"location":"user-guide/job-management/monitoring-jobs.html#why-cant-i-submit-further-jobs","text":"sbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user\u2019s size and/or time limits) \u2026 means that you have reached the maximum of allowed jobs to be submitted to a specific partition.","title":"Why can't I submit further jobs?"},{"location":"user-guide/job-management/monitoring-jobs.html#examples","text":"List all currently running jobs of user foo: squeue --user = foo --states = PD,R List all currently running jobs of user foo in partition bar : squeue --user = foo --partition = bar --states = R","title":"Examples"},{"location":"user-guide/job-management/monitoring-jobs.html#scontrol","text":"Use the scontrol command to show more detailed information about a job Syntax scontrol [ options ] [ command ]","title":"scontrol"},{"location":"user-guide/job-management/monitoring-jobs.html#examples_1","text":"Show detailed information about job with ID 500: scontrol show jobid 500 Show even more detailed information about job with ID 500 (including the jobscript): scontrol -dd show jobid 500","title":"Examples"},{"location":"user-guide/job-management/monitoring-jobs.html#sacct","text":"Use the sacct command to query information about past jobs Syntax sacct [ options ] Common options --endtime = end_time Select jobs in any state before the specified time. --starttime = start_time Select jobs in any state after the specified time. --state = state [ ,state [ ,... ]] Select jobs based on their state during the time period given. By default, the start and end time will be the current time when the --state option is specified, and hence only currently running jobs will be displayed.","title":"sacct"},{"location":"user-guide/job-management/submission.html","text":"Job Submission Description This section describes the interaction with the resource manager. The subchapters contain information about submitting jobs to the cluster, monitoring active jobs and retrieving useful information about resource usage. A cluster is a set of connected computers that work together to solve computational tasks (user jobs) and presents itself to the user as a single system. For the resources of a cluster (e.g. CPUs, GPUs, memory) to be used efficiently, a resource manager (also called workload manager or batch-queuing system) is vital. While there are many different resource managers available, the resource manager of choice on UBELIX is SLURM . After submitting a job to the cluster, SLURM will try to fulfill the job\u2019s resource request by allocating resources to the job. If the requested resources are already available, the job can start immediately. Otherwise, the start of the job is delayed (pending) until enough resources are available. SLURM allows you to monitor active (pending, running) jobs and to retrieve statistics about finished jobs e.g. (peak CPU usage). The subchapters describe individual aspects of SLURM. This page describes the job submission process with Slurm. It is important to collect error/output messages either by writing such information to the default location or by specifying specific locations using the \u2013error/\u2013output option. Do not redirect the error/output stream to /dev/null unless you know what you are doing. Error and output messages are the starting point for investigating a job failure. For backfilling performance and hence to maximize job throughput it is crucial to submit array jobs (collection of similar jobs) instead of submitting the same job repeatedly. Resource Allocation Every job submission starts with a resources allocation (nodes, cores, memory). An allocation is valid for a specific amount of time, and can be created using the salloc, sbatch or srun commands. Whereas salloc and sbatch only create resource allocations, srun launches parallel tasks within such a resource allocation, or implicitly creates an allocation if not started within one. The usual procedure is to combine resource requests and task execution (job steps) in a single batch script (job script) and then submit the script using the sbatch command. Most command options support a short form as well as a long form (e.g. -u , and \u2013user= ). Because few options only support the long form, we will consistently use the long form throughout this documentation. Some options have default values if not specified: The \u2013time option has partition-specific default values (see scontrol show partition ). The \u2013mem-per-cpu option has a global default value of 2048MB. The default partition is the all partition. To select another partition one must use the \u2013partition option, e.g. \u2013partition=long . Important information for investors regarding account selection Investors have two different accounts for accounting purposes. The investor account (increased privileges) is used automatically when using the empi partition (\u2013partition=empi). To use another partition, the user must explicitly select the group account (e.g \u2013account=dcb). To display your accounts, use: sacctmgr show assoc where user= format=user,account%20,partition. sbatch The sbatch command is used to submit a job script for later execution. It is the most common way to submit a job to the cluster due to its reusability. Slurm options are usually embedded in a job script prefixed by \u2018#SBATCH\u2019 directives. Slurm options specified as command line options overwrite corresponding options embedded in the job script Syntax sbatch [ options ] script [ args... ] Job Script Usually a job script consists of two parts. The first part is optional but highly recommended: Slurm-specific options used by the scheduler to manage the resources (e.g. memory) and configure the job environment Job-specific shell commands The job script acts as a wrapper for your actual job. Command-line options can still be used to overwrite embedded options. Although you can specify all Slurm options on the command-line, we encourage you, for clarity and reusability, to embed Slurm options in the job script You can find a template for a job script under /storage/software/workshop/slurm_template.sh. Copy the template to your home directory and adapt it to your needs: cp /gpfs/software/workshop/job_script_template.sh $HOME Options Option Description Example Default Value \u2013mail-user Mail address to contact job owner. Must specify a valid email address! \u2013mail-user=foo.bar@baz.unibe.ch \u2013mail-type When to notify a job owner: none, all, begin, end, fail, requeue, array_tasks \u2013mail-type=end,fail \u2013account Which account to charge. Regular users don\u2019t need to specify this option. For users with enhanced privileges on the empi partition, see here The users default account. \u2013job-name Specify a job name \u2013job-name=\u201dSimple Matlab\u201d \u2013time Expected runtime of the job. Format: dd-hh:mm:ss \u2013time=12:00:00 \u2013time=2-06:00:00 Partition-specific, see scontrol show partition \u2013mem-per-cpu Minimum memory required per allocated CPU in megabytes. Different units can be specified using the suffix [K|M|G] \u2013mem-per-cpu=2G 2048 MB \u2013tmp Specify the amount of disk space that must be available on the compute node(s). The local scratch space for the job is referenced by the variable TMPDIR . Default units are megabytes. Different units can be specified using the suffix [K|M|G|T]. \u2013tmp=8G \u2013tmp=2048 \u2013ntasks Number of tasks (processes). Used for MPI jobs that may run distributed on multiple compute nodes \u2013ntasks=4 1 or to match \u2013nodes, \u2013tasks-per-node if specified \u2013nodes Request a certain number of nodes \u2013nodes=2 1 or to match \u2013ntasks, \u2013tasks-per-node if specified \u2013ntasks-per-node Specifies how many tasks will run on each allocated node. Meant to be used with \u2013nodes . If used with the \u2013ntasks option, the \u2013ntasks option will take precedence and the \u2013ntasks-per-node will be treated as a maximum count of tasks per node. \u2013ntasks-per-node=2 \u2013cpus-per-task Number of CPUs per taks (threads). Used for shared memory jobs that run locally on a single compute node \u2013cpus-per-task=4 1 \u2013array Submit an array job. Use \u201c%\u201d to specify the max number of tasks allowed to run concurrently. \u2013array=1,4,16-32:4 \u2013array=1-100%20 \u2013workdir Set the current working directory. All relative paths used in the job script are relative to this directory The directory from where the sbatch command was executed \u2013output Redirect standard output. All directories specified in the path must exist before the job starts! By default stderr and stdout are connected to the same file slurm-%j.out, where \u2018%j\u2019 is replaced with the job allocation number. \u2013error Redirect standard error. All directories specified in the path must exist before the job starts! By default stderr and stdout are connected to the same file slurm-%j.out, where \u2018%j\u2019 is replaced with the job allocation number. \u2013partition The \u201call\u201d partition is the default partition. A different partition must be requested with the \u2013partition option! \u2013partition=long \u2013partition=debug Default partition: all \u2013dependency Defer the start of this job until the specified dependencies have been satisfied. See man sbatch for a description of all valid dependency types \u2013dependency=afterany:11908 \u2013hold Submit job in hold state. Job is not allowed to run until explicitly released \u2013immediate Only submit the job if all requested resources are immediately available \u2013exclusive Use the compute node(s) exclusively, i.e. do not share nodes with other jobs. CAUTION: Only use this option if you are an experienced user, and you really understand the implications of this feature. If used improperly, the use of this option can lead to a massive waste of computational resources \u2013constraint Request nodes with certain features. This option allows you to request a homogeneous pool of nodes for you MPI job \u2013constraint=ivy (all, long partition) \u2013constraint=sandy (all partition only) \u2013constraint=broadwell (all, empi partition) \u2013parsable Print the job id only Default output: \u201cSubmitted batch job \u201c \u2013test-only Validate the batch script and return the estimated start time considering the current cluster state Example jobs.sh #!/bin/bash #SBATCH --mail-type=none #SBATCH --job-name=\"Serial example\" #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=4G # Your code below this line ./calc_mat.sh Submit the job script: sbatch job.sh Submitted batch job 30215045 See below for more examples salloc The salloc command is used to allocate resources (e.g. nodes), possibly with a set of constraints (e.g. number of processor per node) for later utilization. It is typically used to allocate resources and spawn a shell, in which the srun command is used to launch parallel tasks. Syntax salloc [ options ] [ <command> [ args... ]] Example bash$ salloc -N 2 sh salloc: Granted job allocation 247 sh$ module load openmpi/1.10.2-gcc sh$ srun --mpi = pmi2 mpi_hello_world Hello, World. I am 1 of 2 running on knlnode03.ubelix.unibe.ch Hello, World. I am 0 of 2 running on knlnode02.ubelix.unibe.ch sh$ exit salloc: Relinquishing job allocation 247 srun The srun command creates job steps. One or multiple srun invocations are usually used from within an existing resource allocation. Thereby, a job step can utilize all resources allocated to the job, or utilize only a subset of the resource allocation. Multiple job steps can run sequentially in the order defined in the batch script or run in parallel, but can together never utilize more resources than provided by the allocation. Do not submit a job script using srun . Embedded Slurm options (#SBATCH) are not parsed by srun . Syntax srun [options] executable [args...] When do I use srun in my job script? Use srun in your job script to start MPI tasks run multiple jobs (serial or parallel jobs) simultaneously within an allocation Example Run MPI task: #!/bin/bash #SBATCH --mail-type=none #SBATCH --job-name=\"Open MPI example\" #SBATCH --nodes=2 #SBATCH --ntasks-per-node=16 #SBATCH --mem-per-cpu=2G #SBATCH --time=06:00:00 # Your code below this line module load openmpi/1.10.2-gcc srun --mpi = pmi2 mpi_bin Run two jobs simultaneously: #!/bin/bash #SBATCH --mail-type=none #SBATCH --job-name=\"Open MPI example\" #SBATCH --nodes=2 #SBATCH --ntasks-per-node=2 #SBATCH --mem-per-cpu=2G #SBATCH --time=06:00:00 # Your code below this line # Each job should run on a different node and starts two processes on that node: srun --nodes = 1 --ntasks = 2 job01.sh & srun --nodes = 1 --ntasks = 2 job02.sh & # Wait for both commands to finish. This is important when running bash commands in the background (using &)! wait Requesting a Partition (Queue) The default partition is the \u2018all\u2019 partition. If you do not explicitly request a partition, your job will run in the default partition. To request a different partition, you must use the \u2013partition option: #SBATCH --partition=long See here for a list of availalble partitions. Requesting an Account Accounts are used for accounting purposes. Every user has a default account that is used unless a different account is specified using the \u2013account option. Regular users only have a single account and can thus not request a different account. The default account for regular user is named after their group (e.g. dcb): $ sacctmgr show user foo User Def Acct Admin ---------- ---------- --------- foo bar None The remaining information provided in this section applies only to users with enhanced privileges on the empi partition. Users with enhanced privileges have an additional account for the empi partition. This additional account is set as their default account, which means they don\u2019t have to specify an account when submitting to the empi partition (\u2013partition= empi ), but must specify their \u201cgroup account\u201d (\u2013account= ) for submitting to any other partition (e.g all ). If a wrong account/partition combination is requested, you will experience the following error message: sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified Example Here are some examples on the usage of the \u2013account and \u2013partition options. Regular users: Submit to the \"all\" partition: No options required! Submit to any other partition: --partition = <partname> e.g. --partition = empi Users with enhanced privileges on the empi partition: Submit to the \"all\" partition: --account = <grpname> e.g. --account = dcb Submit to the \"empi\" partition: --partition = empi Submit to any other partition: --account = <grpname> e.g. --account = dcb --partition = <partname> e.g. --partition = long Parallel Jobs A parallel job either runs on multiple CPU cores on a single compute node, or on multiple CPU cores distributed over multiple compute nodes. With Slurm you can request tasks, and CPUs per task. A task corresponds to a process that may be made up of multiple threads (CPUs per task). Different tasks of a job allocation may run on different compute nodes, while all threads that belong to a certain process execute on the same node. For shared memory jobs (SMP, parallel jobs that run on a single compute node) one would request a single task and a certain number of CPUs for that task: #SBATCH --cpus-per-task=16 For MPI jobs (parallel jobs that may be distributed over multiple compute nodes) one would request a certain number of tasks and certain number of nodes: #SBATCH --nodes=2 #SBATCH --ntasks-per-node=16 The requested node,task, and CPU resources must match! For example, you cannot request one node (\u2013nodes=1) and more tasks (\u2013ntasks-per-node) than CPU cores are available on a single node in the partition. In such a case you will experience an error message: sbatch: error: Batch job submission failed: Requested node configuration is not available. Open MPI Open MPI was compiled with Slurm support, which means that you do not have to specify the number of processes and the execution hosts using the -np and the -hostfile options. Slurm will automatically provide this information to mpirun based on the allocated tasks: #!/bin/bash #SBATCH --mail-user=foo.bar@baz.unibe.ch ( ... ) #SBATCH --nodes=4 #SBATCH --ntasks-per-node=16 module load openmpi/1.10.2-gcc mpirun <options> <binary> Environment Variables Slurm sets various environment variables available in the context of the job script. Some are set based on the requested resources for the job. Environment Variable Set By Option Description SLURM_JOB_NAME \u2013job-name Name of the job SLURM_ARRAY_JOB_ID ID of your job SLURM_ARRAY_TASK_ID \u2013array ID of the current array task SLURM_ARRAY_TASK_MAX \u2013array Job array\u2019s maximum ID (index) number SLURM_ARRAY_TASK_MIN \u2013array Job array\u2019s minimum ID (index) number SLURM_ARRAY_TASK_STEP \u2013array Job array\u2019s index step size SLURM_NTASKS \u2013ntasks Same as -n, \u2013ntasks SLURM_NTASKS_PER_NODE \u2013ntasks-per-node Number of tasks requested per node. Only set if the \u2013ntasks-per-node option is specified SLURM_CPUS_PER_TASK \u2013cpus-per-task Number of cpus requested per task. Only set if the \u2013cpus-per-task option is specified TMPDIR References the disk space for the job on the local scratch Job Examples Sequential Job Running a Single Job Step #!/bin/bash #SBATCH --mail-user=foo.bar@baz.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --job-name=\"Serial Job\" #SBATCH --time=04:00:00 # Your code below this line echo \"I'm on host:\" hostname echo \"Environment variables:\" env Parallel Jobs Shared Memory Jobs (e.g. OpenMP) SMP parallelization is based upon dynamically created threads (fork and join) that share memory on a single node. The key request is \u201c\u2013cpus-per-task\u201d. To run N threads in parallel, we request N CPUs on the node (\u2013cpus-per-task=N). OpenMP is not slurm-aware, you need to specify \u201cexport OMP_NUM_THREADS=\u2026\u201d in your submission script! Thereby OMP_NUM_THREADS (max number of thread spawned by your program) must correspond the number of cores requested. As an example, consider the following job script: #!/bin/bash #SBATCH --mail-user=foo.bar@baz.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --job-name=\"SMP Job\" #SBATCH --mem-per-cpu=2G #SBATCH --cpus-per-task=16 #SBATCH --time=04:00:00 # Your code below this line # set OMP_NUM_THREADS to the number of --cpus-per-task that we requested export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK ./my_binary For optimal resource management, notably to prevent oversubscribing the compute node, setting the correct number of threads is crucial. The assignment OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK will ensure that your program does not spawn more threads than requested. MPI Jobs (e.g. Open MPI) MPI parallelization is based upon processes (local or distributed) that communicate by passing messages. Since they don\u2019t rely on shared memory those processes can be distributed among several compute nodes. Use the option \u2013ntasks to request a certain number of tasks (processes) that can be distributed over multiple nodes: #!/bin/bash #SBATCH --mail-user=foo.bar@baz.unibe.ch #SBATCH --mail-type=end #SBATCH --job-name=\"MPI Job\" #SBATCH --mem-per-cpu=2G #SBATCH --ntasks=8 #SBATCH --time=04:00:00 # Your code below this line # First set the environment for using Open MPI module load openmpi/1.10.2-gcc srun --mpi = pmi2 ./my_binary # or, mpirun ./my_binary On the \u2018empi\u2019 partition you must use all CPUs provided by a node (20 CPUs). For example to run an OMPI job on 80 CPUs, do: #!/bin/bash #SBATCH --mail-user=foo.bar@baz.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --job-name=\"MPI Job\" #SBATCH --mem-per-cpu=2G #SBATCH --nodes=4 #SBATCH --ntasks-per-node=20 #SBATCH --time=12:00:00 # Your code below this line # First set the environment for using Open MPI module load openmpi/1.10.2-gcc srun --mpi = pmi2 ./my_binary # or, mpirun ./my_binary Performance considerations Job Throughput It is crucial to specify a more or less accurate runtime for your job. Requesting too little will result in job abortion, while requesting too much will have a negative impact on job start time and job throughput: Firstly, jobs with a shorter runtime have a greater chance to benefit from being backfilled between long running jobs and may therefore start earlier if resources are scarce. Secondly, a short running job may still start when a scheduled downtime is getting closer while long running jobs won\u2019t start because they are not guaranteed to finish before the start of the downtime. It is crucial to request the correct amount of cores for your job. Requesting cores that your job cannot utilize is a waste of resources that could otherwise be allocated to other jobs. Hence, jobs that theoretically could run have to wait for the resources to become available. For potential consequences of requesting too less cores on job performance, see below. It is crucial to request the correct amount of memory for your job. Requesting too little memory will result in job abortion. Requesting too much memory is a waste of resources that could otherwise be allocated to other jobs. Job Performance/Runtime It is crucial to request the correct amount of cores for your job. For parallel jobs (shared memory, MPI, hybrid) requesting less cores than processes/threads are spawned by the job will lead to potentially overbooked compute nodes. This is because your job will nevertheless spawn the required number of processes/threads (use a certain number of cores) while to the scheduler it appears that some of the utilized resources are still available, and thus the scheduler will allocate those resources to other jobs. Although under certain circumstances it might make sense to share cores among multiple processes/threads, the above reasoning should be considered as a general guideline, especially for unexperienced user.","title":"Job Submission"},{"location":"user-guide/job-management/submission.html#job-submission","text":"","title":"Job Submission"},{"location":"user-guide/job-management/submission.html#description","text":"This section describes the interaction with the resource manager. The subchapters contain information about submitting jobs to the cluster, monitoring active jobs and retrieving useful information about resource usage. A cluster is a set of connected computers that work together to solve computational tasks (user jobs) and presents itself to the user as a single system. For the resources of a cluster (e.g. CPUs, GPUs, memory) to be used efficiently, a resource manager (also called workload manager or batch-queuing system) is vital. While there are many different resource managers available, the resource manager of choice on UBELIX is SLURM . After submitting a job to the cluster, SLURM will try to fulfill the job\u2019s resource request by allocating resources to the job. If the requested resources are already available, the job can start immediately. Otherwise, the start of the job is delayed (pending) until enough resources are available. SLURM allows you to monitor active (pending, running) jobs and to retrieve statistics about finished jobs e.g. (peak CPU usage). The subchapters describe individual aspects of SLURM. This page describes the job submission process with Slurm. It is important to collect error/output messages either by writing such information to the default location or by specifying specific locations using the \u2013error/\u2013output option. Do not redirect the error/output stream to /dev/null unless you know what you are doing. Error and output messages are the starting point for investigating a job failure. For backfilling performance and hence to maximize job throughput it is crucial to submit array jobs (collection of similar jobs) instead of submitting the same job repeatedly.","title":"Description"},{"location":"user-guide/job-management/submission.html#resource-allocation","text":"Every job submission starts with a resources allocation (nodes, cores, memory). An allocation is valid for a specific amount of time, and can be created using the salloc, sbatch or srun commands. Whereas salloc and sbatch only create resource allocations, srun launches parallel tasks within such a resource allocation, or implicitly creates an allocation if not started within one. The usual procedure is to combine resource requests and task execution (job steps) in a single batch script (job script) and then submit the script using the sbatch command. Most command options support a short form as well as a long form (e.g. -u , and \u2013user= ). Because few options only support the long form, we will consistently use the long form throughout this documentation. Some options have default values if not specified: The \u2013time option has partition-specific default values (see scontrol show partition ). The \u2013mem-per-cpu option has a global default value of 2048MB. The default partition is the all partition. To select another partition one must use the \u2013partition option, e.g. \u2013partition=long . Important information for investors regarding account selection Investors have two different accounts for accounting purposes. The investor account (increased privileges) is used automatically when using the empi partition (\u2013partition=empi). To use another partition, the user must explicitly select the group account (e.g \u2013account=dcb). To display your accounts, use: sacctmgr show assoc where user= format=user,account%20,partition.","title":"Resource Allocation"},{"location":"user-guide/job-management/submission.html#sbatch","text":"The sbatch command is used to submit a job script for later execution. It is the most common way to submit a job to the cluster due to its reusability. Slurm options are usually embedded in a job script prefixed by \u2018#SBATCH\u2019 directives. Slurm options specified as command line options overwrite corresponding options embedded in the job script Syntax sbatch [ options ] script [ args... ]","title":"sbatch"},{"location":"user-guide/job-management/submission.html#job-script","text":"Usually a job script consists of two parts. The first part is optional but highly recommended: Slurm-specific options used by the scheduler to manage the resources (e.g. memory) and configure the job environment Job-specific shell commands The job script acts as a wrapper for your actual job. Command-line options can still be used to overwrite embedded options. Although you can specify all Slurm options on the command-line, we encourage you, for clarity and reusability, to embed Slurm options in the job script You can find a template for a job script under /storage/software/workshop/slurm_template.sh. Copy the template to your home directory and adapt it to your needs: cp /gpfs/software/workshop/job_script_template.sh $HOME","title":"Job Script"},{"location":"user-guide/job-management/submission.html#options","text":"Option Description Example Default Value \u2013mail-user Mail address to contact job owner. Must specify a valid email address! \u2013mail-user=foo.bar@baz.unibe.ch \u2013mail-type When to notify a job owner: none, all, begin, end, fail, requeue, array_tasks \u2013mail-type=end,fail \u2013account Which account to charge. Regular users don\u2019t need to specify this option. For users with enhanced privileges on the empi partition, see here The users default account. \u2013job-name Specify a job name \u2013job-name=\u201dSimple Matlab\u201d \u2013time Expected runtime of the job. Format: dd-hh:mm:ss \u2013time=12:00:00 \u2013time=2-06:00:00 Partition-specific, see scontrol show partition \u2013mem-per-cpu Minimum memory required per allocated CPU in megabytes. Different units can be specified using the suffix [K|M|G] \u2013mem-per-cpu=2G 2048 MB \u2013tmp Specify the amount of disk space that must be available on the compute node(s). The local scratch space for the job is referenced by the variable TMPDIR . Default units are megabytes. Different units can be specified using the suffix [K|M|G|T]. \u2013tmp=8G \u2013tmp=2048 \u2013ntasks Number of tasks (processes). Used for MPI jobs that may run distributed on multiple compute nodes \u2013ntasks=4 1 or to match \u2013nodes, \u2013tasks-per-node if specified \u2013nodes Request a certain number of nodes \u2013nodes=2 1 or to match \u2013ntasks, \u2013tasks-per-node if specified \u2013ntasks-per-node Specifies how many tasks will run on each allocated node. Meant to be used with \u2013nodes . If used with the \u2013ntasks option, the \u2013ntasks option will take precedence and the \u2013ntasks-per-node will be treated as a maximum count of tasks per node. \u2013ntasks-per-node=2 \u2013cpus-per-task Number of CPUs per taks (threads). Used for shared memory jobs that run locally on a single compute node \u2013cpus-per-task=4 1 \u2013array Submit an array job. Use \u201c%\u201d to specify the max number of tasks allowed to run concurrently. \u2013array=1,4,16-32:4 \u2013array=1-100%20 \u2013workdir Set the current working directory. All relative paths used in the job script are relative to this directory The directory from where the sbatch command was executed \u2013output Redirect standard output. All directories specified in the path must exist before the job starts! By default stderr and stdout are connected to the same file slurm-%j.out, where \u2018%j\u2019 is replaced with the job allocation number. \u2013error Redirect standard error. All directories specified in the path must exist before the job starts! By default stderr and stdout are connected to the same file slurm-%j.out, where \u2018%j\u2019 is replaced with the job allocation number. \u2013partition The \u201call\u201d partition is the default partition. A different partition must be requested with the \u2013partition option! \u2013partition=long \u2013partition=debug Default partition: all \u2013dependency Defer the start of this job until the specified dependencies have been satisfied. See man sbatch for a description of all valid dependency types \u2013dependency=afterany:11908 \u2013hold Submit job in hold state. Job is not allowed to run until explicitly released \u2013immediate Only submit the job if all requested resources are immediately available \u2013exclusive Use the compute node(s) exclusively, i.e. do not share nodes with other jobs. CAUTION: Only use this option if you are an experienced user, and you really understand the implications of this feature. If used improperly, the use of this option can lead to a massive waste of computational resources \u2013constraint Request nodes with certain features. This option allows you to request a homogeneous pool of nodes for you MPI job \u2013constraint=ivy (all, long partition) \u2013constraint=sandy (all partition only) \u2013constraint=broadwell (all, empi partition) \u2013parsable Print the job id only Default output: \u201cSubmitted batch job \u201c \u2013test-only Validate the batch script and return the estimated start time considering the current cluster state","title":"Options"},{"location":"user-guide/job-management/submission.html#example","text":"jobs.sh #!/bin/bash #SBATCH --mail-type=none #SBATCH --job-name=\"Serial example\" #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=4G # Your code below this line ./calc_mat.sh Submit the job script: sbatch job.sh Submitted batch job 30215045 See below for more examples","title":"Example"},{"location":"user-guide/job-management/submission.html#salloc","text":"The salloc command is used to allocate resources (e.g. nodes), possibly with a set of constraints (e.g. number of processor per node) for later utilization. It is typically used to allocate resources and spawn a shell, in which the srun command is used to launch parallel tasks. Syntax salloc [ options ] [ <command> [ args... ]] Example bash$ salloc -N 2 sh salloc: Granted job allocation 247 sh$ module load openmpi/1.10.2-gcc sh$ srun --mpi = pmi2 mpi_hello_world Hello, World. I am 1 of 2 running on knlnode03.ubelix.unibe.ch Hello, World. I am 0 of 2 running on knlnode02.ubelix.unibe.ch sh$ exit salloc: Relinquishing job allocation 247","title":"salloc"},{"location":"user-guide/job-management/submission.html#srun","text":"The srun command creates job steps. One or multiple srun invocations are usually used from within an existing resource allocation. Thereby, a job step can utilize all resources allocated to the job, or utilize only a subset of the resource allocation. Multiple job steps can run sequentially in the order defined in the batch script or run in parallel, but can together never utilize more resources than provided by the allocation. Do not submit a job script using srun . Embedded Slurm options (#SBATCH) are not parsed by srun . Syntax srun [options] executable [args...]","title":"srun"},{"location":"user-guide/job-management/submission.html#when-do-i-use-srun-in-my-job-script","text":"Use srun in your job script to start MPI tasks run multiple jobs (serial or parallel jobs) simultaneously within an allocation Example Run MPI task: #!/bin/bash #SBATCH --mail-type=none #SBATCH --job-name=\"Open MPI example\" #SBATCH --nodes=2 #SBATCH --ntasks-per-node=16 #SBATCH --mem-per-cpu=2G #SBATCH --time=06:00:00 # Your code below this line module load openmpi/1.10.2-gcc srun --mpi = pmi2 mpi_bin Run two jobs simultaneously: #!/bin/bash #SBATCH --mail-type=none #SBATCH --job-name=\"Open MPI example\" #SBATCH --nodes=2 #SBATCH --ntasks-per-node=2 #SBATCH --mem-per-cpu=2G #SBATCH --time=06:00:00 # Your code below this line # Each job should run on a different node and starts two processes on that node: srun --nodes = 1 --ntasks = 2 job01.sh & srun --nodes = 1 --ntasks = 2 job02.sh & # Wait for both commands to finish. This is important when running bash commands in the background (using &)! wait","title":"When do I use srun in my job script?"},{"location":"user-guide/job-management/submission.html#requesting-a-partition-queue","text":"The default partition is the \u2018all\u2019 partition. If you do not explicitly request a partition, your job will run in the default partition. To request a different partition, you must use the \u2013partition option: #SBATCH --partition=long See here for a list of availalble partitions.","title":"Requesting a Partition (Queue)"},{"location":"user-guide/job-management/submission.html#requesting-an-account","text":"Accounts are used for accounting purposes. Every user has a default account that is used unless a different account is specified using the \u2013account option. Regular users only have a single account and can thus not request a different account. The default account for regular user is named after their group (e.g. dcb): $ sacctmgr show user foo User Def Acct Admin ---------- ---------- --------- foo bar None The remaining information provided in this section applies only to users with enhanced privileges on the empi partition. Users with enhanced privileges have an additional account for the empi partition. This additional account is set as their default account, which means they don\u2019t have to specify an account when submitting to the empi partition (\u2013partition= empi ), but must specify their \u201cgroup account\u201d (\u2013account= ) for submitting to any other partition (e.g all ). If a wrong account/partition combination is requested, you will experience the following error message: sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified","title":"Requesting an Account"},{"location":"user-guide/job-management/submission.html#example_1","text":"Here are some examples on the usage of the \u2013account and \u2013partition options. Regular users: Submit to the \"all\" partition: No options required! Submit to any other partition: --partition = <partname> e.g. --partition = empi Users with enhanced privileges on the empi partition: Submit to the \"all\" partition: --account = <grpname> e.g. --account = dcb Submit to the \"empi\" partition: --partition = empi Submit to any other partition: --account = <grpname> e.g. --account = dcb --partition = <partname> e.g. --partition = long","title":"Example"},{"location":"user-guide/job-management/submission.html#parallel-jobs","text":"A parallel job either runs on multiple CPU cores on a single compute node, or on multiple CPU cores distributed over multiple compute nodes. With Slurm you can request tasks, and CPUs per task. A task corresponds to a process that may be made up of multiple threads (CPUs per task). Different tasks of a job allocation may run on different compute nodes, while all threads that belong to a certain process execute on the same node. For shared memory jobs (SMP, parallel jobs that run on a single compute node) one would request a single task and a certain number of CPUs for that task: #SBATCH --cpus-per-task=16 For MPI jobs (parallel jobs that may be distributed over multiple compute nodes) one would request a certain number of tasks and certain number of nodes: #SBATCH --nodes=2 #SBATCH --ntasks-per-node=16 The requested node,task, and CPU resources must match! For example, you cannot request one node (\u2013nodes=1) and more tasks (\u2013ntasks-per-node) than CPU cores are available on a single node in the partition. In such a case you will experience an error message: sbatch: error: Batch job submission failed: Requested node configuration is not available.","title":"Parallel Jobs"},{"location":"user-guide/job-management/submission.html#open-mpi","text":"Open MPI was compiled with Slurm support, which means that you do not have to specify the number of processes and the execution hosts using the -np and the -hostfile options. Slurm will automatically provide this information to mpirun based on the allocated tasks: #!/bin/bash #SBATCH --mail-user=foo.bar@baz.unibe.ch ( ... ) #SBATCH --nodes=4 #SBATCH --ntasks-per-node=16 module load openmpi/1.10.2-gcc mpirun <options> <binary>","title":"Open MPI"},{"location":"user-guide/job-management/submission.html#environment-variables","text":"Slurm sets various environment variables available in the context of the job script. Some are set based on the requested resources for the job. Environment Variable Set By Option Description SLURM_JOB_NAME \u2013job-name Name of the job SLURM_ARRAY_JOB_ID ID of your job SLURM_ARRAY_TASK_ID \u2013array ID of the current array task SLURM_ARRAY_TASK_MAX \u2013array Job array\u2019s maximum ID (index) number SLURM_ARRAY_TASK_MIN \u2013array Job array\u2019s minimum ID (index) number SLURM_ARRAY_TASK_STEP \u2013array Job array\u2019s index step size SLURM_NTASKS \u2013ntasks Same as -n, \u2013ntasks SLURM_NTASKS_PER_NODE \u2013ntasks-per-node Number of tasks requested per node. Only set if the \u2013ntasks-per-node option is specified SLURM_CPUS_PER_TASK \u2013cpus-per-task Number of cpus requested per task. Only set if the \u2013cpus-per-task option is specified TMPDIR References the disk space for the job on the local scratch","title":"Environment Variables"},{"location":"user-guide/job-management/submission.html#job-examples","text":"","title":"Job Examples"},{"location":"user-guide/job-management/submission.html#sequential-job","text":"Running a Single Job Step #!/bin/bash #SBATCH --mail-user=foo.bar@baz.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --job-name=\"Serial Job\" #SBATCH --time=04:00:00 # Your code below this line echo \"I'm on host:\" hostname echo \"Environment variables:\" env","title":"Sequential Job"},{"location":"user-guide/job-management/submission.html#parallel-jobs_1","text":"Shared Memory Jobs (e.g. OpenMP) SMP parallelization is based upon dynamically created threads (fork and join) that share memory on a single node. The key request is \u201c\u2013cpus-per-task\u201d. To run N threads in parallel, we request N CPUs on the node (\u2013cpus-per-task=N). OpenMP is not slurm-aware, you need to specify \u201cexport OMP_NUM_THREADS=\u2026\u201d in your submission script! Thereby OMP_NUM_THREADS (max number of thread spawned by your program) must correspond the number of cores requested. As an example, consider the following job script: #!/bin/bash #SBATCH --mail-user=foo.bar@baz.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --job-name=\"SMP Job\" #SBATCH --mem-per-cpu=2G #SBATCH --cpus-per-task=16 #SBATCH --time=04:00:00 # Your code below this line # set OMP_NUM_THREADS to the number of --cpus-per-task that we requested export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK ./my_binary For optimal resource management, notably to prevent oversubscribing the compute node, setting the correct number of threads is crucial. The assignment OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK will ensure that your program does not spawn more threads than requested. MPI Jobs (e.g. Open MPI) MPI parallelization is based upon processes (local or distributed) that communicate by passing messages. Since they don\u2019t rely on shared memory those processes can be distributed among several compute nodes. Use the option \u2013ntasks to request a certain number of tasks (processes) that can be distributed over multiple nodes: #!/bin/bash #SBATCH --mail-user=foo.bar@baz.unibe.ch #SBATCH --mail-type=end #SBATCH --job-name=\"MPI Job\" #SBATCH --mem-per-cpu=2G #SBATCH --ntasks=8 #SBATCH --time=04:00:00 # Your code below this line # First set the environment for using Open MPI module load openmpi/1.10.2-gcc srun --mpi = pmi2 ./my_binary # or, mpirun ./my_binary On the \u2018empi\u2019 partition you must use all CPUs provided by a node (20 CPUs). For example to run an OMPI job on 80 CPUs, do: #!/bin/bash #SBATCH --mail-user=foo.bar@baz.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --job-name=\"MPI Job\" #SBATCH --mem-per-cpu=2G #SBATCH --nodes=4 #SBATCH --ntasks-per-node=20 #SBATCH --time=12:00:00 # Your code below this line # First set the environment for using Open MPI module load openmpi/1.10.2-gcc srun --mpi = pmi2 ./my_binary # or, mpirun ./my_binary","title":"Parallel Jobs"},{"location":"user-guide/job-management/submission.html#performance-considerations","text":"","title":"Performance considerations"},{"location":"user-guide/job-management/submission.html#job-throughput","text":"It is crucial to specify a more or less accurate runtime for your job. Requesting too little will result in job abortion, while requesting too much will have a negative impact on job start time and job throughput: Firstly, jobs with a shorter runtime have a greater chance to benefit from being backfilled between long running jobs and may therefore start earlier if resources are scarce. Secondly, a short running job may still start when a scheduled downtime is getting closer while long running jobs won\u2019t start because they are not guaranteed to finish before the start of the downtime. It is crucial to request the correct amount of cores for your job. Requesting cores that your job cannot utilize is a waste of resources that could otherwise be allocated to other jobs. Hence, jobs that theoretically could run have to wait for the resources to become available. For potential consequences of requesting too less cores on job performance, see below. It is crucial to request the correct amount of memory for your job. Requesting too little memory will result in job abortion. Requesting too much memory is a waste of resources that could otherwise be allocated to other jobs.","title":"Job Throughput"},{"location":"user-guide/job-management/submission.html#job-performanceruntime","text":"It is crucial to request the correct amount of cores for your job. For parallel jobs (shared memory, MPI, hybrid) requesting less cores than processes/threads are spawned by the job will lead to potentially overbooked compute nodes. This is because your job will nevertheless spawn the required number of processes/threads (use a certain number of cores) while to the scheduler it appears that some of the utilized resources are still available, and thus the scheduler will allocate those resources to other jobs. Although under certain circumstances it might make sense to share cores among multiple processes/threads, the above reasoning should be considered as a general guideline, especially for unexperienced user.","title":"Job Performance/Runtime"},{"location":"user-guide/software/JupyterLab.html","text":"Jupyter Lab Description Some useful information on using Jupyter Lab on UBELIX compute nodes. IMPORTANT: in the following we show how to start the server on a compute node. Please keep in mind that these resources will be dedicated for you, thus and idle session will waste resources. Please quit your session as soon as you don\u2019t use it anymore , even for a lunch break. Your notebook will maintain all you input/output. Overview On UBELIX we provide Jupyter Lab for working with Jupyter Notebooks. JupyterLab is a single-user web-based Notebook server, running in the user space. JupyterLab servers should be started preferably on a compute node, especially for compute intensive or memory intensive workloads. After starting the Jupyter Lab server your local browser can be connected using port forwarding. Therefore port forwarding needs to be enabled properly. On this page we describe: Launch JupyterLab Connect to UBELIX and establishing SSH port forwarding SSH with port forwarding Launch the JupyterLab server Launch JupyterLab in your local browser Kernels Packages Launch JupyterLab Since JupyterLab is a web based application, a port needs to be forwarded to your local machine, where your browser can connect to. This port numbers need to be between 2000 and 65000 and need to be unique on the present machine. The default port for JupyterLab is 8888, but only one user can use this at a time. To avoid the need for modifying the following procedure again and again, we suggest to (once) select a unique number (between 2000 and 65000). And then following commands can be hopefully reused without modification. The port needs to be specified while establishing the connection to UBELIX and while launching JupyterLab. In the following we use the port number 15051 ( please select another number ). Setup SSH with port forwarding First, the port forwarding needs to be enabled between your local machine and UBELIX. Therewith a local port will be connected to the remote port on UBELIX. For simplicity, we kept both numbers the same (here 15051). This can be specified on the command line in the terminal. Note: MobaXterm has an internal terminal which acts like a linux terminal and can be configured as described in the Standard Terminal Setup. Therewith, the SSH command line approach above can be used. SSH with port forwarding The ssh command need to be called with following arguments: ssh -Y -L 15051:localhost:15051 submit.unibe.ch If configured in your .ssh/config , you can also use the alias instead of the full name for UBELIX. Launch the JupyterLab server On UBELIX, the required Anaconda3 module needs to be loaded. If you want to use additional kernels (R) you need to load additional modules, e.g. IRkernel (for R kernels): module load Anaconda3 A script is provided, taking care of enabling the port forwarding to the compute node and launching JupyterLab. As an example a session with 45 min on 2 core can be launched using: jupyter-compute 15051 --ntasks 2 --time=00:45:00 # please change port number This tool will lauch the server on a compute node, and establish the port forwarding. After general output, JupyterLab prints a URL with a unique key and the network port number where the web-server is listening, this should look similar to: ... [C 21:43:35.291 LabApp] To access the notebook, open this file in a browser: file:///gpfs/homefs/id/ms20e149/.local/share/jupyter/runtime/nbserver-30194-open.html Or copy and paste one of these URLs: http://anode001:15051/?token=69ba5d24acab5915f2520c008a57df51f3cc38b7050ea073 or http://127.0.0.1:15051/?token=69ba5d24acab5915f2520c008a57df51f3cc38b7050ea073 The last line needs to be copied in your local browser. More resources can be requested, e.g. by using: jupyter-compute 15051 --ntasks 1 -t 60 --cpus-per-task 5 --mem 512MB Where 5 cores are requested for threading and a total memory of 3GB. Please do not use multiprocessing.cpu_count() since this is returning the total amount of cores on the node. Furthermore, if you use libraries, which implement threading: align the numbers of threads (often called jobs) to the selected number of cores (otherwise the performance will be affected). OR requesting GPU resources on a node with a NVIDIA graphics card: jupyter-compute 15051 --ntasks 1 -t 60 --partition=gpu --gres=gpu:gtx1080ti:1 Note: This tool can only be used in the all and gpu partition. JupyterLab in your local browser Finally, you need to open your local web browser and copy and paste the URL specified by the JupyterLab server into the address bar. After initializing Jupyter Lab you should see a page similar to: Therewith the Notebook and its containing tasks are performed on a compute node, which can double check e.g. using using the following in Python: import socket print(socket.gethostname()) IMPORTANT: Please remember to stop your Jupyter Lab server and therewith your slurm job, when you do not need it anymore. Thus, the resource get available to other users again. Note: After stopping the JupyterLab server some sessions may get corrupted and do not take input correctly anymore. In this case just quit and re-establish your ssh session. Kernels The following JupyterLab kernel are installed: Python3 R R verify that the module IRkernel is loaded module load IRkernel Packages There are a long list of default packages provided by Anaconda3 (list all using !pip list ) and R (list using installed.packages(.Library) , note the list is shortened). Furthermore, you can install additional packages in Python using pip install --user or in R using install.packages(\"sampling\") .","title":"JupyterLab"},{"location":"user-guide/software/JupyterLab.html#jupyter-lab","text":"","title":"Jupyter Lab"},{"location":"user-guide/software/JupyterLab.html#description","text":"Some useful information on using Jupyter Lab on UBELIX compute nodes. IMPORTANT: in the following we show how to start the server on a compute node. Please keep in mind that these resources will be dedicated for you, thus and idle session will waste resources. Please quit your session as soon as you don\u2019t use it anymore , even for a lunch break. Your notebook will maintain all you input/output.","title":"Description"},{"location":"user-guide/software/JupyterLab.html#overview","text":"On UBELIX we provide Jupyter Lab for working with Jupyter Notebooks. JupyterLab is a single-user web-based Notebook server, running in the user space. JupyterLab servers should be started preferably on a compute node, especially for compute intensive or memory intensive workloads. After starting the Jupyter Lab server your local browser can be connected using port forwarding. Therefore port forwarding needs to be enabled properly. On this page we describe: Launch JupyterLab Connect to UBELIX and establishing SSH port forwarding SSH with port forwarding Launch the JupyterLab server Launch JupyterLab in your local browser Kernels Packages","title":"Overview"},{"location":"user-guide/software/JupyterLab.html#launch-jupyterlab","text":"Since JupyterLab is a web based application, a port needs to be forwarded to your local machine, where your browser can connect to. This port numbers need to be between 2000 and 65000 and need to be unique on the present machine. The default port for JupyterLab is 8888, but only one user can use this at a time. To avoid the need for modifying the following procedure again and again, we suggest to (once) select a unique number (between 2000 and 65000). And then following commands can be hopefully reused without modification. The port needs to be specified while establishing the connection to UBELIX and while launching JupyterLab. In the following we use the port number 15051 ( please select another number ).","title":"Launch JupyterLab"},{"location":"user-guide/software/JupyterLab.html#setup-ssh-with-port-forwarding","text":"First, the port forwarding needs to be enabled between your local machine and UBELIX. Therewith a local port will be connected to the remote port on UBELIX. For simplicity, we kept both numbers the same (here 15051). This can be specified on the command line in the terminal. Note: MobaXterm has an internal terminal which acts like a linux terminal and can be configured as described in the Standard Terminal Setup. Therewith, the SSH command line approach above can be used.","title":"Setup SSH with port forwarding"},{"location":"user-guide/software/JupyterLab.html#ssh-with-port-forwarding","text":"The ssh command need to be called with following arguments: ssh -Y -L 15051:localhost:15051 submit.unibe.ch If configured in your .ssh/config , you can also use the alias instead of the full name for UBELIX.","title":"SSH with port forwarding"},{"location":"user-guide/software/JupyterLab.html#launch-the-jupyterlab-server","text":"On UBELIX, the required Anaconda3 module needs to be loaded. If you want to use additional kernels (R) you need to load additional modules, e.g. IRkernel (for R kernels): module load Anaconda3 A script is provided, taking care of enabling the port forwarding to the compute node and launching JupyterLab. As an example a session with 45 min on 2 core can be launched using: jupyter-compute 15051 --ntasks 2 --time=00:45:00 # please change port number This tool will lauch the server on a compute node, and establish the port forwarding. After general output, JupyterLab prints a URL with a unique key and the network port number where the web-server is listening, this should look similar to: ... [C 21:43:35.291 LabApp] To access the notebook, open this file in a browser: file:///gpfs/homefs/id/ms20e149/.local/share/jupyter/runtime/nbserver-30194-open.html Or copy and paste one of these URLs: http://anode001:15051/?token=69ba5d24acab5915f2520c008a57df51f3cc38b7050ea073 or http://127.0.0.1:15051/?token=69ba5d24acab5915f2520c008a57df51f3cc38b7050ea073 The last line needs to be copied in your local browser. More resources can be requested, e.g. by using: jupyter-compute 15051 --ntasks 1 -t 60 --cpus-per-task 5 --mem 512MB Where 5 cores are requested for threading and a total memory of 3GB. Please do not use multiprocessing.cpu_count() since this is returning the total amount of cores on the node. Furthermore, if you use libraries, which implement threading: align the numbers of threads (often called jobs) to the selected number of cores (otherwise the performance will be affected). OR requesting GPU resources on a node with a NVIDIA graphics card: jupyter-compute 15051 --ntasks 1 -t 60 --partition=gpu --gres=gpu:gtx1080ti:1 Note: This tool can only be used in the all and gpu partition.","title":"Launch the JupyterLab server"},{"location":"user-guide/software/JupyterLab.html#jupyterlab-in-your-local-browser","text":"Finally, you need to open your local web browser and copy and paste the URL specified by the JupyterLab server into the address bar. After initializing Jupyter Lab you should see a page similar to: Therewith the Notebook and its containing tasks are performed on a compute node, which can double check e.g. using using the following in Python: import socket print(socket.gethostname()) IMPORTANT: Please remember to stop your Jupyter Lab server and therewith your slurm job, when you do not need it anymore. Thus, the resource get available to other users again. Note: After stopping the JupyterLab server some sessions may get corrupted and do not take input correctly anymore. In this case just quit and re-establish your ssh session.","title":"JupyterLab in your local browser"},{"location":"user-guide/software/JupyterLab.html#kernels","text":"The following JupyterLab kernel are installed: Python3 R","title":"Kernels"},{"location":"user-guide/software/JupyterLab.html#r","text":"verify that the module IRkernel is loaded module load IRkernel","title":"R"},{"location":"user-guide/software/JupyterLab.html#packages","text":"There are a long list of default packages provided by Anaconda3 (list all using !pip list ) and R (list using installed.packages(.Library) , note the list is shortened). Furthermore, you can install additional packages in Python using pip install --user or in R using install.packages(\"sampling\") .","title":"Packages"},{"location":"user-guide/software/installing-custom-software.html","text":"Installing Custom Software Description UBELIX comes with a plethora of software pre-installed. You can find a list of already available software using the command module avail . The module environment is described here . If needed, every user can install custom software within his space, which is described in the following using Easybuild or the manual approach.. Note: You cannot use the packet management utility yum for this, since this command requires root privileges to install software system wide. Instead you have to compile and install the software yourself. If you think that some missing software could be of general interest for the UBELIX community, you can ask us to install the software system wide. Since maintaining software is a lot of work, we will select carefully which software we will install globally. Modules On UBELIX we use the Lmod module system which allows users to enable software package by package. The module system allows us to have multiple versions of the same software product installed as well as preventing unwanted influences between software packages. EasyBuild When possible we use EasyBuild to provision software packages. Easyconfigs are used to describe the whole build process, including the location of the sources, dependencies and its versions, the used environment, compile arguments, etc. These Easyconfigs are publicly available here . Users mainly have two options, building the software package by performing all the steps manually (see example down below) or using the EasyBuild. Using EasyBuild For EasyBuild we provide a setup which lets you create an own software stack in private or group shared space. By loading the following module you can build and software packages in the following spaces: module software location accessiblity CustomRepo/user $HOME private to you CustomRepo/project /storage/research/<projectID>/share project based, with collaborators CustomRepo/institute /home/ubelix/<instituteID>/share all institute members Note: The project module requires an environment variable CUSTOM_REPO_PROJECT_ID to be set to the project ID. The prepared setup will handle installation directories for EasyBuild (for software package and module file), and provides easy access later on, for you and you collaborators. Furthermore, we provide an easybuils wrapper (called eb-install-all ) to build for all available CPU architectures. Therewith you have always the correct version available. The CustomRepo module also takes care to use the correct version when launching on the compute node later on. Additionally, we provide another wrapper ( eb-install-generic ) which installs architecture independent packages like python scripts into a separate location. This location is also available using the CustomRepo module. The synatax is: eb_install_{all,generic} [options] [easybuild options] <easyconfig>.eb You can provide additional slurm arguments to both wrappers, e.g. --slurm_args='--account=xyz --time=00:10:00' and if necessary, you can specify specific architectures to build on, e.g. --arch='broadwell' if you only need the broadwell version. After this options, easybuild arguments can be provided like \u2013robot` (please use slurm arguments first). As an example the package Relion can be build in group space /storage/research/id_test/share using: # setup the environment export CUSTOM_REPO_PROJECT_ID = id_test # only necessary for CustomRepo/project module load EasyBuild module load CustomRepo/project # get the easyconfig; can aslo be downloaded differntly; can be addapted e.g. for a different version number wget https://raw.githubusercontent.com/easybuilders/easybuild-easyconfigs/4.1.x/easybuild/easyconfigs/r/RELION/RELION-3.0_beta.2018.08.02-intel-2018a.eb # installation for all different CPU architectures eb-install-all --slurm_args = '--time=00:10:00 --account=id_test' --robot RELION-3.0_beta.2018.08.02-intel-2018a.eb Note: The --robot option advice EasyBuild to additionally install all required depencencies, if related easyconfigs can be found on the system. To use these package any user who has access to the space (e.g. project collaborators) can use in their batch scripts: #SBATCH export CUSTOM_REPO_PROJECT_ID = id_test # only necessary for CustomRepo/project module load CustomRepo/project module load RELION/3.0_beta.2018.08.02 #srun <executable> Note: If you only have one project you can define the $CUSTOM_REPO_PROJECT_ID in your ~/.bashrc to have it permanently defined. Manually compiling With Linux, you typically compile, link, and install a program like this: tar xzvf some-software-0.1.tar.gz cd some-software-0.1 ./configure --prefix = $HOME /my_custom_software/some-software make make install make clean configure is usually a complex shell script that gathers information about your system and makes sure that everything needed for compiling the program is available. It may also create a Makefile that is used by the make command. With the --prefix option you can specify a base directory, relative to which make install will install the files. The make utility is what does the actual compiling and linking. If for example some additional library is missing on the system or not found in the expected location, the command will normally exit immediately. make install puts the compiled files in the proper directories (e.g. $HOME/my_custom_software/some-software/bin , $HOME/my_custom_software/some-software/lib , \u2026). make clean cleans up temporary files that were generated during the compiling and linking stage. GNU make documentation (advanced): http://www.gnu.org/software/make/manual/make.html Providing packages You can use the CustomRepo setup to easily provide access to the packages to you and you collaborators. After loading one of the CustomRepo module (see above) you can install your package under $EASYBUILD_PREFIX for specific architectures or $EASYBUILD_PREFIX/../generic and place your modulefiles e.g. at $EASYBUILD_PREFIX/../generic/modulefiles/all . Therewith you just need to load the CustomRepo module to access your software products.","title":"Installing Custom Software"},{"location":"user-guide/software/installing-custom-software.html#installing-custom-software","text":"","title":"Installing Custom Software"},{"location":"user-guide/software/installing-custom-software.html#description","text":"UBELIX comes with a plethora of software pre-installed. You can find a list of already available software using the command module avail . The module environment is described here . If needed, every user can install custom software within his space, which is described in the following using Easybuild or the manual approach.. Note: You cannot use the packet management utility yum for this, since this command requires root privileges to install software system wide. Instead you have to compile and install the software yourself. If you think that some missing software could be of general interest for the UBELIX community, you can ask us to install the software system wide. Since maintaining software is a lot of work, we will select carefully which software we will install globally.","title":"Description"},{"location":"user-guide/software/installing-custom-software.html#modules","text":"On UBELIX we use the Lmod module system which allows users to enable software package by package. The module system allows us to have multiple versions of the same software product installed as well as preventing unwanted influences between software packages.","title":"Modules"},{"location":"user-guide/software/installing-custom-software.html#easybuild","text":"When possible we use EasyBuild to provision software packages. Easyconfigs are used to describe the whole build process, including the location of the sources, dependencies and its versions, the used environment, compile arguments, etc. These Easyconfigs are publicly available here . Users mainly have two options, building the software package by performing all the steps manually (see example down below) or using the EasyBuild.","title":"EasyBuild"},{"location":"user-guide/software/installing-custom-software.html#using-easybuild","text":"For EasyBuild we provide a setup which lets you create an own software stack in private or group shared space. By loading the following module you can build and software packages in the following spaces: module software location accessiblity CustomRepo/user $HOME private to you CustomRepo/project /storage/research/<projectID>/share project based, with collaborators CustomRepo/institute /home/ubelix/<instituteID>/share all institute members Note: The project module requires an environment variable CUSTOM_REPO_PROJECT_ID to be set to the project ID. The prepared setup will handle installation directories for EasyBuild (for software package and module file), and provides easy access later on, for you and you collaborators. Furthermore, we provide an easybuils wrapper (called eb-install-all ) to build for all available CPU architectures. Therewith you have always the correct version available. The CustomRepo module also takes care to use the correct version when launching on the compute node later on. Additionally, we provide another wrapper ( eb-install-generic ) which installs architecture independent packages like python scripts into a separate location. This location is also available using the CustomRepo module. The synatax is: eb_install_{all,generic} [options] [easybuild options] <easyconfig>.eb You can provide additional slurm arguments to both wrappers, e.g. --slurm_args='--account=xyz --time=00:10:00' and if necessary, you can specify specific architectures to build on, e.g. --arch='broadwell' if you only need the broadwell version. After this options, easybuild arguments can be provided like \u2013robot` (please use slurm arguments first). As an example the package Relion can be build in group space /storage/research/id_test/share using: # setup the environment export CUSTOM_REPO_PROJECT_ID = id_test # only necessary for CustomRepo/project module load EasyBuild module load CustomRepo/project # get the easyconfig; can aslo be downloaded differntly; can be addapted e.g. for a different version number wget https://raw.githubusercontent.com/easybuilders/easybuild-easyconfigs/4.1.x/easybuild/easyconfigs/r/RELION/RELION-3.0_beta.2018.08.02-intel-2018a.eb # installation for all different CPU architectures eb-install-all --slurm_args = '--time=00:10:00 --account=id_test' --robot RELION-3.0_beta.2018.08.02-intel-2018a.eb Note: The --robot option advice EasyBuild to additionally install all required depencencies, if related easyconfigs can be found on the system. To use these package any user who has access to the space (e.g. project collaborators) can use in their batch scripts: #SBATCH export CUSTOM_REPO_PROJECT_ID = id_test # only necessary for CustomRepo/project module load CustomRepo/project module load RELION/3.0_beta.2018.08.02 #srun <executable> Note: If you only have one project you can define the $CUSTOM_REPO_PROJECT_ID in your ~/.bashrc to have it permanently defined.","title":"Using EasyBuild"},{"location":"user-guide/software/installing-custom-software.html#manually-compiling","text":"With Linux, you typically compile, link, and install a program like this: tar xzvf some-software-0.1.tar.gz cd some-software-0.1 ./configure --prefix = $HOME /my_custom_software/some-software make make install make clean configure is usually a complex shell script that gathers information about your system and makes sure that everything needed for compiling the program is available. It may also create a Makefile that is used by the make command. With the --prefix option you can specify a base directory, relative to which make install will install the files. The make utility is what does the actual compiling and linking. If for example some additional library is missing on the system or not found in the expected location, the command will normally exit immediately. make install puts the compiled files in the proper directories (e.g. $HOME/my_custom_software/some-software/bin , $HOME/my_custom_software/some-software/lib , \u2026). make clean cleans up temporary files that were generated during the compiling and linking stage. GNU make documentation (advanced): http://www.gnu.org/software/make/manual/make.html","title":"Manually compiling"},{"location":"user-guide/software/installing-custom-software.html#providing-packages","text":"You can use the CustomRepo setup to easily provide access to the packages to you and you collaborators. After loading one of the CustomRepo module (see above) you can install your package under $EASYBUILD_PREFIX for specific architectures or $EASYBUILD_PREFIX/../generic and place your modulefiles e.g. at $EASYBUILD_PREFIX/../generic/modulefiles/all . Therewith you just need to load the CustomRepo module to access your software products.","title":"Providing packages"},{"location":"user-guide/software/matlab.html","text":"Matlab Description UBELIX is always featuring the latest two (b)-releases of Matlab. Facts about Matlab on UBELIX It can run in parallel on one node , thanks to the Parallel Computing ToolBox It can take advantage of GPUs It cannot run on more than one node as we do not have the Distributed Computing Toolbox. Matlab is NOT FREE to use! Every user using Matlab on UBELIX must have at least one valid license. You can buy licenses at our software shop . MATLAB Version: 9.3.0.713579 (R2017b) contains: MATLAB Version: 9.1.0.441655 (R2016b) contains: Simulink Version 9.0 (R2017b) Simulink Version 8.8 (R2016b) Bioinformatics Toolbox Version 4.9 (R2017b) Communications System Toolbox Version 6.3 (R2016b) Communications System Toolbox Version 6.5 (R2017b) Computer Vision System Toolbox Version 7.2 (R2016b) Computer Vision System Toolbox Version 8.0 (R2017b) Computer Vision System Toolbox Version 7.2 (R2016b) Control System Toolbox Version 10.3 (R2017b) Curve Fitting Toolbox Version 3.5.4 (R2016b) Curve Fitting Toolbox Version 3.5.6 (R2017b) DSP System Toolbox Version 9.3 (R2016b) DSP System Toolbox Version 9.5 (R2017b) Database Toolbox Version 7.0 (R2016b) Database Toolbox Version 8.0 (R2017b) Financial Toolbox Version 5.8 (R2016b) Financial Toolbox Version 5.10 (R2017b) Fixed-Point Designer Version 5.3 (R2016b) Fixed-Point Designer Version 6.0 (R2017b) Fuzzy Logic Toolbox Version 2.2.24 (R2016b) Fuzzy Logic Toolbox Version 2.3 (R2017b) Image Acquisition Toolbox Version 5.1 (R2016b) Global Optimization Toolbox Version 3.4.3 (R2017b) Image Processing Toolbox Version 9.5 (R2016b) Image Acquisition Toolbox Version 5.3 (R2017b) MATLAB Coder Version 3.2 (R2016b) Image Processing Toolbox Version 10.1 (R2017b) MATLAB Compiler Version 6.3 (R2016b) Instrument Control Toolbox Version 3.12 (R2017b) MATLAB Compiler SDK Version 6.3 (R2016b) MATLAB Coder Version 3.4 (R2017b) Mapping Toolbox Version 4.4 (R2016b) MATLAB Compiler Version 6.5 (R2017b) Neural Network Toolbox Version 9.1 (R2016b) MATLAB Compiler SDK Version 6.4 (R2017b) Optimization Toolbox Version 7.5 (R2016b) Mapping Toolbox Version 4.5.1 (R2017b) Parallel Computing Toolbox Version 6.9 (R2016b) Model Predictive Control Toolbox Version 6.0 (R2017b) Partial Differential Equation Toolbox Version 2.3 (R2016b) Neural Network Toolbox Version 11.0 (R2017b) Robust Control Toolbox Version 6.2 (R2016b) Optimization Toolbox Version 8.0 (R2017b) Signal Processing Toolbox Version 7.3 (R2016b) Parallel Computing Toolbox Version 6.11 (R2017b) Simscape Version 4.1 (R2016b) Partial Differential Equation Toolbox Version 2.5 (R2017b) Simscape Multibody Version 4.9 (R2016b) Robust Control Toolbox Version 6.4 (R2017b) Simscape Power Systems Version 6.6 (R2016b) Signal Processing Toolbox Version 7.5 (R2017b) Simulink Coder Version 8.11 (R2016b) Simscape Version 4.3 (R2017b) Simulink Control Design Version 4.4 (R2016b) Simscape Multibody Version 5.1 (R2017b) Simulink Design Optimization Version 3.1 (R2016b) Simscape Power Systems Version 6.8 (R2017b) Simulink Verification and Validation Version 3.12 (R2016b) Simulink Check Version 4.0 (R2017b) Stateflow Version 8.8 (R2016b) Simulink Coder Version 8.13 (R2017b) Statistics and Machine Learning Toolbox Version 11.0 (R2016b) Simulink Control Design Version 5.0 (R2017b) Symbolic Math Toolbox Version 7.1 (R2016b) Simulink Coverage Version 4.0 (R2017b) System Identification Toolbox Version 9.5 (R2016b) Simulink Design Optimization Version 3.3 (R2017b) Wavelet Toolbox Version 4.17 (R2016b) Simulink Requirements Version 1.0 (R2017b) Stateflow Version 9.0 (R2017b) Statistics and Machine Learning Toolbox Version 11.2 (R2017b) Symbolic Math Toolbox Version 8.0 (R2017b) System Identification Toolbox Version 9.7 (R2017b) Wavelet Toolbox Version 4.19 (R2017b) Running Matlab on the Compute Nodes Submitting a Matlab job to the cluster is very similar to submitting any other serial job. Lets try to run a simple Matlab script which we will put in a file boxfilter.m boxfilter.m % Compute a local mean filter over a neighborhood of 11x11 pixels % Read image into workspace: original = imread ( 'girlface.png' ) ; % Perform the mean filtering: filtered = imboxfilt ( original, 11 ) ; % Save the original and the filtered image side-by-side: imwrite ([ original, filtered ] , 'comparison.png' ) ; Now we need a submission script boxfilter.qsub !#/bin/bash #SBATCH -mail-user=foo@bar.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --job-name=boxfilter #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=2G # Load Matlab form the environment modules module load matlab/R2015b # Tell Matlab to run our box filter.m file and exit matlab -nodisplay -r \"boxfilter, exit\" Passing Arguments to a m-File There are several ways to provide input arguments in Matlab. Define the Variables Before Running the Script Lets take the box filter.m example from above. The script is not universal because the name of the input image and the box size is hardcoded in the script. We make the script more generally usable by: boxfilter.m % Compute a local mean filter over a neighborhood of 11x11 pixels % Read image into workspace: original = imread ( inputImg ) ; % Perform the mean filtering: filtered = imboxfilt ( original, x ) ; % Save the original and the filtered image side-by-side: imwrite ([ original, filtered ] , 'comparison.png' ) ; and then: boxfilter.qsub !#/bin/bash ( ... ) # Load Matlab form the environment modules module load matlab/R2015b # Tell Matlab to run our box filter.m file and exit matlab -nodisplay -r \"inputImg='girlface.png'; x=11; boxfilter, exit\" Advanced Topics Multithreading By default, MATLAB makes use of the multithreading capabilities of the node on which it is running. It is crucial that you allocate the same number of slots for your job as your job utilizes cores. Disable Computational Multithreading If you do not need multithreading for your application consider to disable computational multithreading by setting the -singleCompThread option when starting MATLAB: matlab -nodisplay -singleCompThread -r \"boxfilter('girlface.png', 'comparison.png', 11); exit\" Disable Computational Multithreading If you do not need multithreading for your application consider to disable computational multithreading by setting the -singleCompThread option when starting MATLAB: matlab -nodisplay -singleCompThread -r \"boxfilter('girlface.png', 'comparison.png', 11); exit\" Running MATLAB in Multithreaded Mode Most of the time, running MATLAB in single-threaded mode will meet your needs. If you have mathematically intense computations that might benefit from multi-threading capabilities provided by MATLAB\u2019s BLAS implementation, then you should limit MATLAB to a well defined number of threads, so that you can allocate the correct number of slots for your job. Use the maxNumCompThreads(N) function to control the number of computational threads:","title":"Matlab"},{"location":"user-guide/software/matlab.html#matlab","text":"","title":"Matlab"},{"location":"user-guide/software/matlab.html#description","text":"UBELIX is always featuring the latest two (b)-releases of Matlab.","title":"Description"},{"location":"user-guide/software/matlab.html#facts-about-matlab-on-ubelix","text":"It can run in parallel on one node , thanks to the Parallel Computing ToolBox It can take advantage of GPUs It cannot run on more than one node as we do not have the Distributed Computing Toolbox. Matlab is NOT FREE to use! Every user using Matlab on UBELIX must have at least one valid license. You can buy licenses at our software shop . MATLAB Version: 9.3.0.713579 (R2017b) contains: MATLAB Version: 9.1.0.441655 (R2016b) contains: Simulink Version 9.0 (R2017b) Simulink Version 8.8 (R2016b) Bioinformatics Toolbox Version 4.9 (R2017b) Communications System Toolbox Version 6.3 (R2016b) Communications System Toolbox Version 6.5 (R2017b) Computer Vision System Toolbox Version 7.2 (R2016b) Computer Vision System Toolbox Version 8.0 (R2017b) Computer Vision System Toolbox Version 7.2 (R2016b) Control System Toolbox Version 10.3 (R2017b) Curve Fitting Toolbox Version 3.5.4 (R2016b) Curve Fitting Toolbox Version 3.5.6 (R2017b) DSP System Toolbox Version 9.3 (R2016b) DSP System Toolbox Version 9.5 (R2017b) Database Toolbox Version 7.0 (R2016b) Database Toolbox Version 8.0 (R2017b) Financial Toolbox Version 5.8 (R2016b) Financial Toolbox Version 5.10 (R2017b) Fixed-Point Designer Version 5.3 (R2016b) Fixed-Point Designer Version 6.0 (R2017b) Fuzzy Logic Toolbox Version 2.2.24 (R2016b) Fuzzy Logic Toolbox Version 2.3 (R2017b) Image Acquisition Toolbox Version 5.1 (R2016b) Global Optimization Toolbox Version 3.4.3 (R2017b) Image Processing Toolbox Version 9.5 (R2016b) Image Acquisition Toolbox Version 5.3 (R2017b) MATLAB Coder Version 3.2 (R2016b) Image Processing Toolbox Version 10.1 (R2017b) MATLAB Compiler Version 6.3 (R2016b) Instrument Control Toolbox Version 3.12 (R2017b) MATLAB Compiler SDK Version 6.3 (R2016b) MATLAB Coder Version 3.4 (R2017b) Mapping Toolbox Version 4.4 (R2016b) MATLAB Compiler Version 6.5 (R2017b) Neural Network Toolbox Version 9.1 (R2016b) MATLAB Compiler SDK Version 6.4 (R2017b) Optimization Toolbox Version 7.5 (R2016b) Mapping Toolbox Version 4.5.1 (R2017b) Parallel Computing Toolbox Version 6.9 (R2016b) Model Predictive Control Toolbox Version 6.0 (R2017b) Partial Differential Equation Toolbox Version 2.3 (R2016b) Neural Network Toolbox Version 11.0 (R2017b) Robust Control Toolbox Version 6.2 (R2016b) Optimization Toolbox Version 8.0 (R2017b) Signal Processing Toolbox Version 7.3 (R2016b) Parallel Computing Toolbox Version 6.11 (R2017b) Simscape Version 4.1 (R2016b) Partial Differential Equation Toolbox Version 2.5 (R2017b) Simscape Multibody Version 4.9 (R2016b) Robust Control Toolbox Version 6.4 (R2017b) Simscape Power Systems Version 6.6 (R2016b) Signal Processing Toolbox Version 7.5 (R2017b) Simulink Coder Version 8.11 (R2016b) Simscape Version 4.3 (R2017b) Simulink Control Design Version 4.4 (R2016b) Simscape Multibody Version 5.1 (R2017b) Simulink Design Optimization Version 3.1 (R2016b) Simscape Power Systems Version 6.8 (R2017b) Simulink Verification and Validation Version 3.12 (R2016b) Simulink Check Version 4.0 (R2017b) Stateflow Version 8.8 (R2016b) Simulink Coder Version 8.13 (R2017b) Statistics and Machine Learning Toolbox Version 11.0 (R2016b) Simulink Control Design Version 5.0 (R2017b) Symbolic Math Toolbox Version 7.1 (R2016b) Simulink Coverage Version 4.0 (R2017b) System Identification Toolbox Version 9.5 (R2016b) Simulink Design Optimization Version 3.3 (R2017b) Wavelet Toolbox Version 4.17 (R2016b) Simulink Requirements Version 1.0 (R2017b) Stateflow Version 9.0 (R2017b) Statistics and Machine Learning Toolbox Version 11.2 (R2017b) Symbolic Math Toolbox Version 8.0 (R2017b) System Identification Toolbox Version 9.7 (R2017b) Wavelet Toolbox Version 4.19 (R2017b)","title":"Facts about Matlab on UBELIX"},{"location":"user-guide/software/matlab.html#running-matlab-on-the-compute-nodes","text":"Submitting a Matlab job to the cluster is very similar to submitting any other serial job. Lets try to run a simple Matlab script which we will put in a file boxfilter.m boxfilter.m % Compute a local mean filter over a neighborhood of 11x11 pixels % Read image into workspace: original = imread ( 'girlface.png' ) ; % Perform the mean filtering: filtered = imboxfilt ( original, 11 ) ; % Save the original and the filtered image side-by-side: imwrite ([ original, filtered ] , 'comparison.png' ) ; Now we need a submission script boxfilter.qsub !#/bin/bash #SBATCH -mail-user=foo@bar.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --job-name=boxfilter #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=2G # Load Matlab form the environment modules module load matlab/R2015b # Tell Matlab to run our box filter.m file and exit matlab -nodisplay -r \"boxfilter, exit\"","title":"Running Matlab on the Compute Nodes"},{"location":"user-guide/software/matlab.html#passing-arguments-to-a-m-file","text":"There are several ways to provide input arguments in Matlab.","title":"Passing Arguments to a m-File"},{"location":"user-guide/software/matlab.html#define-the-variables-before-running-the-script","text":"Lets take the box filter.m example from above. The script is not universal because the name of the input image and the box size is hardcoded in the script. We make the script more generally usable by: boxfilter.m % Compute a local mean filter over a neighborhood of 11x11 pixels % Read image into workspace: original = imread ( inputImg ) ; % Perform the mean filtering: filtered = imboxfilt ( original, x ) ; % Save the original and the filtered image side-by-side: imwrite ([ original, filtered ] , 'comparison.png' ) ; and then: boxfilter.qsub !#/bin/bash ( ... ) # Load Matlab form the environment modules module load matlab/R2015b # Tell Matlab to run our box filter.m file and exit matlab -nodisplay -r \"inputImg='girlface.png'; x=11; boxfilter, exit\"","title":"Define the Variables Before Running the Script"},{"location":"user-guide/software/matlab.html#advanced-topics","text":"","title":"Advanced Topics"},{"location":"user-guide/software/matlab.html#multithreading","text":"By default, MATLAB makes use of the multithreading capabilities of the node on which it is running. It is crucial that you allocate the same number of slots for your job as your job utilizes cores. Disable Computational Multithreading If you do not need multithreading for your application consider to disable computational multithreading by setting the -singleCompThread option when starting MATLAB: matlab -nodisplay -singleCompThread -r \"boxfilter('girlface.png', 'comparison.png', 11); exit\" Disable Computational Multithreading If you do not need multithreading for your application consider to disable computational multithreading by setting the -singleCompThread option when starting MATLAB: matlab -nodisplay -singleCompThread -r \"boxfilter('girlface.png', 'comparison.png', 11); exit\" Running MATLAB in Multithreaded Mode Most of the time, running MATLAB in single-threaded mode will meet your needs. If you have mathematically intense computations that might benefit from multi-threading capabilities provided by MATLAB\u2019s BLAS implementation, then you should limit MATLAB to a well defined number of threads, so that you can allocate the correct number of slots for your job. Use the maxNumCompThreads(N) function to control the number of computational threads:","title":"Multithreading"},{"location":"user-guide/software/pre-installed-software.html","text":"Pre-Installed Software Description This page contains a list of pre-installed software that is available for all UBELIX users. If you want to install custom software yourself, take a look here . If you think that some missing software could be of general interest for the UBELIX community, you can ask us to install the software system wide. Since maintaining software is a lot of work, we will select carefully which software we will install globally. Environment Modules To make certain versions of a software available, the user must first \u201cload/add\u201d the corresponding modulefile. Environment modules allow to maintain different versions of the same software by altering the shell environment variables (e.g. $PATH) accordingly. Each modulefile contains all information needed to configure the shell for a specific software. List all Available Modulefiles $ module avail --------------------------------------------------------------------------------------- /software.el7/modulefiles/all ---------------------------------------------------------------------------------------- Advisor/2018_update3 foss/2017a iccifort/2017.4.196-GCC-6.4.0-2.28 numactl/2.0.11-GCC-5.4.0-2.26 Autotools/20150215-GCC-5.4.0-2.26 foss/2017b iccifort/2018.1.163-GCC-6.4.0-2.28 numactl/2.0.11-GCC-6.3.0-2.27 Autotools/20150215-GCCcore-6.3.0 foss/2018a iccifort/2018.3.222-GCC-7.3.0-2.30 ( D ) numactl/2.0.11-GCCcore-6.4.0 Autotools/20170619-GCCcore-6.4.0 foss/2018b ICU/61.1-GCCcore-6.4.0 numactl/2.0.11-GCCcore-7.3.0 Autotools/20180311-GCCcore-7.3.0 foss/2019a ( D ) IDL/8.6 numactl/2.0.12-GCCcore-8.2.0 ( D ) Autotools/20180311-GCCcore-8.2.0 ( D ) fosscuda/2019a iimpi/2017b OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1 Boost/1.66.0-foss-2018a Gaussian/g09.D01 iimpi/2018a OpenBLAS/0.2.19-GCC-6.3.0-2.27-LAPACK-3.7.0 CMake/3.9.1-GCCcore-6.4.0 Gaussian/g16.A03 ( D ) iimpi/2018b ( D ) OpenBLAS/0.2.20-GCC-6.4.0-2.28 CMake/3.9.5-GCCcore-6.4.0 GCC/5.4.0-2.26 imkl/2017.3.196-iimpi-2017b OpenBLAS/0.3.1-GCC-7.3.0-2.30 CMake/3.10.2-GCCcore-6.4.0 GCC/6.3.0-2.27 imkl/2017.3.196-iompi-2017b OpenBLAS/0.3.5-GCC-8.2.0-2.31.1 ( D ) CMake/3.11.1-GCCcore-6.4.0 GCC/6.4.0-2.28 imkl/2018.1.163-iimpi-2018a OpenMPI/1.10.3-GCC-5.4.0-2.26 CMake/3.11.4-GCCcore-7.3.0 ( D ) GCC/7.3.0-2.30 imkl/2018.1.163-iompi-2018a OpenMPI/2.0.2-GCC-6.3.0-2.27 CUDA/8.0.61 GCC/8.2.0-2.31.1 ( D ) imkl/2018.3.222-iimpi-2018b OpenMPI/2.1.1-GCC-6.4.0-2.28 CUDA/9.0.176 GCCcore/5.4.0 imkl/2018.3.222-iompi-2018b ( D ) OpenMPI/2.1.1-iccifort-2017.4.196-GCC-6.4.0-2.28 CUDA/9.1.85 GCCcore/6.3.0 impi/2017.3.196-iccifort-2017.4.196-GCC-6.4.0-2.28 OpenMPI/2.1.2-GCC-6.4.0-2.28 CUDA/9.2.88 GCCcore/6.4.0 impi/2018.1.163-iccifort-2018.1.163-GCC-6.4.0-2.28 OpenMPI/2.1.2-iccifort-2018.1.163-GCC-6.4.0-2.28 CUDA/10.1.105-GCC-8.2.0-2.31.1 ( D ) GCCcore/7.3.0 impi/2018.3.222-iccifort-2018.3.222-GCC-7.3.0-2.30 ( D ) OpenMPI/3.1.1-GCC-7.3.0-2.30 cuDNN/6.0-CUDA-8.0.61 GCCcore/8.2.0 ( D ) Inspector/2018_update3 OpenMPI/3.1.1-iccifort-2018.3.222-GCC-7.3.0-2.30 cuDNN/7.0.5-CUDA-9.0.176 gcccuda/2019a intel/2017b OpenMPI/3.1.3-GCC-8.2.0-2.31.1 cuDNN/7.0.5-CUDA-9.1.85 GEOS/3.6.2-foss-2018a-Python-3.6.4 intel/2018a OpenMPI/3.1.3-gcccuda-2019a ( D ) cuDNN/7.1.4-CUDA-9.2.88 git-lfs/2.4.2 intel/2018b ( D ) Perl/5.26.1-GCCcore-6.4.0 cuDNN/7.6.0.64-gcccuda-2019a ( D ) GMP/6.1.2-GCCcore-6.4.0 iomkl/2017b PGI/17.10-GCC-6.4.0-2.28 cURL/7.58.0-GCCcore-6.4.0 gompi/2016b iomkl/2018a PGI/18.4-GCC-6.4.0-2.28 cURL/7.60.0-GCCcore-7.3.0 ( D ) gompi/2017a iomkl/2018b ( D ) PGI/19.4-GCC-8.2.0-2.31.1 ( D ) Doxygen/1.8.13-GCCcore-6.4.0 gompi/2017b iompi/2017b Python/2.7.14-foss-2018a Doxygen/1.8.14-GCCcore-7.3.0 ( D ) gompi/2018a iompi/2018a Python/2.7.14-GCCcore-6.4.0-bare EasyBuild/3.6.1 gompi/2018b iompi/2018b ( D ) Python/3.6.4-foss-2018a ( D ) EasyBuild/3.6.2 gompi/2019a ( D ) itac/2018.3.022 R/3.4.4-foss-2018a-X11-20180131 EasyBuild/3.7.1 gompic/2019a Java/1.7.0_60 ScaLAPACK/2.0.2-gompi-2016b-OpenBLAS-0.2.18-LAPACK-3.6.1 EasyBuild/3.9.1 ( D ) GSL/2.4-GCCcore-6.4.0 Java/1.7.0_80 ScaLAPACK/2.0.2-gompi-2017a-OpenBLAS-0.2.19-LAPACK-3.7.0 FFTW/3.3.4-gompi-2016b HDF5/1.10.1-foss-2018a Java/1.8.0_121 ScaLAPACK/2.0.2-gompi-2017b-OpenBLAS-0.2.20 FFTW/3.3.6-gompi-2017a HDF5/1.10.2-foss-2018b ( D ) Java/1.8.0_152 ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20 FFTW/3.3.6-gompi-2017b help2man/1.47.4-GCCcore-6.3.0 Java/1.8.0_162 ( D ) ScaLAPACK/2.0.2-gompi-2018b-OpenBLAS-0.3.1 FFTW/3.3.7-gompi-2018a help2man/1.47.4-GCCcore-6.4.0 libsndfile/1.0.28-GCCcore-6.4.0 ScaLAPACK/2.0.2-gompi-2019a-OpenBLAS-0.3.5 FFTW/3.3.7-intel-2017b help2man/1.47.4-GCCcore-7.3.0 LLVM/5.0.1-GCCcore-6.4.0 ScaLAPACK/2.0.2-gompic-2019a-OpenBLAS-0.3.5 ( D ) FFTW/3.3.7-intel-2018a help2man/1.47.4 Mako/1.0.7-foss-2018a-Python-2.7.14 SQLite/3.20.1-GCCcore-6.4.0 FFTW/3.3.7-iomkl-2018a help2man/1.47.7-GCCcore-8.2.0 ( D ) MATLAB/2016b SQLite/3.21.0-GCCcore-6.4.0 ( D ) FFTW/3.3.8-gompi-2018b hwloc/1.11.3-GCC-5.4.0-2.26 MATLAB/2017b tmux/2.7 FFTW/3.3.8-gompi-2019a hwloc/1.11.5-GCC-6.3.0-2.27 MATLAB/2018b ( D ) UDUNITS/2.2.26-foss-2018a FFTW/3.3.8-gompic-2019a hwloc/1.11.7-GCCcore-6.4.0 netCDF/4.6.0-foss-2018a vital-it/7 FFTW/3.3.8-intel-2018b hwloc/1.11.8-GCCcore-6.4.0 netCDF/4.6.1-foss-2018b ( D ) VTune/2018_update3 FFTW/3.3.8-iomkl-2018b ( D ) hwloc/1.11.10-GCCcore-7.3.0 nettle/3.4-foss-2018a X11/20180131-GCCcore-6.4.0 foss/2016b hwloc/1.11.11-GCCcore-8.2.0 ( D ) NLopt/2.4.2-foss-2018a Where: D: Default Module Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\" . Bioinformatics Software In co-operation with the Vital-IT Group of the SIB Swiss Institute of Bioinformatics , a large set of bioinformatics software tools and databases is available to the life science community. To also list all modulefiles provided by Vital-IT, you have to first load the vital-it modulefile: Loading the vital-it modulefile automatically configures the environment to use specific versions of selected software, e.g. python v2.7.5, and gcc v4.9.1 $ module load vital-it && module avail ---------------------------------------------------------------------------------------------- /software/module ---------------------------------------------------------------------------------------------- Blast/blast/latest SequenceAnalysis/primer3/2.3.7 UHTS/Analysis/mummer/4.0.0beta1 ( D ) Blast/blast/2.2.26 ( D ) SequenceAnalysis/PrimerDesign/iPCR/1.0 UHTS/Analysis/NanoOK/1.2.6 Blast/ncbi-blast/latest SequenceAnalysis/ProtoGene/4.2.2 UHTS/Analysis/nanoraw/0.5 Blast/ncbi-blast/2.2.31+ SequenceAnalysis/readseq/2.1.30 UHTS/Analysis/oncodrivefm/1.0.3 ( ... ) SequenceAnalysis/OrthologyAnalysis/OMA/2.1.1 UHTS/Analysis/msprime/0.7.0 Utility/rpy2/2.9.1 ( D ) SequenceAnalysis/orthomclSoftware/2.0.9 UHTS/Analysis/MultiQC/1.3 Utility/Solver/SoPlex/4.0.0 SequenceAnalysis/patsearch/1 UHTS/Analysis/MultiQC/1.7 ( D ) Utility/Tarql/1.1 SequenceAnalysis/pftools/2.3.4 UHTS/Analysis/mummer/3.9.4alpha Utility/UCSC-utils/359 SequenceAnalysis/pftools/2.3.5.d ( D ) UHTS/Analysis/mummer/4.0.0beta --------------------------------------------------------------------------------------- /software.el7/modulefiles/all ---------------------------------------------------------------------------------------- Advisor/2018_update3 foss/2017a iccifort/2017.4.196-GCC-6.4.0-2.28 numactl/2.0.11-GCC-5.4.0-2.26 Autotools/20150215-GCC-5.4.0-2.26 foss/2017b iccifort/2018.1.163-GCC-6.4.0-2.28 numactl/2.0.11-GCC-6.3.0-2.27 Autotools/20150215-GCCcore-6.3.0 foss/2018a iccifort/2018.3.222-GCC-7.3.0-2.30 ( D ) numactl/2.0.11-GCCcore-6.4.0 Autotools/20170619-GCCcore-6.4.0 foss/2018b ICU/61.1-GCCcore-6.4.0 numactl/2.0.11-GCCcore-7.3.0 Autotools/20180311-GCCcore-7.3.0 foss/2019a ( D ) IDL/8.6 numactl/2.0.12-GCCcore-8.2.0 ( D ) Autotools/20180311-GCCcore-8.2.0 ( D ) fosscuda/2019a iimpi/2017b OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1 Boost/1.66.0-foss-2018a Gaussian/g09.D01 iimpi/2018a OpenBLAS/0.2.19-GCC-6.3.0-2.27-LAPACK-3.7.0 CMake/3.9.1-GCCcore-6.4.0 Gaussian/g16.A03 ( D ) iimpi/2018b ( D ) OpenBLAS/0.2.20-GCC-6.4.0-2.28 CMake/3.9.5-GCCcore-6.4.0 GCC/5.4.0-2.26 imkl/2017.3.196-iimpi-2017b OpenBLAS/0.3.1-GCC-7.3.0-2.30 CMake/3.10.2-GCCcore-6.4.0 GCC/6.3.0-2.27 imkl/2017.3.196-iompi-2017b OpenBLAS/0.3.5-GCC-8.2.0-2.31.1 ( D ) CMake/3.11.1-GCCcore-6.4.0 GCC/6.4.0-2.28 imkl/2018.1.163-iimpi-2018a OpenMPI/1.10.3-GCC-5.4.0-2.26 CMake/3.11.4-GCCcore-7.3.0 ( D ) GCC/7.3.0-2.30 imkl/2018.1.163-iompi-2018a OpenMPI/2.0.2-GCC-6.3.0-2.27 CUDA/8.0.61 GCC/8.2.0-2.31.1 ( D ) imkl/2018.3.222-iimpi-2018b OpenMPI/2.1.1-GCC-6.4.0-2.28 CUDA/9.0.176 GCCcore/5.4.0 imkl/2018.3.222-iompi-2018b ( D ) OpenMPI/2.1.1-iccifort-2017.4.196-GCC-6.4.0-2.28 CUDA/9.1.85 GCCcore/6.3.0 impi/2017.3.196-iccifort-2017.4.196-GCC-6.4.0-2.28 OpenMPI/2.1.2-GCC-6.4.0-2.28 CUDA/9.2.88 GCCcore/6.4.0 impi/2018.1.163-iccifort-2018.1.163-GCC-6.4.0-2.28 OpenMPI/2.1.2-iccifort-2018.1.163-GCC-6.4.0-2.28 CUDA/10.1.105-GCC-8.2.0-2.31.1 ( D ) GCCcore/7.3.0 impi/2018.3.222-iccifort-2018.3.222-GCC-7.3.0-2.30 ( D ) OpenMPI/3.1.1-GCC-7.3.0-2.30 cuDNN/6.0-CUDA-8.0.61 GCCcore/8.2.0 ( D ) Inspector/2018_update3 OpenMPI/3.1.1-iccifort-2018.3.222-GCC-7.3.0-2.30 cuDNN/7.0.5-CUDA-9.0.176 gcccuda/2019a intel/2017b OpenMPI/3.1.3-GCC-8.2.0-2.31.1 cuDNN/7.0.5-CUDA-9.1.85 GEOS/3.6.2-foss-2018a-Python-3.6.4 intel/2018a OpenMPI/3.1.3-gcccuda-2019a ( D ) cuDNN/7.1.4-CUDA-9.2.88 git-lfs/2.4.2 intel/2018b ( D ) Perl/5.26.1-GCCcore-6.4.0 cuDNN/7.6.0.64-gcccuda-2019a ( D ) GMP/6.1.2-GCCcore-6.4.0 iomkl/2017b PGI/17.10-GCC-6.4.0-2.28 cURL/7.58.0-GCCcore-6.4.0 gompi/2016b iomkl/2018a PGI/18.4-GCC-6.4.0-2.28 cURL/7.60.0-GCCcore-7.3.0 ( D ) gompi/2017a iomkl/2018b ( D ) PGI/19.4-GCC-8.2.0-2.31.1 ( D ) Doxygen/1.8.13-GCCcore-6.4.0 gompi/2017b iompi/2017b Python/2.7.14-foss-2018a Doxygen/1.8.14-GCCcore-7.3.0 ( D ) gompi/2018a iompi/2018a Python/2.7.14-GCCcore-6.4.0-bare EasyBuild/3.6.1 gompi/2018b iompi/2018b ( D ) Python/3.6.4-foss-2018a ( D ) EasyBuild/3.6.2 gompi/2019a ( D ) itac/2018.3.022 R/3.4.4-foss-2018a-X11-20180131 EasyBuild/3.7.1 gompic/2019a Java/1.7.0_60 ScaLAPACK/2.0.2-gompi-2016b-OpenBLAS-0.2.18-LAPACK-3.6.1 EasyBuild/3.9.1 ( D ) GSL/2.4-GCCcore-6.4.0 Java/1.7.0_80 ScaLAPACK/2.0.2-gompi-2017a-OpenBLAS-0.2.19-LAPACK-3.7.0 FFTW/3.3.4-gompi-2016b HDF5/1.10.1-foss-2018a Java/1.8.0_121 ScaLAPACK/2.0.2-gompi-2017b-OpenBLAS-0.2.20 FFTW/3.3.6-gompi-2017a HDF5/1.10.2-foss-2018b ( D ) Java/1.8.0_152 ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20 FFTW/3.3.6-gompi-2017b help2man/1.47.4-GCCcore-6.3.0 Java/1.8.0_162 ( D ) ScaLAPACK/2.0.2-gompi-2018b-OpenBLAS-0.3.1 FFTW/3.3.7-gompi-2018a help2man/1.47.4-GCCcore-6.4.0 libsndfile/1.0.28-GCCcore-6.4.0 ScaLAPACK/2.0.2-gompi-2019a-OpenBLAS-0.3.5 FFTW/3.3.7-intel-2017b help2man/1.47.4-GCCcore-7.3.0 LLVM/5.0.1-GCCcore-6.4.0 ScaLAPACK/2.0.2-gompic-2019a-OpenBLAS-0.3.5 ( D ) FFTW/3.3.7-intel-2018a help2man/1.47.4 Mako/1.0.7-foss-2018a-Python-2.7.14 SQLite/3.20.1-GCCcore-6.4.0 FFTW/3.3.7-iomkl-2018a help2man/1.47.7-GCCcore-8.2.0 ( D ) MATLAB/2016b SQLite/3.21.0-GCCcore-6.4.0 ( D ) FFTW/3.3.8-gompi-2018b hwloc/1.11.3-GCC-5.4.0-2.26 MATLAB/2017b tmux/2.7 FFTW/3.3.8-gompi-2019a hwloc/1.11.5-GCC-6.3.0-2.27 MATLAB/2018b ( D ) UDUNITS/2.2.26-foss-2018a FFTW/3.3.8-gompic-2019a hwloc/1.11.7-GCCcore-6.4.0 netCDF/4.6.0-foss-2018a vital-it/7 ( L ) FFTW/3.3.8-intel-2018b hwloc/1.11.8-GCCcore-6.4.0 netCDF/4.6.1-foss-2018b ( D ) VTune/2018_update3 FFTW/3.3.8-iomkl-2018b ( D ) hwloc/1.11.10-GCCcore-7.3.0 nettle/3.4-foss-2018a X11/20180131-GCCcore-6.4.0 foss/2016b hwloc/1.11.11-GCCcore-8.2.0 ( D ) NLopt/2.4.2-foss-2018a Where: L: Module is loaded D: Default Module Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\" . Load/Add a Modulefile module load OpenMPI/3.1.3-GCC-8.2.0-2.31.1 or equivalently: $ module add OpenMPI/3.1.3-GCC-8.2.0-2.31.1 List all Loaded Modulefiles $ module list Currently Loaded Modules: 1 ) GCCcore/8.2.0 3 ) binutils/.2.31.1-GCCcore-8.2.0 ( H ) 5 ) numactl/2.0.12-GCCcore-8.2.0 7 ) libxml2/.2.9.8-GCCcore-8.2.0 ( H ) 9 ) hwloc/1.11.11-GCCcore-8.2.0 2 ) zlib/.1.2.11-GCCcore-8.2.0 ( H ) 4 ) GCC/8.2.0-2.31.1 6 ) XZ/.5.2.4-GCCcore-8.2.0 ( H ) 8 ) libpciaccess/.0.14-GCCcore-8.2.0 ( H ) 10 ) OpenMPI/3.1.3-GCC-8.2.0-2.31.1 Where: H: Hidden Module Unload/remove a Modulefile This will only unload the specified modulefile, but not the dependencies that where automatically loaded when loading the specified modulefile (see purge below). $ module unload OpenMPI/3.1.3-GCC-8.2.0-2.31.1 or equivalently: $ module rm OpenMPI/3.1.3-GCC-8.2.0-2.31.1 Purge all Modulefiles This will unload all previously loaded modulefiles. $ module purge","title":"Pre-Installed Software"},{"location":"user-guide/software/pre-installed-software.html#pre-installed-software","text":"","title":"Pre-Installed Software"},{"location":"user-guide/software/pre-installed-software.html#description","text":"This page contains a list of pre-installed software that is available for all UBELIX users. If you want to install custom software yourself, take a look here . If you think that some missing software could be of general interest for the UBELIX community, you can ask us to install the software system wide. Since maintaining software is a lot of work, we will select carefully which software we will install globally.","title":"Description"},{"location":"user-guide/software/pre-installed-software.html#environment-modules","text":"To make certain versions of a software available, the user must first \u201cload/add\u201d the corresponding modulefile. Environment modules allow to maintain different versions of the same software by altering the shell environment variables (e.g. $PATH) accordingly. Each modulefile contains all information needed to configure the shell for a specific software.","title":"Environment Modules"},{"location":"user-guide/software/pre-installed-software.html#list-all-available-modulefiles","text":"$ module avail --------------------------------------------------------------------------------------- /software.el7/modulefiles/all ---------------------------------------------------------------------------------------- Advisor/2018_update3 foss/2017a iccifort/2017.4.196-GCC-6.4.0-2.28 numactl/2.0.11-GCC-5.4.0-2.26 Autotools/20150215-GCC-5.4.0-2.26 foss/2017b iccifort/2018.1.163-GCC-6.4.0-2.28 numactl/2.0.11-GCC-6.3.0-2.27 Autotools/20150215-GCCcore-6.3.0 foss/2018a iccifort/2018.3.222-GCC-7.3.0-2.30 ( D ) numactl/2.0.11-GCCcore-6.4.0 Autotools/20170619-GCCcore-6.4.0 foss/2018b ICU/61.1-GCCcore-6.4.0 numactl/2.0.11-GCCcore-7.3.0 Autotools/20180311-GCCcore-7.3.0 foss/2019a ( D ) IDL/8.6 numactl/2.0.12-GCCcore-8.2.0 ( D ) Autotools/20180311-GCCcore-8.2.0 ( D ) fosscuda/2019a iimpi/2017b OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1 Boost/1.66.0-foss-2018a Gaussian/g09.D01 iimpi/2018a OpenBLAS/0.2.19-GCC-6.3.0-2.27-LAPACK-3.7.0 CMake/3.9.1-GCCcore-6.4.0 Gaussian/g16.A03 ( D ) iimpi/2018b ( D ) OpenBLAS/0.2.20-GCC-6.4.0-2.28 CMake/3.9.5-GCCcore-6.4.0 GCC/5.4.0-2.26 imkl/2017.3.196-iimpi-2017b OpenBLAS/0.3.1-GCC-7.3.0-2.30 CMake/3.10.2-GCCcore-6.4.0 GCC/6.3.0-2.27 imkl/2017.3.196-iompi-2017b OpenBLAS/0.3.5-GCC-8.2.0-2.31.1 ( D ) CMake/3.11.1-GCCcore-6.4.0 GCC/6.4.0-2.28 imkl/2018.1.163-iimpi-2018a OpenMPI/1.10.3-GCC-5.4.0-2.26 CMake/3.11.4-GCCcore-7.3.0 ( D ) GCC/7.3.0-2.30 imkl/2018.1.163-iompi-2018a OpenMPI/2.0.2-GCC-6.3.0-2.27 CUDA/8.0.61 GCC/8.2.0-2.31.1 ( D ) imkl/2018.3.222-iimpi-2018b OpenMPI/2.1.1-GCC-6.4.0-2.28 CUDA/9.0.176 GCCcore/5.4.0 imkl/2018.3.222-iompi-2018b ( D ) OpenMPI/2.1.1-iccifort-2017.4.196-GCC-6.4.0-2.28 CUDA/9.1.85 GCCcore/6.3.0 impi/2017.3.196-iccifort-2017.4.196-GCC-6.4.0-2.28 OpenMPI/2.1.2-GCC-6.4.0-2.28 CUDA/9.2.88 GCCcore/6.4.0 impi/2018.1.163-iccifort-2018.1.163-GCC-6.4.0-2.28 OpenMPI/2.1.2-iccifort-2018.1.163-GCC-6.4.0-2.28 CUDA/10.1.105-GCC-8.2.0-2.31.1 ( D ) GCCcore/7.3.0 impi/2018.3.222-iccifort-2018.3.222-GCC-7.3.0-2.30 ( D ) OpenMPI/3.1.1-GCC-7.3.0-2.30 cuDNN/6.0-CUDA-8.0.61 GCCcore/8.2.0 ( D ) Inspector/2018_update3 OpenMPI/3.1.1-iccifort-2018.3.222-GCC-7.3.0-2.30 cuDNN/7.0.5-CUDA-9.0.176 gcccuda/2019a intel/2017b OpenMPI/3.1.3-GCC-8.2.0-2.31.1 cuDNN/7.0.5-CUDA-9.1.85 GEOS/3.6.2-foss-2018a-Python-3.6.4 intel/2018a OpenMPI/3.1.3-gcccuda-2019a ( D ) cuDNN/7.1.4-CUDA-9.2.88 git-lfs/2.4.2 intel/2018b ( D ) Perl/5.26.1-GCCcore-6.4.0 cuDNN/7.6.0.64-gcccuda-2019a ( D ) GMP/6.1.2-GCCcore-6.4.0 iomkl/2017b PGI/17.10-GCC-6.4.0-2.28 cURL/7.58.0-GCCcore-6.4.0 gompi/2016b iomkl/2018a PGI/18.4-GCC-6.4.0-2.28 cURL/7.60.0-GCCcore-7.3.0 ( D ) gompi/2017a iomkl/2018b ( D ) PGI/19.4-GCC-8.2.0-2.31.1 ( D ) Doxygen/1.8.13-GCCcore-6.4.0 gompi/2017b iompi/2017b Python/2.7.14-foss-2018a Doxygen/1.8.14-GCCcore-7.3.0 ( D ) gompi/2018a iompi/2018a Python/2.7.14-GCCcore-6.4.0-bare EasyBuild/3.6.1 gompi/2018b iompi/2018b ( D ) Python/3.6.4-foss-2018a ( D ) EasyBuild/3.6.2 gompi/2019a ( D ) itac/2018.3.022 R/3.4.4-foss-2018a-X11-20180131 EasyBuild/3.7.1 gompic/2019a Java/1.7.0_60 ScaLAPACK/2.0.2-gompi-2016b-OpenBLAS-0.2.18-LAPACK-3.6.1 EasyBuild/3.9.1 ( D ) GSL/2.4-GCCcore-6.4.0 Java/1.7.0_80 ScaLAPACK/2.0.2-gompi-2017a-OpenBLAS-0.2.19-LAPACK-3.7.0 FFTW/3.3.4-gompi-2016b HDF5/1.10.1-foss-2018a Java/1.8.0_121 ScaLAPACK/2.0.2-gompi-2017b-OpenBLAS-0.2.20 FFTW/3.3.6-gompi-2017a HDF5/1.10.2-foss-2018b ( D ) Java/1.8.0_152 ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20 FFTW/3.3.6-gompi-2017b help2man/1.47.4-GCCcore-6.3.0 Java/1.8.0_162 ( D ) ScaLAPACK/2.0.2-gompi-2018b-OpenBLAS-0.3.1 FFTW/3.3.7-gompi-2018a help2man/1.47.4-GCCcore-6.4.0 libsndfile/1.0.28-GCCcore-6.4.0 ScaLAPACK/2.0.2-gompi-2019a-OpenBLAS-0.3.5 FFTW/3.3.7-intel-2017b help2man/1.47.4-GCCcore-7.3.0 LLVM/5.0.1-GCCcore-6.4.0 ScaLAPACK/2.0.2-gompic-2019a-OpenBLAS-0.3.5 ( D ) FFTW/3.3.7-intel-2018a help2man/1.47.4 Mako/1.0.7-foss-2018a-Python-2.7.14 SQLite/3.20.1-GCCcore-6.4.0 FFTW/3.3.7-iomkl-2018a help2man/1.47.7-GCCcore-8.2.0 ( D ) MATLAB/2016b SQLite/3.21.0-GCCcore-6.4.0 ( D ) FFTW/3.3.8-gompi-2018b hwloc/1.11.3-GCC-5.4.0-2.26 MATLAB/2017b tmux/2.7 FFTW/3.3.8-gompi-2019a hwloc/1.11.5-GCC-6.3.0-2.27 MATLAB/2018b ( D ) UDUNITS/2.2.26-foss-2018a FFTW/3.3.8-gompic-2019a hwloc/1.11.7-GCCcore-6.4.0 netCDF/4.6.0-foss-2018a vital-it/7 FFTW/3.3.8-intel-2018b hwloc/1.11.8-GCCcore-6.4.0 netCDF/4.6.1-foss-2018b ( D ) VTune/2018_update3 FFTW/3.3.8-iomkl-2018b ( D ) hwloc/1.11.10-GCCcore-7.3.0 nettle/3.4-foss-2018a X11/20180131-GCCcore-6.4.0 foss/2016b hwloc/1.11.11-GCCcore-8.2.0 ( D ) NLopt/2.4.2-foss-2018a Where: D: Default Module Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\" .","title":"List all Available Modulefiles"},{"location":"user-guide/software/pre-installed-software.html#bioinformatics-software","text":"In co-operation with the Vital-IT Group of the SIB Swiss Institute of Bioinformatics , a large set of bioinformatics software tools and databases is available to the life science community. To also list all modulefiles provided by Vital-IT, you have to first load the vital-it modulefile: Loading the vital-it modulefile automatically configures the environment to use specific versions of selected software, e.g. python v2.7.5, and gcc v4.9.1 $ module load vital-it && module avail ---------------------------------------------------------------------------------------------- /software/module ---------------------------------------------------------------------------------------------- Blast/blast/latest SequenceAnalysis/primer3/2.3.7 UHTS/Analysis/mummer/4.0.0beta1 ( D ) Blast/blast/2.2.26 ( D ) SequenceAnalysis/PrimerDesign/iPCR/1.0 UHTS/Analysis/NanoOK/1.2.6 Blast/ncbi-blast/latest SequenceAnalysis/ProtoGene/4.2.2 UHTS/Analysis/nanoraw/0.5 Blast/ncbi-blast/2.2.31+ SequenceAnalysis/readseq/2.1.30 UHTS/Analysis/oncodrivefm/1.0.3 ( ... ) SequenceAnalysis/OrthologyAnalysis/OMA/2.1.1 UHTS/Analysis/msprime/0.7.0 Utility/rpy2/2.9.1 ( D ) SequenceAnalysis/orthomclSoftware/2.0.9 UHTS/Analysis/MultiQC/1.3 Utility/Solver/SoPlex/4.0.0 SequenceAnalysis/patsearch/1 UHTS/Analysis/MultiQC/1.7 ( D ) Utility/Tarql/1.1 SequenceAnalysis/pftools/2.3.4 UHTS/Analysis/mummer/3.9.4alpha Utility/UCSC-utils/359 SequenceAnalysis/pftools/2.3.5.d ( D ) UHTS/Analysis/mummer/4.0.0beta --------------------------------------------------------------------------------------- /software.el7/modulefiles/all ---------------------------------------------------------------------------------------- Advisor/2018_update3 foss/2017a iccifort/2017.4.196-GCC-6.4.0-2.28 numactl/2.0.11-GCC-5.4.0-2.26 Autotools/20150215-GCC-5.4.0-2.26 foss/2017b iccifort/2018.1.163-GCC-6.4.0-2.28 numactl/2.0.11-GCC-6.3.0-2.27 Autotools/20150215-GCCcore-6.3.0 foss/2018a iccifort/2018.3.222-GCC-7.3.0-2.30 ( D ) numactl/2.0.11-GCCcore-6.4.0 Autotools/20170619-GCCcore-6.4.0 foss/2018b ICU/61.1-GCCcore-6.4.0 numactl/2.0.11-GCCcore-7.3.0 Autotools/20180311-GCCcore-7.3.0 foss/2019a ( D ) IDL/8.6 numactl/2.0.12-GCCcore-8.2.0 ( D ) Autotools/20180311-GCCcore-8.2.0 ( D ) fosscuda/2019a iimpi/2017b OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1 Boost/1.66.0-foss-2018a Gaussian/g09.D01 iimpi/2018a OpenBLAS/0.2.19-GCC-6.3.0-2.27-LAPACK-3.7.0 CMake/3.9.1-GCCcore-6.4.0 Gaussian/g16.A03 ( D ) iimpi/2018b ( D ) OpenBLAS/0.2.20-GCC-6.4.0-2.28 CMake/3.9.5-GCCcore-6.4.0 GCC/5.4.0-2.26 imkl/2017.3.196-iimpi-2017b OpenBLAS/0.3.1-GCC-7.3.0-2.30 CMake/3.10.2-GCCcore-6.4.0 GCC/6.3.0-2.27 imkl/2017.3.196-iompi-2017b OpenBLAS/0.3.5-GCC-8.2.0-2.31.1 ( D ) CMake/3.11.1-GCCcore-6.4.0 GCC/6.4.0-2.28 imkl/2018.1.163-iimpi-2018a OpenMPI/1.10.3-GCC-5.4.0-2.26 CMake/3.11.4-GCCcore-7.3.0 ( D ) GCC/7.3.0-2.30 imkl/2018.1.163-iompi-2018a OpenMPI/2.0.2-GCC-6.3.0-2.27 CUDA/8.0.61 GCC/8.2.0-2.31.1 ( D ) imkl/2018.3.222-iimpi-2018b OpenMPI/2.1.1-GCC-6.4.0-2.28 CUDA/9.0.176 GCCcore/5.4.0 imkl/2018.3.222-iompi-2018b ( D ) OpenMPI/2.1.1-iccifort-2017.4.196-GCC-6.4.0-2.28 CUDA/9.1.85 GCCcore/6.3.0 impi/2017.3.196-iccifort-2017.4.196-GCC-6.4.0-2.28 OpenMPI/2.1.2-GCC-6.4.0-2.28 CUDA/9.2.88 GCCcore/6.4.0 impi/2018.1.163-iccifort-2018.1.163-GCC-6.4.0-2.28 OpenMPI/2.1.2-iccifort-2018.1.163-GCC-6.4.0-2.28 CUDA/10.1.105-GCC-8.2.0-2.31.1 ( D ) GCCcore/7.3.0 impi/2018.3.222-iccifort-2018.3.222-GCC-7.3.0-2.30 ( D ) OpenMPI/3.1.1-GCC-7.3.0-2.30 cuDNN/6.0-CUDA-8.0.61 GCCcore/8.2.0 ( D ) Inspector/2018_update3 OpenMPI/3.1.1-iccifort-2018.3.222-GCC-7.3.0-2.30 cuDNN/7.0.5-CUDA-9.0.176 gcccuda/2019a intel/2017b OpenMPI/3.1.3-GCC-8.2.0-2.31.1 cuDNN/7.0.5-CUDA-9.1.85 GEOS/3.6.2-foss-2018a-Python-3.6.4 intel/2018a OpenMPI/3.1.3-gcccuda-2019a ( D ) cuDNN/7.1.4-CUDA-9.2.88 git-lfs/2.4.2 intel/2018b ( D ) Perl/5.26.1-GCCcore-6.4.0 cuDNN/7.6.0.64-gcccuda-2019a ( D ) GMP/6.1.2-GCCcore-6.4.0 iomkl/2017b PGI/17.10-GCC-6.4.0-2.28 cURL/7.58.0-GCCcore-6.4.0 gompi/2016b iomkl/2018a PGI/18.4-GCC-6.4.0-2.28 cURL/7.60.0-GCCcore-7.3.0 ( D ) gompi/2017a iomkl/2018b ( D ) PGI/19.4-GCC-8.2.0-2.31.1 ( D ) Doxygen/1.8.13-GCCcore-6.4.0 gompi/2017b iompi/2017b Python/2.7.14-foss-2018a Doxygen/1.8.14-GCCcore-7.3.0 ( D ) gompi/2018a iompi/2018a Python/2.7.14-GCCcore-6.4.0-bare EasyBuild/3.6.1 gompi/2018b iompi/2018b ( D ) Python/3.6.4-foss-2018a ( D ) EasyBuild/3.6.2 gompi/2019a ( D ) itac/2018.3.022 R/3.4.4-foss-2018a-X11-20180131 EasyBuild/3.7.1 gompic/2019a Java/1.7.0_60 ScaLAPACK/2.0.2-gompi-2016b-OpenBLAS-0.2.18-LAPACK-3.6.1 EasyBuild/3.9.1 ( D ) GSL/2.4-GCCcore-6.4.0 Java/1.7.0_80 ScaLAPACK/2.0.2-gompi-2017a-OpenBLAS-0.2.19-LAPACK-3.7.0 FFTW/3.3.4-gompi-2016b HDF5/1.10.1-foss-2018a Java/1.8.0_121 ScaLAPACK/2.0.2-gompi-2017b-OpenBLAS-0.2.20 FFTW/3.3.6-gompi-2017a HDF5/1.10.2-foss-2018b ( D ) Java/1.8.0_152 ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20 FFTW/3.3.6-gompi-2017b help2man/1.47.4-GCCcore-6.3.0 Java/1.8.0_162 ( D ) ScaLAPACK/2.0.2-gompi-2018b-OpenBLAS-0.3.1 FFTW/3.3.7-gompi-2018a help2man/1.47.4-GCCcore-6.4.0 libsndfile/1.0.28-GCCcore-6.4.0 ScaLAPACK/2.0.2-gompi-2019a-OpenBLAS-0.3.5 FFTW/3.3.7-intel-2017b help2man/1.47.4-GCCcore-7.3.0 LLVM/5.0.1-GCCcore-6.4.0 ScaLAPACK/2.0.2-gompic-2019a-OpenBLAS-0.3.5 ( D ) FFTW/3.3.7-intel-2018a help2man/1.47.4 Mako/1.0.7-foss-2018a-Python-2.7.14 SQLite/3.20.1-GCCcore-6.4.0 FFTW/3.3.7-iomkl-2018a help2man/1.47.7-GCCcore-8.2.0 ( D ) MATLAB/2016b SQLite/3.21.0-GCCcore-6.4.0 ( D ) FFTW/3.3.8-gompi-2018b hwloc/1.11.3-GCC-5.4.0-2.26 MATLAB/2017b tmux/2.7 FFTW/3.3.8-gompi-2019a hwloc/1.11.5-GCC-6.3.0-2.27 MATLAB/2018b ( D ) UDUNITS/2.2.26-foss-2018a FFTW/3.3.8-gompic-2019a hwloc/1.11.7-GCCcore-6.4.0 netCDF/4.6.0-foss-2018a vital-it/7 ( L ) FFTW/3.3.8-intel-2018b hwloc/1.11.8-GCCcore-6.4.0 netCDF/4.6.1-foss-2018b ( D ) VTune/2018_update3 FFTW/3.3.8-iomkl-2018b ( D ) hwloc/1.11.10-GCCcore-7.3.0 nettle/3.4-foss-2018a X11/20180131-GCCcore-6.4.0 foss/2016b hwloc/1.11.11-GCCcore-8.2.0 ( D ) NLopt/2.4.2-foss-2018a Where: L: Module is loaded D: Default Module Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\" .","title":"Bioinformatics Software"},{"location":"user-guide/software/pre-installed-software.html#loadadd-a-modulefile","text":"module load OpenMPI/3.1.3-GCC-8.2.0-2.31.1 or equivalently: $ module add OpenMPI/3.1.3-GCC-8.2.0-2.31.1","title":"Load/Add a Modulefile"},{"location":"user-guide/software/pre-installed-software.html#list-all-loaded-modulefiles","text":"$ module list Currently Loaded Modules: 1 ) GCCcore/8.2.0 3 ) binutils/.2.31.1-GCCcore-8.2.0 ( H ) 5 ) numactl/2.0.12-GCCcore-8.2.0 7 ) libxml2/.2.9.8-GCCcore-8.2.0 ( H ) 9 ) hwloc/1.11.11-GCCcore-8.2.0 2 ) zlib/.1.2.11-GCCcore-8.2.0 ( H ) 4 ) GCC/8.2.0-2.31.1 6 ) XZ/.5.2.4-GCCcore-8.2.0 ( H ) 8 ) libpciaccess/.0.14-GCCcore-8.2.0 ( H ) 10 ) OpenMPI/3.1.3-GCC-8.2.0-2.31.1 Where: H: Hidden Module","title":"List all Loaded Modulefiles"},{"location":"user-guide/software/pre-installed-software.html#unloadremove-a-modulefile","text":"This will only unload the specified modulefile, but not the dependencies that where automatically loaded when loading the specified modulefile (see purge below). $ module unload OpenMPI/3.1.3-GCC-8.2.0-2.31.1 or equivalently: $ module rm OpenMPI/3.1.3-GCC-8.2.0-2.31.1","title":"Unload/remove a Modulefile"},{"location":"user-guide/software/pre-installed-software.html#purge-all-modulefiles","text":"This will unload all previously loaded modulefiles. $ module purge","title":"Purge all Modulefiles"},{"location":"user-guide/software/python.html","text":"Python Description Some useful information on using Python. Advanced Topics Managing Virtual Environments, Versions with Anaconda Anaconda is a high performance distribution of Python that includes the most popular packages for data science (numpy, scipy,\u2026). It also features conda, a package, dependency and environment manager. With Anaconda you can run multiple versions of Python in isolated environments. Installing Anaconda Download the appropriate installer for the default Python environment. You can install other Python versions later by creating additional environments (see below): Python 2.7 $ wget http://repo.continuum.io/archive/Anaconda2-5.3.1-Linux-x86_64.sh or Python 3.7 $ wget http://repo.continuum.io/archive/Anaconda3-5.3.1-Linux-x86_64.sh Run the installer and follow the instructions: Python 2.7 $ bash Anaconda2-5.3.1-Linux-x86_64.sh or Python 3.7 $ bash Anaconda3-5.3.1-Linux-x86_64.sh This will create a default environment with the selected version of Python and adds numerous packages to the environment. After prepending the Anaconda bin directory to the path open a new terminal for the change to become active. Test the installation by listing all installed packages: $ conda list","title":"Python"},{"location":"user-guide/software/python.html#python","text":"","title":"Python"},{"location":"user-guide/software/python.html#description","text":"Some useful information on using Python.","title":"Description"},{"location":"user-guide/software/python.html#advanced-topics","text":"","title":"Advanced Topics"},{"location":"user-guide/software/python.html#managing-virtual-environments-versions-with-anaconda","text":"Anaconda is a high performance distribution of Python that includes the most popular packages for data science (numpy, scipy,\u2026). It also features conda, a package, dependency and environment manager. With Anaconda you can run multiple versions of Python in isolated environments.","title":"Managing Virtual Environments, Versions with Anaconda"},{"location":"user-guide/software/python.html#installing-anaconda","text":"Download the appropriate installer for the default Python environment. You can install other Python versions later by creating additional environments (see below): Python 2.7 $ wget http://repo.continuum.io/archive/Anaconda2-5.3.1-Linux-x86_64.sh or Python 3.7 $ wget http://repo.continuum.io/archive/Anaconda3-5.3.1-Linux-x86_64.sh Run the installer and follow the instructions: Python 2.7 $ bash Anaconda2-5.3.1-Linux-x86_64.sh or Python 3.7 $ bash Anaconda3-5.3.1-Linux-x86_64.sh This will create a default environment with the selected version of Python and adds numerous packages to the environment. After prepending the Anaconda bin directory to the path open a new terminal for the change to become active. Test the installation by listing all installed packages: $ conda list","title":"Installing Anaconda"},{"location":"user-guide/software/r.html","text":"R Description UBELIX no longer features the R version from EPEL as this version gets automatically updated and therefore things are not reproducible. R isn now provided by an environment module and must be loaded explicitly: module load R/3.4.4-foss-2018a-X11-20180131 -bash-4.1$ R --version R version 3 .4.4 ( 2018 -03-15 ) -- \"Someone to Lean On\" Copyright ( C ) 2018 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu ( 64 -bit ) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3 . For more information about these matters see http://www.gnu.org/licenses/. The Vital-IT project is also providing some versions. The following commands will list the available versions: module load vital-it module avail 2 > & 1 | grep \" R\\/\" R/3.4.2 R/latest To use one of these version, you have to load the respective module, which then masks the system\u2019s version, i.e. module load vital-it module load R/3.4.2 Do not forget to put those two lines into your job script as well in order to use the same version from within the job later on a compute node! Basic Topics Customizing the Workspace At startup, unless \u2013no-init-file, or \u2013vanilla was given, R searches for a user profile in the current directory (from where R was started), or in the user\u2019s home directory (in that order). A different path of the user profile file can be specified by the R_PROFILE_USER environment variable. The found user profile is then sourced into the workspace. You can use this file to customize your workspace, i.e., to set specific options, define functions, load libraries, and so on. Consider the following example: .Rprofile # Set some options options ( stringsAsFactors = FALSE ) options ( max.print = 100 ) options ( scipen = 10 ) # Load class library library ( class ) # Don't save workspace by default q <- function ( save = \"no\" , ... ) { quit ( save = save, ... ) } # User-defined function for setting standard seed mySeed <- function () set.seed ( 5450 ) # User-defined function for calculating L1/L2-norm, returns euclidian distance (L2-norm) by default myDistance <- function ( x, y, type = c ( \"Euclidian\" , \"L2\" , \"Manhattan\" , \"L1\" )) { type <- match.arg ( type ) if (( type == \"Manhattan\" ) | ( type == \"L1\" )) { d <- sum ( abs ( x - y ) ) } else { d <- sqrt ( sum ( ( x - y ) ^ 2 ) ) } return ( d ) } Installing Packages Run R interactively. To install additional R packages call the install.packages() function with the name of the package as argument. Upon installing the first package, you will receive a warning that you do not have sufficient permissions to write to \u201c/usr/lib64/R/library\u201d. Type \u201cy\u201d to use a personal library instead: > install.packages ( \"doParallel\" ) Installing package into \u2018/usr/lib64/R/library\u2019 ( as \u2018lib\u2019 is unspecified ) Warnung in install.packages ( \"doParallel\" ) ' lib = \"/usr/lib64/R/library\" ist nicht schreibbar Would you like to use a personal library instead? ( y/n ) Next, type \u201cy\u201d to create your personal library at the default location within your HOME directory: Would you like to create a personal library ~/R/x86_64-redhat-linux-gnu-library/3.4 Next, select a CRAN mirror to download from. The mirrorlist will be not the same as below. The mirrolist is constantly changing, but will look like it. Pick any country nearby, i.e. Switzerland. If https makes problems, pick \u201c(HTTP mirrors)\u201d and then select something nearby as shown below --- Bitte einen CRAN Spiegel f\u00fcr diese Sitzung ausw\u00e4hlen --- Error in download.file ( url, destfile = f, quiet = TRUE ) : nicht unterst\u00fctztes URL Schema HTTPS CRAN mirror 1 : 0 -Cloud [ https ] 2 : Austria [ https ] 3 : Chile [ https ] 4 : China ( Beijing 4 ) [ https ] 5 : Colombia ( Cali ) [ https ] 6 : France ( Lyon 2 ) [ https ] 7 : France ( Paris 2 ) [ https ] 8 : Germany ( M\u00fcnster ) [ https ] 9 : Iceland [ https ] 10 : Mexico ( Mexico City ) [ https ] 11 : Russia ( Moscow ) [ https ] 12 : Spain ( A Coru\u00f1a ) [ https ] 13 : Switzerland [ https ] 14 : UK ( Bristol ) [ https ] 15 : UK ( Cambridge ) [ https ] 16 : USA ( CA 1 ) [ https ] 17 : USA ( KS ) [ https ] 18 : USA ( MI 1 ) [ https ] 19 : USA ( TN ) [ https ] 20 : USA ( TX ) [ https ] 21 : USA ( WA ) [ https ] 22 : ( HTTP mirrors ) Selection: 22 HTTP CRAN mirror 1 : 0 -Cloud 2 : Algeria 3 : Argentina ( La Plata ) 4 : Australia ( Canberra ) 5 : Australia ( Melbourne ) 6 : Austria 7 : Belgium ( Antwerp ) 8 : Belgium ( Ghent ) ( ... ) 65 : Slovakia 66 : South Africa ( Cape Town ) 67 : South Africa ( Johannesburg ) 68 : Spain ( A Coru\u00f1a ) 69 : Spain ( Madrid ) 70 : Sweden 71 : Switzerland 72 : Taiwan ( Chungli ) 73 : Taiwan ( Taipei ) 74 : Thailand 75 : Turkey ( Denizli ) 76 : Turkey ( Mersin ) ( ... ) 93 : USA ( OH 2 ) 94 : USA ( OR ) 95 : USA ( PA 2 ) 96 : USA ( TN ) 97 : USA ( TX ) 98 : USA ( WA ) 99 : Venezuela Selection: 71 Finally, the package gets installed. After installing the package you can close the interactive session by typing q(). Do not forget to load the corresponding library (for each R session) before using functions provided by the package: > library ( doParallel ) Batch Execution of R The syntax for running R non-interactively with input read from infile and output send to outfile is: R CMD BATCH [ options ] infile [ outfile ] Suppose you placed your R code in a file called foo.R: foo.R set.seed ( 3000 ) valx<-seq ( -2,2,0.01 ) valy<-2*valx+rnorm ( length ( valx ) ,0,4 ) # Save plot to pdf pdf ( 'histplot.pdf' ) hist ( valy,prob = TRUE,breaks = 20 , main = \"Histogram and PDF\" ,xlab = \"y\" , ylim = c ( 0 ,0.15 )) curve ( dnorm ( x,mean ( valy ) ,sd ( valy )) ,add = T,col = \"red\" ) dev.off () To execute foo.R on the cluster, add the R call to your job script\u2026 Rbatch.sh #! /bin/bash #SBATCH --mail-user=<put your valid email address here!> #SBATCH --mail-type=end,fail #SBATCH --time=01:00:00 #SBATCH --mem-per-cpu=2G # Put your code below this line module load vital-it module load R/3.4.2 R CMD BATCH --no-save --no-restore foo.R \u2026and submit your job script to the cluster: sbatch Rbatch.sh Advanced Topics Parallel R By default, R will not make use of multiple cores available on compute nodes to parallelize computations. Parallel processing functionality is provided by add-on packages. Consider the following contrived example to get you started. To follow the example, you need the following packages installed, and the corresponding libraries loaded: > library ( doParallel ) > library ( foreach ) The foreach package provides a looping construct for executing R statements repeatedly, either sequentially (similar to a for loop) or in parallel. While the binary operator %do% is used for executing the statements sequentially, the %dopar% operator is used to execute code in parallel using the currently registered backend. The getDoParWorkers() function returns the number of execution workers (cores) available in the currently registered doPar backend, by default this corresponds to one worker: > getDoParWorkers () [ 1 ] 1 Hence, the following R code will execute on a single core (even with the %dopar% operator): > start.time <- Sys.time () > foreach ( i = 4 :1, .combine = 'c' , .inorder = FALSE ) %dopar% { + Sys.sleep ( 3 *i ) + i + } end.time <- Sys.time () exec.time <- end.time - start.time [ 1 ] 4 3 2 1 Let\u2019s measure the runtime of the sequentiall execution: > start.time <- Sys.time () ; foreach ( i = 4 :1, .combine = 'c' , .inorder = TRUE ) %dopar% { Sys.sleep ( 3 *i ) ; i } ; end.time <- Sys.time () ; exec.time <- end.time - start.time ; exec.time [ 1 ] 4 3 2 1 Time difference of 30 .04088 secs Now, we will register a parallel backend to allow the %dopar% operator to execute in parallel. The doParallel package provides a parallel backend for the %dopar% operator. Let\u2019s find out the number of cores available on the current node > detectCores () [ 1 ] 24 To register the doPar backend call the function registerDoParallel(). With no arguments provided, the number of cores assigned to the backend matches the value of options(\u201ccores\u201d) , or if not set, to half of the cores detected by the parallel package. registerDoParallel () > getDoParWorkers () [ 1 ] 12 To assign 4 cores to the parallel backend: > registerDoParallel ( cores = 4 ) > getDoParWorkers () [ 1 ] 4 Request the correct number of slots Because it is crucial to request the correct number of slots for a parallel job, we propose to set the number of cores for the doPar backend to the number of slots allocated to your job: registerDoParallel(cores=Sys.getenv(\"SLURM_CPUS_PER_TASK\")) Now, run the example again: > foreach ( i = 4 :1, .combine = 'c' , .inorder = FALSE ) %dopar% { + Sys.sleep ( 3 *i ) + i + } [ 1 ] 4 3 2 1 Well, the output is basically the same (the results are combined in the same order!). Let\u2019s again measure the runtime of the parallel execution on 4 cores: The binary operator %do% willl always execute a foreach-loop sequentially even if registerDoParallel was called before! To correctly run a foreach in parallel, two conditions must be met: registerDoParallel() must be called with a certain number of cores The %dopar% operator must be used in the foreach-loop to have it run in parallel! Installing DESeq2 from Bioconductor packages DESeq2 1 installed from Bioconductor 2 has many dependencies. Two odd facts are hindering a succesful build of DESeq2 in first place: data.table is needed by Hmisc, which in turn is needed by DESeq2. While Hmisc is automatically installed prior to DESeq2, data.table is not and has to be installed manually first. https://bioconductor.org/packages/release/bioc/html/DESeq2.html \u21a9 https://bioconductor.org/ \u21a9","title":"R"},{"location":"user-guide/software/r.html#r","text":"","title":"R"},{"location":"user-guide/software/r.html#description","text":"UBELIX no longer features the R version from EPEL as this version gets automatically updated and therefore things are not reproducible. R isn now provided by an environment module and must be loaded explicitly: module load R/3.4.4-foss-2018a-X11-20180131 -bash-4.1$ R --version R version 3 .4.4 ( 2018 -03-15 ) -- \"Someone to Lean On\" Copyright ( C ) 2018 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu ( 64 -bit ) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3 . For more information about these matters see http://www.gnu.org/licenses/. The Vital-IT project is also providing some versions. The following commands will list the available versions: module load vital-it module avail 2 > & 1 | grep \" R\\/\" R/3.4.2 R/latest To use one of these version, you have to load the respective module, which then masks the system\u2019s version, i.e. module load vital-it module load R/3.4.2 Do not forget to put those two lines into your job script as well in order to use the same version from within the job later on a compute node!","title":"Description"},{"location":"user-guide/software/r.html#basic-topics","text":"","title":"Basic Topics"},{"location":"user-guide/software/r.html#customizing-the-workspace","text":"At startup, unless \u2013no-init-file, or \u2013vanilla was given, R searches for a user profile in the current directory (from where R was started), or in the user\u2019s home directory (in that order). A different path of the user profile file can be specified by the R_PROFILE_USER environment variable. The found user profile is then sourced into the workspace. You can use this file to customize your workspace, i.e., to set specific options, define functions, load libraries, and so on. Consider the following example: .Rprofile # Set some options options ( stringsAsFactors = FALSE ) options ( max.print = 100 ) options ( scipen = 10 ) # Load class library library ( class ) # Don't save workspace by default q <- function ( save = \"no\" , ... ) { quit ( save = save, ... ) } # User-defined function for setting standard seed mySeed <- function () set.seed ( 5450 ) # User-defined function for calculating L1/L2-norm, returns euclidian distance (L2-norm) by default myDistance <- function ( x, y, type = c ( \"Euclidian\" , \"L2\" , \"Manhattan\" , \"L1\" )) { type <- match.arg ( type ) if (( type == \"Manhattan\" ) | ( type == \"L1\" )) { d <- sum ( abs ( x - y ) ) } else { d <- sqrt ( sum ( ( x - y ) ^ 2 ) ) } return ( d ) }","title":"Customizing the Workspace"},{"location":"user-guide/software/r.html#installing-packages","text":"Run R interactively. To install additional R packages call the install.packages() function with the name of the package as argument. Upon installing the first package, you will receive a warning that you do not have sufficient permissions to write to \u201c/usr/lib64/R/library\u201d. Type \u201cy\u201d to use a personal library instead: > install.packages ( \"doParallel\" ) Installing package into \u2018/usr/lib64/R/library\u2019 ( as \u2018lib\u2019 is unspecified ) Warnung in install.packages ( \"doParallel\" ) ' lib = \"/usr/lib64/R/library\" ist nicht schreibbar Would you like to use a personal library instead? ( y/n ) Next, type \u201cy\u201d to create your personal library at the default location within your HOME directory: Would you like to create a personal library ~/R/x86_64-redhat-linux-gnu-library/3.4 Next, select a CRAN mirror to download from. The mirrorlist will be not the same as below. The mirrolist is constantly changing, but will look like it. Pick any country nearby, i.e. Switzerland. If https makes problems, pick \u201c(HTTP mirrors)\u201d and then select something nearby as shown below --- Bitte einen CRAN Spiegel f\u00fcr diese Sitzung ausw\u00e4hlen --- Error in download.file ( url, destfile = f, quiet = TRUE ) : nicht unterst\u00fctztes URL Schema HTTPS CRAN mirror 1 : 0 -Cloud [ https ] 2 : Austria [ https ] 3 : Chile [ https ] 4 : China ( Beijing 4 ) [ https ] 5 : Colombia ( Cali ) [ https ] 6 : France ( Lyon 2 ) [ https ] 7 : France ( Paris 2 ) [ https ] 8 : Germany ( M\u00fcnster ) [ https ] 9 : Iceland [ https ] 10 : Mexico ( Mexico City ) [ https ] 11 : Russia ( Moscow ) [ https ] 12 : Spain ( A Coru\u00f1a ) [ https ] 13 : Switzerland [ https ] 14 : UK ( Bristol ) [ https ] 15 : UK ( Cambridge ) [ https ] 16 : USA ( CA 1 ) [ https ] 17 : USA ( KS ) [ https ] 18 : USA ( MI 1 ) [ https ] 19 : USA ( TN ) [ https ] 20 : USA ( TX ) [ https ] 21 : USA ( WA ) [ https ] 22 : ( HTTP mirrors ) Selection: 22 HTTP CRAN mirror 1 : 0 -Cloud 2 : Algeria 3 : Argentina ( La Plata ) 4 : Australia ( Canberra ) 5 : Australia ( Melbourne ) 6 : Austria 7 : Belgium ( Antwerp ) 8 : Belgium ( Ghent ) ( ... ) 65 : Slovakia 66 : South Africa ( Cape Town ) 67 : South Africa ( Johannesburg ) 68 : Spain ( A Coru\u00f1a ) 69 : Spain ( Madrid ) 70 : Sweden 71 : Switzerland 72 : Taiwan ( Chungli ) 73 : Taiwan ( Taipei ) 74 : Thailand 75 : Turkey ( Denizli ) 76 : Turkey ( Mersin ) ( ... ) 93 : USA ( OH 2 ) 94 : USA ( OR ) 95 : USA ( PA 2 ) 96 : USA ( TN ) 97 : USA ( TX ) 98 : USA ( WA ) 99 : Venezuela Selection: 71 Finally, the package gets installed. After installing the package you can close the interactive session by typing q(). Do not forget to load the corresponding library (for each R session) before using functions provided by the package: > library ( doParallel )","title":"Installing Packages"},{"location":"user-guide/software/r.html#batch-execution-of-r","text":"The syntax for running R non-interactively with input read from infile and output send to outfile is: R CMD BATCH [ options ] infile [ outfile ] Suppose you placed your R code in a file called foo.R: foo.R set.seed ( 3000 ) valx<-seq ( -2,2,0.01 ) valy<-2*valx+rnorm ( length ( valx ) ,0,4 ) # Save plot to pdf pdf ( 'histplot.pdf' ) hist ( valy,prob = TRUE,breaks = 20 , main = \"Histogram and PDF\" ,xlab = \"y\" , ylim = c ( 0 ,0.15 )) curve ( dnorm ( x,mean ( valy ) ,sd ( valy )) ,add = T,col = \"red\" ) dev.off () To execute foo.R on the cluster, add the R call to your job script\u2026 Rbatch.sh #! /bin/bash #SBATCH --mail-user=<put your valid email address here!> #SBATCH --mail-type=end,fail #SBATCH --time=01:00:00 #SBATCH --mem-per-cpu=2G # Put your code below this line module load vital-it module load R/3.4.2 R CMD BATCH --no-save --no-restore foo.R \u2026and submit your job script to the cluster: sbatch Rbatch.sh","title":"Batch Execution of R"},{"location":"user-guide/software/r.html#advanced-topics","text":"","title":"Advanced Topics"},{"location":"user-guide/software/r.html#parallel-r","text":"By default, R will not make use of multiple cores available on compute nodes to parallelize computations. Parallel processing functionality is provided by add-on packages. Consider the following contrived example to get you started. To follow the example, you need the following packages installed, and the corresponding libraries loaded: > library ( doParallel ) > library ( foreach ) The foreach package provides a looping construct for executing R statements repeatedly, either sequentially (similar to a for loop) or in parallel. While the binary operator %do% is used for executing the statements sequentially, the %dopar% operator is used to execute code in parallel using the currently registered backend. The getDoParWorkers() function returns the number of execution workers (cores) available in the currently registered doPar backend, by default this corresponds to one worker: > getDoParWorkers () [ 1 ] 1 Hence, the following R code will execute on a single core (even with the %dopar% operator): > start.time <- Sys.time () > foreach ( i = 4 :1, .combine = 'c' , .inorder = FALSE ) %dopar% { + Sys.sleep ( 3 *i ) + i + } end.time <- Sys.time () exec.time <- end.time - start.time [ 1 ] 4 3 2 1 Let\u2019s measure the runtime of the sequentiall execution: > start.time <- Sys.time () ; foreach ( i = 4 :1, .combine = 'c' , .inorder = TRUE ) %dopar% { Sys.sleep ( 3 *i ) ; i } ; end.time <- Sys.time () ; exec.time <- end.time - start.time ; exec.time [ 1 ] 4 3 2 1 Time difference of 30 .04088 secs Now, we will register a parallel backend to allow the %dopar% operator to execute in parallel. The doParallel package provides a parallel backend for the %dopar% operator. Let\u2019s find out the number of cores available on the current node > detectCores () [ 1 ] 24 To register the doPar backend call the function registerDoParallel(). With no arguments provided, the number of cores assigned to the backend matches the value of options(\u201ccores\u201d) , or if not set, to half of the cores detected by the parallel package. registerDoParallel () > getDoParWorkers () [ 1 ] 12 To assign 4 cores to the parallel backend: > registerDoParallel ( cores = 4 ) > getDoParWorkers () [ 1 ] 4 Request the correct number of slots Because it is crucial to request the correct number of slots for a parallel job, we propose to set the number of cores for the doPar backend to the number of slots allocated to your job: registerDoParallel(cores=Sys.getenv(\"SLURM_CPUS_PER_TASK\")) Now, run the example again: > foreach ( i = 4 :1, .combine = 'c' , .inorder = FALSE ) %dopar% { + Sys.sleep ( 3 *i ) + i + } [ 1 ] 4 3 2 1 Well, the output is basically the same (the results are combined in the same order!). Let\u2019s again measure the runtime of the parallel execution on 4 cores: The binary operator %do% willl always execute a foreach-loop sequentially even if registerDoParallel was called before! To correctly run a foreach in parallel, two conditions must be met: registerDoParallel() must be called with a certain number of cores The %dopar% operator must be used in the foreach-loop to have it run in parallel!","title":"Parallel R"},{"location":"user-guide/software/r.html#installing-deseq2-from-bioconductor-packages","text":"DESeq2 1 installed from Bioconductor 2 has many dependencies. Two odd facts are hindering a succesful build of DESeq2 in first place: data.table is needed by Hmisc, which in turn is needed by DESeq2. While Hmisc is automatically installed prior to DESeq2, data.table is not and has to be installed manually first. https://bioconductor.org/packages/release/bioc/html/DESeq2.html \u21a9 https://bioconductor.org/ \u21a9","title":"Installing DESeq2 from Bioconductor packages"},{"location":"user-guide/software/relion.html","text":"Relion Description Some useful information on using Relion. Running Relion A standard submission script serves as a template for your Relion jobs. Create a file with the following content within your home directory: qsub.sh #!/bin/bash #SBATCH --mail-user=<put your valid email address> #SBATCH --mail-type=end,fail #SBATCH --ntasks=XXXmpinodesXXX #SBATCH --time=XXXextra1XXX #SBATCH --mem-per-cpu=XXXextra2XXX #SBATCH --partition=XXXqueueXXX #SBATCH --error=XXXerrfileXXX #SBATCH --output=XXXoutfileXXX module load relion/1.4 mpiexec XXXcommandXXX ####the end Substitute your own email address! Keywords starting and finishing with \u201cXXX\u201d are recognized by Relion and should not be edited. To select a specific processor family you can edit the -pe option and subsitute \u201corte-sandy\u201d, \u201corte-ivy\u201d or \u201corte-broadwell\u201d for \u201corte\u201d. Now, you can set up tasks that will run on the cluster as follows: Start the Relion GUI Click on the \u201cRunning\u201d tab Add appropriate values for each option: Number of MPI procs: The number of processes the job should use. Number of threads: Currently only 1 thread is supported on Ubelix. Available RAM per thread: Relion jobs can be quite eager but it is impossible to precisely predict how much RAM each process will need. 4 is usually a good place to start. This option here is only indicative and puts no limit on the RAM that Relion can use. Nonetheless to prevent stupid mistakes, you should always enter the same amount of RAM here as in the option \u201cMaximum RAM per process\u201d (see below). Submit to queue: Must be set to yes if the aim is to run on Ubelix queuing system. Queue name: In general set it to \u00ab all \u00bb. If you want to use a specific queue, please refer to https://docs.id.unibe.ch/ubelix/advanced-topics/parallel-jobs Queue submit command: Set it to \u201csbatch\u201d. Maximum CPU time: The maximum allowed running time. See https://docs.id.unibe.ch/ubelix/ubelix-101/the-job-script (Mandatory options) for details on the meaning of this option for Ubelix usage. Maximum RAM process: The maximum allowed RAM per process allowed by Ubelix. If you ask for too much RAM your job is less likely to start fast. If you ask for too little RAM your job will crash. The error output by Relion and Ubelix in such case is not always explicit. Nevertheless, too little RAM is the most common cause of crash. Therefore if you experience an unexpected crash, try increasing the available RAM per thread. See https://docs.id.unibe.ch/ubelix/ubelix-101/the-job-script (Mandatory options) for details on the meaning of this option for Ubelix usage. Note that unlike in \u201cAvailable RAM per thread\u201d option, you must append a \u201cG\u201d to the desired number of Gigabytes (for example, 4G). To prevent stupid mistake, you should always enter the same amount of RAM here as in the option \u201cAvailable RAM per thread\u201d. Standard submission script: Path to the standard submission script described above. Minimum dedicated core per node: Set to 1. Further Information Relion wiki: http://www2.mrc-lmb.cam.ac.uk/relion/index.php/Main_Page Tutorial: http://www2.mrc-lmb.cam.ac.uk/groups/scheres/relion13_tutorial.pdf","title":"Relion"},{"location":"user-guide/software/relion.html#relion","text":"","title":"Relion"},{"location":"user-guide/software/relion.html#description","text":"Some useful information on using Relion.","title":"Description"},{"location":"user-guide/software/relion.html#running-relion","text":"A standard submission script serves as a template for your Relion jobs. Create a file with the following content within your home directory: qsub.sh #!/bin/bash #SBATCH --mail-user=<put your valid email address> #SBATCH --mail-type=end,fail #SBATCH --ntasks=XXXmpinodesXXX #SBATCH --time=XXXextra1XXX #SBATCH --mem-per-cpu=XXXextra2XXX #SBATCH --partition=XXXqueueXXX #SBATCH --error=XXXerrfileXXX #SBATCH --output=XXXoutfileXXX module load relion/1.4 mpiexec XXXcommandXXX ####the end Substitute your own email address! Keywords starting and finishing with \u201cXXX\u201d are recognized by Relion and should not be edited. To select a specific processor family you can edit the -pe option and subsitute \u201corte-sandy\u201d, \u201corte-ivy\u201d or \u201corte-broadwell\u201d for \u201corte\u201d. Now, you can set up tasks that will run on the cluster as follows: Start the Relion GUI Click on the \u201cRunning\u201d tab Add appropriate values for each option: Number of MPI procs: The number of processes the job should use. Number of threads: Currently only 1 thread is supported on Ubelix. Available RAM per thread: Relion jobs can be quite eager but it is impossible to precisely predict how much RAM each process will need. 4 is usually a good place to start. This option here is only indicative and puts no limit on the RAM that Relion can use. Nonetheless to prevent stupid mistakes, you should always enter the same amount of RAM here as in the option \u201cMaximum RAM per process\u201d (see below). Submit to queue: Must be set to yes if the aim is to run on Ubelix queuing system. Queue name: In general set it to \u00ab all \u00bb. If you want to use a specific queue, please refer to https://docs.id.unibe.ch/ubelix/advanced-topics/parallel-jobs Queue submit command: Set it to \u201csbatch\u201d. Maximum CPU time: The maximum allowed running time. See https://docs.id.unibe.ch/ubelix/ubelix-101/the-job-script (Mandatory options) for details on the meaning of this option for Ubelix usage. Maximum RAM process: The maximum allowed RAM per process allowed by Ubelix. If you ask for too much RAM your job is less likely to start fast. If you ask for too little RAM your job will crash. The error output by Relion and Ubelix in such case is not always explicit. Nevertheless, too little RAM is the most common cause of crash. Therefore if you experience an unexpected crash, try increasing the available RAM per thread. See https://docs.id.unibe.ch/ubelix/ubelix-101/the-job-script (Mandatory options) for details on the meaning of this option for Ubelix usage. Note that unlike in \u201cAvailable RAM per thread\u201d option, you must append a \u201cG\u201d to the desired number of Gigabytes (for example, 4G). To prevent stupid mistake, you should always enter the same amount of RAM here as in the option \u201cAvailable RAM per thread\u201d. Standard submission script: Path to the standard submission script described above. Minimum dedicated core per node: Set to 1.","title":"Running Relion"},{"location":"user-guide/software/relion.html#further-information","text":"Relion wiki: http://www2.mrc-lmb.cam.ac.uk/relion/index.php/Main_Page Tutorial: http://www2.mrc-lmb.cam.ac.uk/groups/scheres/relion13_tutorial.pdf","title":"Further Information"},{"location":"user-guide/software/singularity.html","text":"Singularity Description Put your scientific workflows, software and libraries in a Singularity container and run it on UBELIX Examples Work interactively Submit an interactive SLURM job and then use the shell command to spawn an interactive shell within the Singularity container: srun --time = 01 :00:00 --mem-per-cpu = 2G --pty bash singularity shell <image> Execute the containers \u201crunscript\u201d #!/bin/bash #SBATCH --partition=all #SBATCH --mem-per-cpu=2G singularity run <image> #or ./<image> Run a command within your container image singularity exec <image> <command> e.g: singularity exec container.img cat /etc/os-release Further Information Official Singularity Documentation can be found at https://sylabs.io/docs/","title":"Singularity"},{"location":"user-guide/software/singularity.html#singularity","text":"","title":"Singularity"},{"location":"user-guide/software/singularity.html#description","text":"Put your scientific workflows, software and libraries in a Singularity container and run it on UBELIX","title":"Description"},{"location":"user-guide/software/singularity.html#examples","text":"","title":"Examples"},{"location":"user-guide/software/singularity.html#work-interactively","text":"Submit an interactive SLURM job and then use the shell command to spawn an interactive shell within the Singularity container: srun --time = 01 :00:00 --mem-per-cpu = 2G --pty bash singularity shell <image>","title":"Work interactively"},{"location":"user-guide/software/singularity.html#execute-the-containers-runscript","text":"#!/bin/bash #SBATCH --partition=all #SBATCH --mem-per-cpu=2G singularity run <image> #or ./<image>","title":"Execute the containers \"runscript\""},{"location":"user-guide/software/singularity.html#run-a-command-within-your-container-image","text":"singularity exec <image> <command> e.g: singularity exec container.img cat /etc/os-release","title":"Run a command within your container image"},{"location":"user-guide/software/singularity.html#further-information","text":"Official Singularity Documentation can be found at https://sylabs.io/docs/","title":"Further Information"},{"location":"user-guide/software/terminal-multiplexer-tmux.html","text":"Terminal Multiplexer (tmux) Description Frequently, people want to run programs on the submit host independently from an SSH session. Besides allowing a user to access multiple terminal sessions inside a single terminal window, tmux also lets you separate a program from the Unix shell that started the program. Tmux allows you detach from your running tmux session (the session will keep running in the background) and attach to the same session later on. Because the tmux session is running on the remote server, your session persists even on logout. Working Example Start a new tmux session on the submit host: $ tmux new -s first_session This will automatically attach you to a tmux session named first_session. Do your work within your tmux session. Detach from the session: Ctrl-b d Now you cloud disconnect from the server and reconnect later on. List all your existing tmux session: $ tmux ls first_session: 1 windows ( created Wed Jan 14 15 :23:11 2016 ) [ 80x85 ] ``` Bash Reattach to an existing tmux session: ``` Bash $ tumb attach -t first_session Further Information A tmux primer: https://danielmiessler.com/study/tmux","title":"Terminal Multiplexer (tmux)"},{"location":"user-guide/software/terminal-multiplexer-tmux.html#terminal-multiplexer-tmux","text":"","title":"Terminal Multiplexer (tmux)"},{"location":"user-guide/software/terminal-multiplexer-tmux.html#description","text":"Frequently, people want to run programs on the submit host independently from an SSH session. Besides allowing a user to access multiple terminal sessions inside a single terminal window, tmux also lets you separate a program from the Unix shell that started the program. Tmux allows you detach from your running tmux session (the session will keep running in the background) and attach to the same session later on. Because the tmux session is running on the remote server, your session persists even on logout.","title":"Description"},{"location":"user-guide/software/terminal-multiplexer-tmux.html#working-example","text":"Start a new tmux session on the submit host: $ tmux new -s first_session This will automatically attach you to a tmux session named first_session. Do your work within your tmux session. Detach from the session: Ctrl-b d Now you cloud disconnect from the server and reconnect later on. List all your existing tmux session: $ tmux ls first_session: 1 windows ( created Wed Jan 14 15 :23:11 2016 ) [ 80x85 ] ``` Bash Reattach to an existing tmux session: ``` Bash $ tumb attach -t first_session","title":"Working Example"},{"location":"user-guide/software/terminal-multiplexer-tmux.html#further-information","text":"A tmux primer: https://danielmiessler.com/study/tmux","title":"Further Information"}]}