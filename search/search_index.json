{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"COVID-19 Pandemic: impact on UBELIX operations The University of Bern switched to emergency operations (minimal presence). The HPC team put in all effort to slow the spread of COVID-19 by working remotly. The UBELIX system stays in production and the HPC team do their best to continue the services, even if problems may take longer to solve. For any questions or issue please write to: hpc@id.unibe.ch The UniBE HPC team would like to contribute to the research in the fight against COVID-19. Any related studies may get special UBELIX priorities as well as special software support on request ( hpc@id.unibe.ch ). Welcome to the High Performance Computing (HPC) documentation of the University of Bern Introduction Official documentation site of the high performance computing and the HPC cluster UBELIX. Currently, the UBELIX cluster runs about 330 compute nodes featuring almost 6\u2018300 CPU cores and about 300\u2018000 GPU cores. The infrastructure is available to all University personnel for their scientific work. The cluster can also be used by students within a scope of a thesis or a course. If your campus account is not yet activated for UBELIX, the Getting Started Guide might be a good place to get you started. UBELIX features a plethora of software and applications, which is outlined on the page Software, but the users are free to compile and install their own software within their home directories. If you are wondering\u2026 UBELIX is an acronym and stands for U niversity of Be rn Li nu x Cluster (Naming similarities to known Gauls are purely coincidental and not intended in any way). Job Monitoroing See what is currently running on UBELIX on the Job Monitoring pages. Acknowledging UBELIX When you present results generated using our cluster UBELIX, we kindly ask you to acknowledge the usage of the cluster. We would also highly appreciate if you could send us a copy of your papers, posters and presentations mentioning the UBELIX cluster. Public visibility of our cluster and documenting results are important for us to ensure long-term funding of UBELIX. Whenever the UBELIX infrastructure has been used to produce results used in a publication or poster, we kindly request citing the service in the acknowledgements: \"Calculations were performed on UBELIX (http://www.id.unibe.ch/hpc), the HPC cluster at the University of Bern.\" Press Kit Occasionally we are asked for images like diagrams illustrating the structure of UBELIX or even pictures of machines/storage or the like. Often this is due to the need to describe UBELIX within a research proposal. To support you with this, we provide a short text about UBELIX and some files to download. You can use all of this within your research proposal. COMING SOON","title":"Index"},{"location":"index.html#welcome-to-the-high-performance-computing-hpc-documentation-of-the-university-of-bern","text":"","title":"Welcome to the High Performance Computing (HPC) documentation of the University of Bern"},{"location":"index.html#introduction","text":"Official documentation site of the high performance computing and the HPC cluster UBELIX. Currently, the UBELIX cluster runs about 330 compute nodes featuring almost 6\u2018300 CPU cores and about 300\u2018000 GPU cores. The infrastructure is available to all University personnel for their scientific work. The cluster can also be used by students within a scope of a thesis or a course. If your campus account is not yet activated for UBELIX, the Getting Started Guide might be a good place to get you started. UBELIX features a plethora of software and applications, which is outlined on the page Software, but the users are free to compile and install their own software within their home directories. If you are wondering\u2026 UBELIX is an acronym and stands for U niversity of Be rn Li nu x Cluster (Naming similarities to known Gauls are purely coincidental and not intended in any way). Job Monitoroing See what is currently running on UBELIX on the Job Monitoring pages.","title":"Introduction"},{"location":"index.html#acknowledging-ubelix","text":"When you present results generated using our cluster UBELIX, we kindly ask you to acknowledge the usage of the cluster. We would also highly appreciate if you could send us a copy of your papers, posters and presentations mentioning the UBELIX cluster. Public visibility of our cluster and documenting results are important for us to ensure long-term funding of UBELIX. Whenever the UBELIX infrastructure has been used to produce results used in a publication or poster, we kindly request citing the service in the acknowledgements: \"Calculations were performed on UBELIX (http://www.id.unibe.ch/hpc), the HPC cluster at the University of Bern.\"","title":"Acknowledging UBELIX"},{"location":"index.html#press-kit","text":"Occasionally we are asked for images like diagrams illustrating the structure of UBELIX or even pictures of machines/storage or the like. Often this is due to the need to describe UBELIX within a research proposal. To support you with this, we provide a short text about UBELIX and some files to download. You can use all of this within your research proposal. COMING SOON","title":"Press Kit"},{"location":"HOME_migration.html","text":"File System Restructure We are restructuring our file systems to update our hardware and improve collaboration between users. There will be following stages: HOME migration to new file system introduction of Workspaces Permission changes and quota reduction of HOME directories DECLUTTERING Migration performance depends on the file space , but also on the amount of files . Therefore, please delete unnecessary data and pack files when possible: Remove obsolete data: rm -r unnec_data Pack data not needed in a short to midterm into tar archives: tar -zcf archive.tar.gz foo bar # where foo and bar are files. remove obsolete custom installed packages, e.g. old Python packages: rm -r $HOME/.local/lib/python*/site-packages/* HOME migration new users HOMEs of recently added and new users are already in the new location. You can verify using pwd , result starts with /storage/homefs . HOME directories will be migrated to /storage/homefs/$USER in our newer and larger SpectrumScale file system. In future, these HOME directories are meant to be private only, without any sharing opportunity. As you may noticed, there will be no institute directory in the structure anymore. See subsection Temporary File Sharing . The migration will be performed institute by institute. Users are notified before and after actual migration. When migration is finished $HOME will point to /storage/homefs/$USER . IMPORTANT: please also read section Quota and permission changes below. Introducing HPC Workspaces HPC Workspaces are group shared places. See HPC Workspace Documentation Temporary space to share data HOMEs are meant to be private spaces. Data sharing is enabled using flexible group based HPC Workspaces . The institute based directories /home/ubelix/<instituteID>/shared were provided using tools similar to Workspaces: module load CustomRepo/ws_inst cd $WORKSPACE With Workspace and migrated data your workflow can still use the environment variable WORKSPACE , which point to your Workspace directory. Keep in mind the subdirectory structure in the previous shared directories. shared directory cleaning In June, the institute shared directories will be removed. You will have more than a month to migrate your data to a Workspace. Quota and permission changes Finally, beginning of June HOME directories will be restricted to be a private space and the quota will be set to max. 1TB and 1M files. Furthermore, the temporary institute sharing directories will be removed. Starting with this change the HOME Snapshot and Backup will be activated and the umask will be by default 002.","title":"IMPORTANT..Home Migration"},{"location":"HOME_migration.html#file-system-restructure","text":"We are restructuring our file systems to update our hardware and improve collaboration between users. There will be following stages: HOME migration to new file system introduction of Workspaces Permission changes and quota reduction of HOME directories DECLUTTERING Migration performance depends on the file space , but also on the amount of files . Therefore, please delete unnecessary data and pack files when possible: Remove obsolete data: rm -r unnec_data Pack data not needed in a short to midterm into tar archives: tar -zcf archive.tar.gz foo bar # where foo and bar are files. remove obsolete custom installed packages, e.g. old Python packages: rm -r $HOME/.local/lib/python*/site-packages/*","title":"File System Restructure"},{"location":"HOME_migration.html#home-migration","text":"new users HOMEs of recently added and new users are already in the new location. You can verify using pwd , result starts with /storage/homefs . HOME directories will be migrated to /storage/homefs/$USER in our newer and larger SpectrumScale file system. In future, these HOME directories are meant to be private only, without any sharing opportunity. As you may noticed, there will be no institute directory in the structure anymore. See subsection Temporary File Sharing . The migration will be performed institute by institute. Users are notified before and after actual migration. When migration is finished $HOME will point to /storage/homefs/$USER . IMPORTANT: please also read section Quota and permission changes below.","title":"HOME migration"},{"location":"HOME_migration.html#introducing-hpc-workspaces","text":"HPC Workspaces are group shared places. See HPC Workspace Documentation","title":"Introducing HPC Workspaces"},{"location":"HOME_migration.html#temporary-space-to-share-data","text":"HOMEs are meant to be private spaces. Data sharing is enabled using flexible group based HPC Workspaces . The institute based directories /home/ubelix/<instituteID>/shared were provided using tools similar to Workspaces: module load CustomRepo/ws_inst cd $WORKSPACE With Workspace and migrated data your workflow can still use the environment variable WORKSPACE , which point to your Workspace directory. Keep in mind the subdirectory structure in the previous shared directories. shared directory cleaning In June, the institute shared directories will be removed. You will have more than a month to migrate your data to a Workspace.","title":"Temporary space to share data"},{"location":"HOME_migration.html#quota-and-permission-changes","text":"Finally, beginning of June HOME directories will be restricted to be a private space and the quota will be set to max. 1TB and 1M files. Furthermore, the temporary institute sharing directories will be removed. Starting with this change the HOME Snapshot and Backup will be activated and the umask will be by default 002.","title":"Quota and permission changes"},{"location":"code-of-conduct.html","text":"Code of Conduct On this page we list some expectations from our side and recommended practice that is crucial for maintaining a good and professional working relationship between the user and the system administrators. Most of those contents are quite self-explanatory while others help to reduce the amount of support time needed to allocate. General We assume that you are familiar with some basic knowledge about Linux command line (shell) navigation and shell scripting. If you never worked on the command line, consider some Linux tutorials on the subject first. We expect you to exploit this valuable documentation before asking for help. All that is needed to get some simple jobs done on UBELIX is documented here. Account Staff accounts are preferred over student accounts! If you currently use your student Campus Account to access UBELIX, but you also possess a staff Campus Account, get in contact with us so we can activate your staff Campus Account, migrate all your user data to the new account and deactivate your student account for UBELIX. Mailing List We communicate upcoming events (e.g. maintenance downtimes) on our mailing list. Make sure that you are subscribed to this list, otherwise you will miss important announcements. Security Do not share your account If using public key authentication, do not share your private key General Communication with the Cluster Administrators Use the Service Portal for questions, issues or comments regarding UBELIX. Do not use the personal email address of a cluster administrator. This is important because it keeps all administrators informed about the ongoing problem-solving process, and if one administrator is on vacation, another administrator can help you with your question For each new problem start a new conversation with a new subject. Avoid to write to us by replying to an old answer mail from the last problem that you received from us or even worse by replying to mailing list email you received from us. The point here is that though it looks like an ordinary email, you actually are opening a new ticket in our ticket system (or reopening an old ticket if replying to an old email). Problem-Solving Process Exploit resources provided by your institute/research group before asking the UBELIX staff about domain-specific problems. We make an effort to help you, but we are no experts in your field, hence a colleague from your group who uses the cluster to solve a similar problem like you do might be a better first contact Ask Google for help before contacting us. We often also just \u201cgoogle\u201d for an answer, and then forward the outcome to you. Do not ask for/expect step-by-step solutions to a problem. Sometimes we give step-by-step instructions, but generally you should use our answers to do some refined research on the problem. If you still stuck, we are happy to provide further support Always give an exact as possible description of the problem. Provide your username, error messages, the path to the job script, the id of the job, and other hints that make the problem-solving process as economic as possible. Housekeeping Clean up your home directory frequently, in particular before asking for an increase of your quota limit Do not save thousands of files in a single directory. Distribute the files to subdirectories Job Submission Before submitting the same job a hundred times, please verify that the job finishes successfully. We often experience that hundreds of jobs getting killed due to an invalid path referenced in the job script, which generates hundreds of notification mails in our system. Cluster Performance DO NOT run resource-intensive computations directly on the login node AKA submit node. This will have a negative impact on the performance of the whole cluster. Instead, generate a job script that carries out the computations and submit this job script to the cluster using sbatch. DO NOT run server applications (PostgreSQL server, web server, \u2026) on the front-end server (submit hosts). Such a program usually run as a background process (daemon) rather than being under the direct control of an interactive user. We will immediately kill such processes.","title":"Code of Conduct"},{"location":"code-of-conduct.html#code-of-conduct","text":"On this page we list some expectations from our side and recommended practice that is crucial for maintaining a good and professional working relationship between the user and the system administrators. Most of those contents are quite self-explanatory while others help to reduce the amount of support time needed to allocate.","title":"Code of Conduct"},{"location":"code-of-conduct.html#general","text":"We assume that you are familiar with some basic knowledge about Linux command line (shell) navigation and shell scripting. If you never worked on the command line, consider some Linux tutorials on the subject first. We expect you to exploit this valuable documentation before asking for help. All that is needed to get some simple jobs done on UBELIX is documented here.","title":"General"},{"location":"code-of-conduct.html#account","text":"Staff accounts are preferred over student accounts! If you currently use your student Campus Account to access UBELIX, but you also possess a staff Campus Account, get in contact with us so we can activate your staff Campus Account, migrate all your user data to the new account and deactivate your student account for UBELIX.","title":"Account"},{"location":"code-of-conduct.html#mailing-list","text":"We communicate upcoming events (e.g. maintenance downtimes) on our mailing list. Make sure that you are subscribed to this list, otherwise you will miss important announcements.","title":"Mailing List"},{"location":"code-of-conduct.html#security","text":"Do not share your account If using public key authentication, do not share your private key","title":"Security"},{"location":"code-of-conduct.html#general-communication-with-the-cluster-administrators","text":"Use the Service Portal for questions, issues or comments regarding UBELIX. Do not use the personal email address of a cluster administrator. This is important because it keeps all administrators informed about the ongoing problem-solving process, and if one administrator is on vacation, another administrator can help you with your question For each new problem start a new conversation with a new subject. Avoid to write to us by replying to an old answer mail from the last problem that you received from us or even worse by replying to mailing list email you received from us. The point here is that though it looks like an ordinary email, you actually are opening a new ticket in our ticket system (or reopening an old ticket if replying to an old email).","title":"General Communication with the Cluster Administrators"},{"location":"code-of-conduct.html#problem-solving-process","text":"Exploit resources provided by your institute/research group before asking the UBELIX staff about domain-specific problems. We make an effort to help you, but we are no experts in your field, hence a colleague from your group who uses the cluster to solve a similar problem like you do might be a better first contact Ask Google for help before contacting us. We often also just \u201cgoogle\u201d for an answer, and then forward the outcome to you. Do not ask for/expect step-by-step solutions to a problem. Sometimes we give step-by-step instructions, but generally you should use our answers to do some refined research on the problem. If you still stuck, we are happy to provide further support Always give an exact as possible description of the problem. Provide your username, error messages, the path to the job script, the id of the job, and other hints that make the problem-solving process as economic as possible.","title":"Problem-Solving Process"},{"location":"code-of-conduct.html#housekeeping","text":"Clean up your home directory frequently, in particular before asking for an increase of your quota limit Do not save thousands of files in a single directory. Distribute the files to subdirectories","title":"Housekeeping"},{"location":"code-of-conduct.html#job-submission","text":"Before submitting the same job a hundred times, please verify that the job finishes successfully. We often experience that hundreds of jobs getting killed due to an invalid path referenced in the job script, which generates hundreds of notification mails in our system.","title":"Job Submission"},{"location":"code-of-conduct.html#cluster-performance","text":"DO NOT run resource-intensive computations directly on the login node AKA submit node. This will have a negative impact on the performance of the whole cluster. Instead, generate a job script that carries out the computations and submit this job script to the cluster using sbatch. DO NOT run server applications (PostgreSQL server, web server, \u2026) on the front-end server (submit hosts). Such a program usually run as a background process (daemon) rather than being under the direct control of an interactive user. We will immediately kill such processes.","title":"Cluster Performance"},{"location":"contributing.html","text":"Contributing You can support the UBELIX cluster in different ways: Investments Documentation Improvements Investments Some text about money\u2026 Documentation Improvements Some text aboutn contirbuting to the user guide.","title":"Contributing"},{"location":"contributing.html#contributing","text":"You can support the UBELIX cluster in different ways: Investments Documentation Improvements","title":"Contributing"},{"location":"contributing.html#investments","text":"Some text about money\u2026","title":"Investments"},{"location":"contributing.html#documentation-improvements","text":"Some text aboutn contirbuting to the user guide.","title":"Documentation Improvements"},{"location":"halloffame.html","text":"Hall of Fame If you previously used UBELIX to do your computational work and you acknowledged this in your publication and want to your publication listed here, please drop us a note at hpc@id.unibe.ch. If you are wondering how you can acknowledge the usage of UBELIX in your publication, have a look at the homepage of this documentation, where you will find a text recommendation acknoowledging the use of our cluster. Papers and Articles Authors Title Journal Boris DOI 2020 Riou J, Hauser A et al. Estimation of SARS-CoV-2 mortality during the early stages of an epidemic: A modeling study in Hubei, China, and six regions in Europe PLOS Medicine Direct Link Ricca C, Aschauer U Local polarization in oxygen-deficient LaMnO 3 induced by charge localization in the Jahn-Teller distorted structure Phys. Rev. Res. Details Direct Link Burns E, Lippert T et al. LaTiO 2 N crystallographic orientation control significantly increases visible-light induced charge extraction J. Mat. Chem. A Details Direct Link Ninova S, Aschauer U, et al. Suitability of Cu-substituted \u03b2-Mn 2 V 2 O 7 and Mn-substituted \u03b2-Cu 2 V 2 O 7 for photocatalytic water-splitting J. Chem Phys. 153 Details Direct Link Vonr\u00fcti N, Aschauer U Catalysis on oxidized ferroelectric surfaces\u2014Epitaxially strained LaTiO 2 N and BaTiO 3 for photocatalytic water splitting Chem. Mater. Details Direct Link Bouri M, Aschauer U Suitability of Different Sr 2 TaO 3 N Surface Orientations for Photocatalytic Water Oxidation Chem. Mater Details Direct Link Flores E, Berg E, et al. Cation Ordering and Redox Chemistry of Layered Ni-Rich Li x Ni 1\u20132y Co y Mn y O 2 : An Operando Raman Spectroscopy Study Chem. Mater. Details Direct Link Pawlak R, Meyer E, et al. Bottom-up Synthesis of Nitrogen-Doped Porous Graphene Nanoribbons J.Am.Chem.Soc. Details Direct Link Ricca C, Aschauer U, et al. Self-consistent DFT + U + V study of oxygen vacancies in SrTiO 3 Phys. Rev. Research 2 Details Direct Link Ninova S, Aschauer U, et al. Surface Orientation and Structure of LaTiO 2 N Nanoparticles ACS Appl. Energy Mater Details Direct Link Primasov\u00e1 H, Furrer J, et al. Dinuclear thiolato-bridged arene ruthenium complexes: from reaction conditions and mechanism to synthesis of new complexes RSC Advances Details Direct Link Pfister J-P, Gontier C Identifiability of a Binomial Synapse Front. Comput. Neurosci. Details Direct Link Riou J, Althaus C Pattern of early human-to-human transmission of Wuhan 2019 novel coronavirus (2019-nCoV), December 2019 to January 2020 Euro Surveillance Direct Link 2019 Vonr\u00fcti N, Aschauer U The role of metastability in enhancing water-oxidation activity Phys.Chem.Chem.Phys. Details Bizzotto F, Arenz M, et al. Examining the Structure Sensitivity of the Oxygen Evolution Reaction on Pt Single\u2010Crystal Electrodes: A Combined Experimental and Theoretical Study ChemPhysChem Details Direct Link Vonr\u00fcti N, Aschauer U Band-gap engineering in AB(O x S 1\u2212x ) 3 perovskite oxysulfides: a route to strongly polar materials for photocatalytic water splitting J.Mat. Chem. A Details Direct Link Ouhbi H, Aschauer U Nitrogen Loss and Oxygen Evolution Reaction Activity of Perovskite Oxynitrides ACS Materials Lett. Details Direct Link Hussain H, Thornton G, et al. Water-Induced Reversal of the TiO 2 (011)-(2 \u00d7 1) Surface Reconstruction: Observed with in Situ Surface X-ray Diffraction J.Phys. Chem C Details Direct Link Mantella V, Buonsanti R, et al. Synthesis and Size-Dependent Optical Properties of Intermediate Band Gap Cu 3 VS 4 Nanocrystals Chem. Mater Details Direct Link Aschauer U, Spaldin N, et al. Strain-induced heteronuclear charge disproportionation in EuMnO 3 Phys. Rev. Materials 3 Details Direct Link Ninova S, Aschauer U Anion-order driven polar interfaces at LaTiO 2 N surfaces Journal of Materials Chemistry A Details Direct Link Ricca C, Aschauer U et al. Self-consistent site-dependent DFT+U study of stoichiometric and defective SrMnO 3 Physical Review B Details Direct Link Ouhbia H, Aschauer U Water oxidation catalysis on reconstructed NaTaO 3 (001) surfaces Journal of Materials Chemistry A Details Direct Link Counotte M, Althaus C et al. Impact of age-specific immunity on the timing and burden of the next Zika virus outbreak PLOS NeglectedTropical Diseases Details Direct Link Brugger J, Althaus C Transmission of and susceptibility to seasonal influenza in Switzerland from 2003 to 2015 Epidemics, Elsevier Details Direct Link 2018 Horton P, Br\u00f6nnimann S Impact of global atmospheric reanalyses on statistical precipitation downscaling Climate Dynamics Details Direct Link Vonr\u00fcti N, Aschauer U Epitaxial strain dependence of band gaps in perovskite oxynitrides compared to perovskite oxides American Physical Society Details Direct Link Aschauer U Ultrafast Relaxation Dynamics of the Antiferrodistortive Phase in Ca Doped SrTiO\u2083 American Physical Society Details Direct Link Vonr\u00fcti N, Aschauer U, et al. Elucidation of Li x Ni 0.8 Co 0.15 Al 0.05 O 2 Redox Chemistry by Operando Raman Spectroscopy American Chemical Society Details Direct Link Ouhbi H, Aschauer U Water oxidation chemistry of oxynitrides and oxides: Comparing NaTaO 3 and SrTaO 2 N Surface Science Details Direct Link Aschauer U Surface and Defect Chemistry of Oxide Material CHIMIA Details Direct Link Kasper, C, Hebert, F, Aubin-Horth N, Taborsky B Divergent brain gene expression profiles between alternative behavioural helper types in a cooperative breeder Wiley Molecular Ecology Direct Link Panyasantisuk J, Dall\u2019Ara E, Pretterklieber M, Pahr D.H., Zysset P.K. Mapping anisotropy improves QCT-based finite element estimation of hip strength in pooled stance and side-fall load configurations Medical Engineering & Physics, Elsevier Direct Link Vonr\u00fcti, N, Aschauer U Anion Order and Spontaneous Polarization in LaTiO 2 N Oxynitride Thin Films American Physical Society Details Direct Link Bouri M, Aschauer U Bulk and surface properties of the Ruddlesden-Popper oxynitride Sr 2 TaO 3 N Physical Chemistry Chemical Physics Details Direct Link 2017 Aschauer, U et al. Surface Structure of TiO2 Rutile (011) Exposed to Liquid Water Journal of Physical Chemistry Details Direct Link Kasper, C, K\u00f6lliker, M, Pstma, E, Taborsky B Consistent cooperation in a cichlid fish is caused by maternal and developmental effects rather than heritable genetic variation Proceedings of the Royal Society, Biological Sciences Direct Link Riesen M, Garcia V, Low N, Althaus C Modeling the consequences of regional heterogeneity in human papillomavirus (HPV) vaccination uptake on transmission in Switzerland Vaccine, Elsevier Details Direct Link Kilic C, Raible C, Stocker T Multiple climate States of Habitable Exoplanets: The Rolf of Obliquity and Irradiance The Astrophysical Journal Details Direct Link Kilic C, Raible C, Kirk Impact of variations of gravitational acceleration on the general circulation of the planetary atmosphere Planetary and Space Science Details Direct Link Mueller S, Fix M et al. Simultaneous optimization of photons and electrons for mixed beam radiotherapy al. Physics in Medicine & Biology Direct Link Ninova S, Aschauer U Surface structure and anion order of the oynitride LaTiO 2 N Journal of Materials Chemistry A Details Direct Link Ninova S, Aschauer U et al. LaTiOxNy Thin Film Model Systems for Photocatalytic Water Splitting: Physicochemical Evolution of the Solid-Liquid Interface and the Role of the Crystallographic Orientation Advanced functional materials Details Direct Link Struchen R, Vial F, Andersson M. G. Value of evidence from syndromic surveillance with cumulative evidence from multiple data stream with delayed reporting Scientific Reports Direct Link 2013 Leichtle A, Fiedler G et al. Pancreatic carcinoma, pancreatitis, and healthy controls: metabolite models in a three-class diagnostic dilemma Metabolomics, Springer Details Direct Link Posters Newspapers Title Newspaper Year of Publication Link Berner Forscher entdecken neue Klimazust\u00e4nde, in denen Leben m\u00f6glich ist. Der Bund Direct Link","title":"Hall of Fame"},{"location":"halloffame.html#hall-of-fame","text":"If you previously used UBELIX to do your computational work and you acknowledged this in your publication and want to your publication listed here, please drop us a note at hpc@id.unibe.ch. If you are wondering how you can acknowledge the usage of UBELIX in your publication, have a look at the homepage of this documentation, where you will find a text recommendation acknoowledging the use of our cluster.","title":"Hall of Fame"},{"location":"halloffame.html#papers-and-articles","text":"Authors Title Journal Boris DOI 2020 Riou J, Hauser A et al. Estimation of SARS-CoV-2 mortality during the early stages of an epidemic: A modeling study in Hubei, China, and six regions in Europe PLOS Medicine Direct Link Ricca C, Aschauer U Local polarization in oxygen-deficient LaMnO 3 induced by charge localization in the Jahn-Teller distorted structure Phys. Rev. Res. Details Direct Link Burns E, Lippert T et al. LaTiO 2 N crystallographic orientation control significantly increases visible-light induced charge extraction J. Mat. Chem. A Details Direct Link Ninova S, Aschauer U, et al. Suitability of Cu-substituted \u03b2-Mn 2 V 2 O 7 and Mn-substituted \u03b2-Cu 2 V 2 O 7 for photocatalytic water-splitting J. Chem Phys. 153 Details Direct Link Vonr\u00fcti N, Aschauer U Catalysis on oxidized ferroelectric surfaces\u2014Epitaxially strained LaTiO 2 N and BaTiO 3 for photocatalytic water splitting Chem. Mater. Details Direct Link Bouri M, Aschauer U Suitability of Different Sr 2 TaO 3 N Surface Orientations for Photocatalytic Water Oxidation Chem. Mater Details Direct Link Flores E, Berg E, et al. Cation Ordering and Redox Chemistry of Layered Ni-Rich Li x Ni 1\u20132y Co y Mn y O 2 : An Operando Raman Spectroscopy Study Chem. Mater. Details Direct Link Pawlak R, Meyer E, et al. Bottom-up Synthesis of Nitrogen-Doped Porous Graphene Nanoribbons J.Am.Chem.Soc. Details Direct Link Ricca C, Aschauer U, et al. Self-consistent DFT + U + V study of oxygen vacancies in SrTiO 3 Phys. Rev. Research 2 Details Direct Link Ninova S, Aschauer U, et al. Surface Orientation and Structure of LaTiO 2 N Nanoparticles ACS Appl. Energy Mater Details Direct Link Primasov\u00e1 H, Furrer J, et al. Dinuclear thiolato-bridged arene ruthenium complexes: from reaction conditions and mechanism to synthesis of new complexes RSC Advances Details Direct Link Pfister J-P, Gontier C Identifiability of a Binomial Synapse Front. Comput. Neurosci. Details Direct Link Riou J, Althaus C Pattern of early human-to-human transmission of Wuhan 2019 novel coronavirus (2019-nCoV), December 2019 to January 2020 Euro Surveillance Direct Link 2019 Vonr\u00fcti N, Aschauer U The role of metastability in enhancing water-oxidation activity Phys.Chem.Chem.Phys. Details Bizzotto F, Arenz M, et al. Examining the Structure Sensitivity of the Oxygen Evolution Reaction on Pt Single\u2010Crystal Electrodes: A Combined Experimental and Theoretical Study ChemPhysChem Details Direct Link Vonr\u00fcti N, Aschauer U Band-gap engineering in AB(O x S 1\u2212x ) 3 perovskite oxysulfides: a route to strongly polar materials for photocatalytic water splitting J.Mat. Chem. A Details Direct Link Ouhbi H, Aschauer U Nitrogen Loss and Oxygen Evolution Reaction Activity of Perovskite Oxynitrides ACS Materials Lett. Details Direct Link Hussain H, Thornton G, et al. Water-Induced Reversal of the TiO 2 (011)-(2 \u00d7 1) Surface Reconstruction: Observed with in Situ Surface X-ray Diffraction J.Phys. Chem C Details Direct Link Mantella V, Buonsanti R, et al. Synthesis and Size-Dependent Optical Properties of Intermediate Band Gap Cu 3 VS 4 Nanocrystals Chem. Mater Details Direct Link Aschauer U, Spaldin N, et al. Strain-induced heteronuclear charge disproportionation in EuMnO 3 Phys. Rev. Materials 3 Details Direct Link Ninova S, Aschauer U Anion-order driven polar interfaces at LaTiO 2 N surfaces Journal of Materials Chemistry A Details Direct Link Ricca C, Aschauer U et al. Self-consistent site-dependent DFT+U study of stoichiometric and defective SrMnO 3 Physical Review B Details Direct Link Ouhbia H, Aschauer U Water oxidation catalysis on reconstructed NaTaO 3 (001) surfaces Journal of Materials Chemistry A Details Direct Link Counotte M, Althaus C et al. Impact of age-specific immunity on the timing and burden of the next Zika virus outbreak PLOS NeglectedTropical Diseases Details Direct Link Brugger J, Althaus C Transmission of and susceptibility to seasonal influenza in Switzerland from 2003 to 2015 Epidemics, Elsevier Details Direct Link 2018 Horton P, Br\u00f6nnimann S Impact of global atmospheric reanalyses on statistical precipitation downscaling Climate Dynamics Details Direct Link Vonr\u00fcti N, Aschauer U Epitaxial strain dependence of band gaps in perovskite oxynitrides compared to perovskite oxides American Physical Society Details Direct Link Aschauer U Ultrafast Relaxation Dynamics of the Antiferrodistortive Phase in Ca Doped SrTiO\u2083 American Physical Society Details Direct Link Vonr\u00fcti N, Aschauer U, et al. Elucidation of Li x Ni 0.8 Co 0.15 Al 0.05 O 2 Redox Chemistry by Operando Raman Spectroscopy American Chemical Society Details Direct Link Ouhbi H, Aschauer U Water oxidation chemistry of oxynitrides and oxides: Comparing NaTaO 3 and SrTaO 2 N Surface Science Details Direct Link Aschauer U Surface and Defect Chemistry of Oxide Material CHIMIA Details Direct Link Kasper, C, Hebert, F, Aubin-Horth N, Taborsky B Divergent brain gene expression profiles between alternative behavioural helper types in a cooperative breeder Wiley Molecular Ecology Direct Link Panyasantisuk J, Dall\u2019Ara E, Pretterklieber M, Pahr D.H., Zysset P.K. Mapping anisotropy improves QCT-based finite element estimation of hip strength in pooled stance and side-fall load configurations Medical Engineering & Physics, Elsevier Direct Link Vonr\u00fcti, N, Aschauer U Anion Order and Spontaneous Polarization in LaTiO 2 N Oxynitride Thin Films American Physical Society Details Direct Link Bouri M, Aschauer U Bulk and surface properties of the Ruddlesden-Popper oxynitride Sr 2 TaO 3 N Physical Chemistry Chemical Physics Details Direct Link 2017 Aschauer, U et al. Surface Structure of TiO2 Rutile (011) Exposed to Liquid Water Journal of Physical Chemistry Details Direct Link Kasper, C, K\u00f6lliker, M, Pstma, E, Taborsky B Consistent cooperation in a cichlid fish is caused by maternal and developmental effects rather than heritable genetic variation Proceedings of the Royal Society, Biological Sciences Direct Link Riesen M, Garcia V, Low N, Althaus C Modeling the consequences of regional heterogeneity in human papillomavirus (HPV) vaccination uptake on transmission in Switzerland Vaccine, Elsevier Details Direct Link Kilic C, Raible C, Stocker T Multiple climate States of Habitable Exoplanets: The Rolf of Obliquity and Irradiance The Astrophysical Journal Details Direct Link Kilic C, Raible C, Kirk Impact of variations of gravitational acceleration on the general circulation of the planetary atmosphere Planetary and Space Science Details Direct Link Mueller S, Fix M et al. Simultaneous optimization of photons and electrons for mixed beam radiotherapy al. Physics in Medicine & Biology Direct Link Ninova S, Aschauer U Surface structure and anion order of the oynitride LaTiO 2 N Journal of Materials Chemistry A Details Direct Link Ninova S, Aschauer U et al. LaTiOxNy Thin Film Model Systems for Photocatalytic Water Splitting: Physicochemical Evolution of the Solid-Liquid Interface and the Role of the Crystallographic Orientation Advanced functional materials Details Direct Link Struchen R, Vial F, Andersson M. G. Value of evidence from syndromic surveillance with cumulative evidence from multiple data stream with delayed reporting Scientific Reports Direct Link 2013 Leichtle A, Fiedler G et al. Pancreatic carcinoma, pancreatitis, and healthy controls: metabolite models in a three-class diagnostic dilemma Metabolomics, Springer Details Direct Link","title":"Papers and Articles"},{"location":"halloffame.html#posters","text":"","title":"Posters"},{"location":"halloffame.html#newspapers","text":"Title Newspaper Year of Publication Link Berner Forscher entdecken neue Klimazust\u00e4nde, in denen Leben m\u00f6glich ist. Der Bund Direct Link","title":"Newspapers"},{"location":"mdcheat.html","text":"Markdown Cheatsheet This page outlines all stuff available by installing the base Python-Markdown (comes with MkDocs) and the additional bundle PyMdown Extensions . Markdown Cheatsheet Headings The 3rd level The 4th level The 5th level The 6th level Headings with secondary text The 3rd level with secondary text The 4th level with secondary text The 5th level with secondary text The 6th level with secondary text Emphasis Lists Links Images Code and Syntax Highlighting !/bin/bash include Tables Blockquotes Inline HTML Horizontal Rule Line Breaks YouTube Videos Admonition Abbreviations Definition Lists Footnotes Headings ### The 3rd level #### The 4th level ##### The 5th level ###### The 6th level ## Headings <small>with secondary text</small> ### The 3rd level <small>with secondary text</small> #### The 4th level <small>with secondary text</small> ##### The 5th level <small>with secondary text</small> ###### The 6th level <small>with secondary text</small> The 3rd level The 4th level The 5th level The 6th level Headings with secondary text The 3rd level with secondary text The 4th level with secondary text The 5th level with secondary text The 6th level with secondary text Emphasis Emphasis, aka italics, with *asterisks* or _underscores_. Strong emphasis, aka bold, with **asterisks** or __underscores__. Combined emphasis with **asterisks and _underscores_**. Strikethrough uses two tildes. ~~Scratch this.~~ Emphasis, aka italics, with asterisks or underscores . Strong emphasis, aka bold, with asterisks or underscores . Combined emphasis with asterisks and underscores . Strikethrough uses two tildes. ~~Scratch this.~~ Lists (In this example, leading and trailing spaces are shown with with dots: \u22c5) 1. First ordered list item 2. Another item \u22c5\u22c5\u22c5\u22c5* Unordered sub-list. \u22c5\u22c5\u22c5\u22c5* Item 2 \u22c5\u22c5\u22c5\u22c5* Item 3 1. Actual numbers don't matter, just that it's a number \u22c5\u22c5\u22c5\u22c51. Ordered sub-list \u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c51. Ordered subsub-list \u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c51. Item 2 \u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c51. Item 3 \u22c5\u22c5\u22c5\u22c51. Item 2 \u22c5\u22c5\u22c5\u22c51. Item 3 4. And another item. \u22c5\u22c5\u22c5\u22c5You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown). \u22c5\u22c5\u22c5\u22c5To have a line break without a paragraph, you will need to use two trailing spaces.\u22c5\u22c5 \u22c5\u22c5\u22c5\u22c5Note that this line is separate, but within the same paragraph.\u22c5\u22c5 \u22c5\u22c5\u22c5\u22c5(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.) * Unordered list can use asterisks - Or minuses + Or pluses First ordered list item Another item Unordered sub-list. Item 2 Item 3 Actual numbers don\u2019t matter, just that it\u2019s a number Ordered sub-list Ordered subsub-list Item 2 Item 3 Item 2 Item 3 And another item. You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we\u2019ll use three here to also align the raw Markdown). To have a line break without a paragraph, you will need to use two trailing spaces. Note that this line is separate, but within the same paragraph. (This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.) Unordered list can use asterisks Or minuses Or pluses Links There are two ways to create links. [I'm an inline-style link](https://www.google.com) [I'm an inline-style link with title](https://www.google.com \"Google's Homepage\") [I'm a reference-style link][Arbitrary case-insensitive reference text] [I'm a relative reference to a repository file](../blob/master/LICENSE) [You can use numbers for reference-style link definitions][1] Or leave it empty and use the [link text itself]. URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <http://www.example.com> and sometimes example.com (but not on Github, for example). Some text to show that the reference links can follow later. [arbitrary case-insensitive reference text]: https://www.mozilla.org [1]: http://slashdot.org [link text itself]: http://www.reddit.com I\u2019m an inline-style link I\u2019m an inline-style link with title I\u2019m a reference-style link I\u2019m a relative reference to a repository file You can use numbers for reference-style link definitions Or leave it empty and use the link text itself . URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example). Some text to show that the reference links can follow later. Images Here's our logo (hover to see the title text): Inline-style: ![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 1\") Reference-style: ![alt text][logo] [logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 2\" Here\u2019s our logo (hover to see the title text): Inline-style: Reference-style: Code and Syntax Highlighting Code blocks are part of the Markdown spec, but syntax highlighting isn\u2019t. However, many renderers \u2013 like Github\u2019s and Markdown Here \u2013 support syntax highlighting. Which languages are supported and how those language names should be written will vary from renderer to renderer. Markdown Here supports highlighting for dozens of languages (and not-really-languages, like diffs and HTTP headers); to see the complete list, and how to write the language names, see the highlight.js demo page . Inline `code` has `back-ticks around` it. Inline code has back-ticks around it. Blocks of code are either fenced by lines with three back-ticks ``` , or are indented with four spaces. I recommend only using the fenced code blocks \u2013 they\u2019re easier and only they support syntax highlighting. var s = \"JavaScript syntax highlighting\" ; alert ( s ); s = \"Python syntax highlighting\" print s No language indicated, so no syntax highlighting in Markdown Here (varies on Github). But let's throw in a <b>tag</b>. Even tabbed code example for different language are possible: ```Bash tab= !/bin/bash STR=\u201dHello World!\u201d echo $STR ```C tab= #include int main(void) { printf(\"hello, world\\n\"); } ```C++ tab= include int main() { std::cout << \u201cHello, world!\\n\u201d; return 0; } ```C# tab= using System; class Program { static void Main(string[] args) { Console.WriteLine(\"Hello, world!\"); } } Tables Tables aren\u2019t part of the core Markdown spec, but they are part of GFM and Markdown Here supports them. They are an easy way of adding tables to your email \u2013 a task that would otherwise require copy-pasting from another application. Colons can be used to align columns. | Tables | Are | Cool | | ------------- |:-------------:| -----:| | col 3 is | right-aligned | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 | There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown | Less | Pretty --- | --- | --- *Still* | `renders` | **nicely** 1 | 2 | 3 Colons can be used to align columns. Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u2019t need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown Less Pretty Still renders nicely 1 2 3 Blockquotes > Blockquotes are very handy in email to emulate reply text. > This line is part of the same quote. Quote break. > This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can *put* **Markdown** into a blockquote. Blockquotes are very handy in email to emulate reply text. This line is part of the same quote. Quote break. This is a very long line that will still be quoted properly when it wraps. Oh boy let\u2019s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote. Blockquote nesting is possible: > **Sed aliquet**, neque at rutrum mollis, neque nisi tincidunt nibh, vitae faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem [libero fermentum](#) urna, ut efficitur elit ligula et nunc. > > Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla. Ut sit amet placerat ante. Proin sed **elementum** __nulla__. Nunc vitae sem odio. Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum. eu odio. > > > `Suspendisse rutrum facilisis risus`, eu posuere neque commodo a. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo bibendum, sodales mauris ut, tincidunt massa. Sed aliquet , neque at rutrum mollis, neque nisi tincidunt nibh, vitae faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem libero fermentum urna, ut efficitur elit ligula et nunc. Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla. Ut sit amet placerat ante. Proin sed elementum nulla. Nunc vitae sem odio. Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum. eu odio. Suspendisse rutrum facilisis risus , eu posuere neque commodo a. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo bibendum, sodales mauris ut, tincidunt massa. Other content blocks within a blockquote Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; Praesent at :::js return target , sodales nibh vel, tempor felis. Fusce vel lacinia lacus. Suspendisse rhoncus nunc non nisi iaculis ultrices. Donec consectetur mauris non neque imperdiet, eget volutpat libero. Inline HTML You can also use raw HTML in your Markdown, and it\u2019ll mostly work pretty well. Horizontal Rule Three or more... --- Hyphens *** Asterisks ___ Underscores Three or more\u2026 Hyphens Asterisks Underscores Line Breaks My basic recommendation for learning how line breaks work is to experiment and discover \u2013 hit <Enter> once (i.e., insert one newline), then hit it twice (i.e., insert two newlines), see what happens. You\u2019ll soon learn to get what you want. \u201cMarkdown Toggle\u201d is your friend. Here are some things to try out: Here's a line for us to start with. This line is separated from the one above by two newlines, so it will be a *separate paragraph*. This line is also a separate paragraph, but... This line is only separated by a single newline, so it's a separate line in the *same paragraph*. Here\u2019s a line for us to start with. This line is separated from the one above by two newlines, so it will be a separate paragraph . This line is also begins a separate paragraph, but\u2026 This line is only separated by a single newline, so it\u2019s a separate line in the same paragraph . YouTube Videos They can\u2019t be added directly but you can add an image with a link to the video like this: <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=YOUTUBE_VIDEO_ID_HERE \" target=\"_blank\"><img src=\"http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg\" alt=\"IMAGE ALT TEXT HERE\" width=\"240\" height=\"180\" border=\"10\" /></a> Or, in pure Markdown, but losing the image sizing and border: [![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg)](http://www.youtube.com/watch?v=YOUTUBE_VIDEO_ID_HERE) Referencing a bug by #bugID in your git commit links it to the slip. For example #1. Admonition !!! type \"optional explicit title within double quotes\" Any number of other indented markdown elements. This is the second paragraph. Some title Any number of other indented markdown elements. This is the second paragraph. And this is outside the admonition again. If you don\u2019t want a title, use a blank string \u201c\u201d. Don\u2019t do this at home! rST suggests the following \u201ctypes\u201d: attention, caution, danger, error, hint, important, note, tip, and warning: Some title This is type note Some title This is type hint Some title This is type tip Some title This is type important Some title This is type attention Some title This is type caution Some title This is type warning Some title This is type danger Some title This is type error Abbreviations The HTML specification is maintained by the W3C. *[HTML]: Hyper Text Markup Language *[W3C]: World Wide Web Consortium The HTML specification is maintained by the W3C . Definition Lists Apple : Pomaceous fruit of plants of the genus Malus in the family Rosaceae. Orange : The fruit of an evergreen tree of the genus Citrus. Apple Pomaceous fruit of plants of the genus Malus in the family Rosaceae. Orange The fruit of an evergreen tree of the genus Citrus. Footnotes Footnotes 1 have a label 2 and the footnote\u2019s content. Another Footnote 3 License: CC-BY This is a footnote content. \u21a9 A footnote on the label: \u201c@#$%\u201d. \u21a9 Another content \u21a9","title":"Markdown Cheatsheet"},{"location":"mdcheat.html#markdown-cheatsheet","text":"This page outlines all stuff available by installing the base Python-Markdown (comes with MkDocs) and the additional bundle PyMdown Extensions . Markdown Cheatsheet Headings The 3rd level The 4th level The 5th level The 6th level Headings with secondary text The 3rd level with secondary text The 4th level with secondary text The 5th level with secondary text The 6th level with secondary text Emphasis Lists Links Images Code and Syntax Highlighting !/bin/bash include Tables Blockquotes Inline HTML Horizontal Rule Line Breaks YouTube Videos Admonition Abbreviations Definition Lists Footnotes","title":"Markdown Cheatsheet"},{"location":"mdcheat.html#headings","text":"### The 3rd level #### The 4th level ##### The 5th level ###### The 6th level ## Headings <small>with secondary text</small> ### The 3rd level <small>with secondary text</small> #### The 4th level <small>with secondary text</small> ##### The 5th level <small>with secondary text</small> ###### The 6th level <small>with secondary text</small>","title":"Headings"},{"location":"mdcheat.html#the-3rd-level","text":"","title":"The 3rd level"},{"location":"mdcheat.html#the-4th-level","text":"","title":"The 4th level"},{"location":"mdcheat.html#the-5th-level","text":"","title":"The 5th level"},{"location":"mdcheat.html#the-6th-level","text":"","title":"The 6th level"},{"location":"mdcheat.html#headings-with-secondary-text","text":"","title":"Headings with secondary text"},{"location":"mdcheat.html#the-3rd-level-with-secondary-text","text":"","title":"The 3rd level with secondary text"},{"location":"mdcheat.html#the-4th-level-with-secondary-text","text":"","title":"The 4th level with secondary text"},{"location":"mdcheat.html#the-5th-level-with-secondary-text","text":"","title":"The 5th level with secondary text"},{"location":"mdcheat.html#the-6th-level-with-secondary-text","text":"","title":"The 6th level with secondary text"},{"location":"mdcheat.html#emphasis","text":"Emphasis, aka italics, with *asterisks* or _underscores_. Strong emphasis, aka bold, with **asterisks** or __underscores__. Combined emphasis with **asterisks and _underscores_**. Strikethrough uses two tildes. ~~Scratch this.~~ Emphasis, aka italics, with asterisks or underscores . Strong emphasis, aka bold, with asterisks or underscores . Combined emphasis with asterisks and underscores . Strikethrough uses two tildes. ~~Scratch this.~~","title":"Emphasis"},{"location":"mdcheat.html#lists","text":"(In this example, leading and trailing spaces are shown with with dots: \u22c5) 1. First ordered list item 2. Another item \u22c5\u22c5\u22c5\u22c5* Unordered sub-list. \u22c5\u22c5\u22c5\u22c5* Item 2 \u22c5\u22c5\u22c5\u22c5* Item 3 1. Actual numbers don't matter, just that it's a number \u22c5\u22c5\u22c5\u22c51. Ordered sub-list \u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c51. Ordered subsub-list \u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c51. Item 2 \u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c5\u22c51. Item 3 \u22c5\u22c5\u22c5\u22c51. Item 2 \u22c5\u22c5\u22c5\u22c51. Item 3 4. And another item. \u22c5\u22c5\u22c5\u22c5You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown). \u22c5\u22c5\u22c5\u22c5To have a line break without a paragraph, you will need to use two trailing spaces.\u22c5\u22c5 \u22c5\u22c5\u22c5\u22c5Note that this line is separate, but within the same paragraph.\u22c5\u22c5 \u22c5\u22c5\u22c5\u22c5(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.) * Unordered list can use asterisks - Or minuses + Or pluses First ordered list item Another item Unordered sub-list. Item 2 Item 3 Actual numbers don\u2019t matter, just that it\u2019s a number Ordered sub-list Ordered subsub-list Item 2 Item 3 Item 2 Item 3 And another item. You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we\u2019ll use three here to also align the raw Markdown). To have a line break without a paragraph, you will need to use two trailing spaces. Note that this line is separate, but within the same paragraph. (This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.) Unordered list can use asterisks Or minuses Or pluses","title":"Lists"},{"location":"mdcheat.html#links","text":"There are two ways to create links. [I'm an inline-style link](https://www.google.com) [I'm an inline-style link with title](https://www.google.com \"Google's Homepage\") [I'm a reference-style link][Arbitrary case-insensitive reference text] [I'm a relative reference to a repository file](../blob/master/LICENSE) [You can use numbers for reference-style link definitions][1] Or leave it empty and use the [link text itself]. URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <http://www.example.com> and sometimes example.com (but not on Github, for example). Some text to show that the reference links can follow later. [arbitrary case-insensitive reference text]: https://www.mozilla.org [1]: http://slashdot.org [link text itself]: http://www.reddit.com I\u2019m an inline-style link I\u2019m an inline-style link with title I\u2019m a reference-style link I\u2019m a relative reference to a repository file You can use numbers for reference-style link definitions Or leave it empty and use the link text itself . URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or http://www.example.com and sometimes example.com (but not on Github, for example). Some text to show that the reference links can follow later.","title":"Links"},{"location":"mdcheat.html#images","text":"Here's our logo (hover to see the title text): Inline-style: ![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 1\") Reference-style: ![alt text][logo] [logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 2\" Here\u2019s our logo (hover to see the title text): Inline-style: Reference-style:","title":"Images"},{"location":"mdcheat.html#code-and-syntax-highlighting","text":"Code blocks are part of the Markdown spec, but syntax highlighting isn\u2019t. However, many renderers \u2013 like Github\u2019s and Markdown Here \u2013 support syntax highlighting. Which languages are supported and how those language names should be written will vary from renderer to renderer. Markdown Here supports highlighting for dozens of languages (and not-really-languages, like diffs and HTTP headers); to see the complete list, and how to write the language names, see the highlight.js demo page . Inline `code` has `back-ticks around` it. Inline code has back-ticks around it. Blocks of code are either fenced by lines with three back-ticks ``` , or are indented with four spaces. I recommend only using the fenced code blocks \u2013 they\u2019re easier and only they support syntax highlighting. var s = \"JavaScript syntax highlighting\" ; alert ( s ); s = \"Python syntax highlighting\" print s No language indicated, so no syntax highlighting in Markdown Here (varies on Github). But let's throw in a <b>tag</b>. Even tabbed code example for different language are possible: ```Bash tab=","title":"Code and Syntax Highlighting"},{"location":"mdcheat.html#binbash","text":"STR=\u201dHello World!\u201d echo $STR ```C tab= #include int main(void) { printf(\"hello, world\\n\"); } ```C++ tab=","title":"!/bin/bash"},{"location":"mdcheat.html#include","text":"int main() { std::cout << \u201cHello, world!\\n\u201d; return 0; } ```C# tab= using System; class Program { static void Main(string[] args) { Console.WriteLine(\"Hello, world!\"); } }","title":"include "},{"location":"mdcheat.html#tables","text":"Tables aren\u2019t part of the core Markdown spec, but they are part of GFM and Markdown Here supports them. They are an easy way of adding tables to your email \u2013 a task that would otherwise require copy-pasting from another application. Colons can be used to align columns. | Tables | Are | Cool | | ------------- |:-------------:| -----:| | col 3 is | right-aligned | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 | There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown | Less | Pretty --- | --- | --- *Still* | `renders` | **nicely** 1 | 2 | 3 Colons can be used to align columns. Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don\u2019t need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown Less Pretty Still renders nicely 1 2 3","title":"Tables"},{"location":"mdcheat.html#blockquotes","text":"> Blockquotes are very handy in email to emulate reply text. > This line is part of the same quote. Quote break. > This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can *put* **Markdown** into a blockquote. Blockquotes are very handy in email to emulate reply text. This line is part of the same quote. Quote break. This is a very long line that will still be quoted properly when it wraps. Oh boy let\u2019s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote. Blockquote nesting is possible: > **Sed aliquet**, neque at rutrum mollis, neque nisi tincidunt nibh, vitae faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem [libero fermentum](#) urna, ut efficitur elit ligula et nunc. > > Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla. Ut sit amet placerat ante. Proin sed **elementum** __nulla__. Nunc vitae sem odio. Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum. eu odio. > > > `Suspendisse rutrum facilisis risus`, eu posuere neque commodo a. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo bibendum, sodales mauris ut, tincidunt massa. Sed aliquet , neque at rutrum mollis, neque nisi tincidunt nibh, vitae faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem libero fermentum urna, ut efficitur elit ligula et nunc. Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla. Ut sit amet placerat ante. Proin sed elementum nulla. Nunc vitae sem odio. Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum. eu odio. Suspendisse rutrum facilisis risus , eu posuere neque commodo a. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo bibendum, sodales mauris ut, tincidunt massa. Other content blocks within a blockquote Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; Praesent at :::js return target , sodales nibh vel, tempor felis. Fusce vel lacinia lacus. Suspendisse rhoncus nunc non nisi iaculis ultrices. Donec consectetur mauris non neque imperdiet, eget volutpat libero.","title":"Blockquotes"},{"location":"mdcheat.html#inline-html","text":"You can also use raw HTML in your Markdown, and it\u2019ll mostly work pretty well.","title":"Inline HTML"},{"location":"mdcheat.html#horizontal-rule","text":"Three or more... --- Hyphens *** Asterisks ___ Underscores Three or more\u2026 Hyphens Asterisks Underscores","title":"Horizontal Rule"},{"location":"mdcheat.html#line-breaks","text":"My basic recommendation for learning how line breaks work is to experiment and discover \u2013 hit <Enter> once (i.e., insert one newline), then hit it twice (i.e., insert two newlines), see what happens. You\u2019ll soon learn to get what you want. \u201cMarkdown Toggle\u201d is your friend. Here are some things to try out: Here's a line for us to start with. This line is separated from the one above by two newlines, so it will be a *separate paragraph*. This line is also a separate paragraph, but... This line is only separated by a single newline, so it's a separate line in the *same paragraph*. Here\u2019s a line for us to start with. This line is separated from the one above by two newlines, so it will be a separate paragraph . This line is also begins a separate paragraph, but\u2026 This line is only separated by a single newline, so it\u2019s a separate line in the same paragraph .","title":"Line Breaks"},{"location":"mdcheat.html#youtube-videos","text":"They can\u2019t be added directly but you can add an image with a link to the video like this: <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=YOUTUBE_VIDEO_ID_HERE \" target=\"_blank\"><img src=\"http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg\" alt=\"IMAGE ALT TEXT HERE\" width=\"240\" height=\"180\" border=\"10\" /></a> Or, in pure Markdown, but losing the image sizing and border: [![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg)](http://www.youtube.com/watch?v=YOUTUBE_VIDEO_ID_HERE) Referencing a bug by #bugID in your git commit links it to the slip. For example #1.","title":"YouTube Videos"},{"location":"mdcheat.html#admonition","text":"!!! type \"optional explicit title within double quotes\" Any number of other indented markdown elements. This is the second paragraph. Some title Any number of other indented markdown elements. This is the second paragraph. And this is outside the admonition again. If you don\u2019t want a title, use a blank string \u201c\u201d. Don\u2019t do this at home! rST suggests the following \u201ctypes\u201d: attention, caution, danger, error, hint, important, note, tip, and warning: Some title This is type note Some title This is type hint Some title This is type tip Some title This is type important Some title This is type attention Some title This is type caution Some title This is type warning Some title This is type danger Some title This is type error","title":"Admonition"},{"location":"mdcheat.html#abbreviations","text":"The HTML specification is maintained by the W3C. *[HTML]: Hyper Text Markup Language *[W3C]: World Wide Web Consortium The HTML specification is maintained by the W3C .","title":"Abbreviations"},{"location":"mdcheat.html#definition-lists","text":"Apple : Pomaceous fruit of plants of the genus Malus in the family Rosaceae. Orange : The fruit of an evergreen tree of the genus Citrus. Apple Pomaceous fruit of plants of the genus Malus in the family Rosaceae. Orange The fruit of an evergreen tree of the genus Citrus.","title":"Definition Lists"},{"location":"mdcheat.html#footnotes","text":"Footnotes 1 have a label 2 and the footnote\u2019s content. Another Footnote 3 License: CC-BY This is a footnote content. \u21a9 A footnote on the label: \u201c@#$%\u201d. \u21a9 Another content \u21a9","title":"Footnotes"},{"location":"quick-start.html","text":"Quick Start This section is intended as a brief introduction into HPC, especially to the present system UBELIX. This page is an summary, a hands-on introduction, which targets primarily users without prior knowledge in high-performance computing. However, basic Linux knowledge is a prerequisite. If you are not familiar with basic Linux commands, there are many beginner tutorials available online. After reading this page you will have composed and submitted your first job successfully to the cluster. Links are provided throughout the text to point you to more in-depth information on the topic. Cluster Rules Before we start: as everywhere where people come together, a common sense is needed to allow for a good cooperation and to enable a positive HPC experience. Be always aware that you are working on a shared system where your behaviour could have a negative impact on the workflow of other users. Please find the list of the most important guidelines in our code of conduct . Request an Account Before you can start working on the HPCs, staff and students of the University of Bern must have their Campus Account (CA) registered for the HPCs. External researchers that collaborate with an institute of the University of Bern must apply for a CA through that institute. See Accounts and Activation for more information getting access to UBELIX. HPC Workspace Workspaces provide are a collaborative environment, including group based access to permanent and temporary storage, as well as group based compute resource accounting. Research group leaders need to apply for an workspace, see Workspace Management . For an introduction to HPC Workspaces see Workspace Overview Login To connect to the cluster, you must log in to the submit host from inside the university network (e.g. from a workstation on the campus). If you want to connect from a remote location (e.g. from your computer at home) you must first establish a VPN connection to get access to the university network. To connect from a UNIX-like system (Linux, Mac OS X, MobaXterm on Windows) simply use a secure shell (SSH) to log in to the submit host: ssh <username>@submit.unibe.ch # the following is equivalent ssh -l <username> submit.unibe.ch Welcome $HOME After successful login to the cluster, your will find yourself in the directory /storage/homefs/$USER , where $USER is your Campus Account username. This is your home directory and serves as the repository for your personal files, and configurations. You can reference your home directory by ~ or $HOME . Your home directory is located on a shared file system. Therefore, all files and directories are always available on all cluster nodes and must hence not be copied between those nodes. HOME directories have a daily snapshot and backup procedures. Disk space is managed by quotas . By default, each user has 1TB of disk space available. Keep your home directory clean by regularly deleting old data or by moving data to a private storage. migration change No backup service for data in your home directory will be provided until the migration and Workspace introduction phase is finished. It is your own responsibility to backup important data to a private location. Home directories created before 2021, may still be located at /home/ubelix/<your_institute>/<your_campus_account>/ . Previous extended HOME quota will be kept until the Workspace introduction phase is finished. You can always print the current working directory using the pwd (present working directory) command: pwd /storage/homefs/<username> Copy Data At some point, you will probably need to copy files between your local computer and the cluster. There are different ways to achieve this, depending on your local operating system (OS). To copy a file from your local computer running a UNIX-like OS use the secure copy command scp on your local workstation: scp /path/to/file <username>@submit.unibe.ch:/path/to/target_dir/ To copy a file from the cluster to your local computer running a UNIX-like OS also use the secure copy command scp on your local workstation: scp <username>@submit.unibe.ch:/path/to/file /path/to/target_dir/ More information about file transfer can be found on the page File Transfer to/from UBELIX . Use Software On our HPCs you can make use of already pre-installed software or you can compile and install your own software. We use a module system to manage software packages, even different versions of the same software. This allows you to focus on getting your work done instead of compiling software. E.g. to get a list of all provided packages: module avail A package name can be added to list all packages containing that string. The module spider command encountering also results from the VitalIT software stack. Workspace software stacks module spider or module avail will only find packages in a Workspace software stack if the Workspace module for that workspace is loaded Furthermore, we are suggesting to work with so called toolchains. These are collections of modules build on top of each other. E.g. setting the environment for compiling an scientific application with math. libraries, OpenMPI and GCC, load: $ module load foss $ module list module list Currently Loaded Modules: 1 ) GCCcore/9.3.0 4 ) GCC/9.3.0 7 ) libxml2/.2.9.10-GCCcore-9.3.0 ( H ) 10 ) UCX/1.8.0-GCCcore-9.3.0 13 ) gompi/2020a 16 ) foss/2020a 2 ) zlib/.1.2.11-GCCcore-9.3.0 ( H ) 5 ) numactl/2.0.13-GCCcore-9.3.0 8 ) libpciaccess/.0.16-GCCcore-9.3.0 ( H ) 11 ) OpenMPI/4.0.3-GCC-9.3.0 14 ) FFTW/3.3.8-gompi-2020a 3 ) binutils/.2.34-GCCcore-9.3.0 ( H ) 6 ) XZ/.5.2.5-GCCcore-9.3.0 ( H ) 9 ) hwloc/2.2.0-GCCcore-9.3.0 12 ) OpenBLAS/0.3.9-GCC-9.3.0 15 ) ScaLAPACK/2.1.0-gompi-2020a Where: H: Hidden Module You can also specify version numbers there. Scope The loaded version of a software is only active in your current session. If you open a new shell you are again using the default version of the software. Therefore, it is crucial to load the required modules from within your job script. But also keep in mind that the current environment will get forwarded into a job submitted from it. This may lead to conflicting versions of loaded modules and modules loaded in the script. With the module environment you can also easily install, maintain and provide software packages in your workspaces and share with your collaborators. The Software section is dedicated to this topic. More information can be found there. Hello World Doing useful computations consists of running commands that work on data and generate a result. These computations are resource-intensive. That is what the compute nodes are there for. These over 300 servers do the heavy lifting as soon as resources are free for you. Currently you are on a submit server also known as login server. This server is for preparing the computations, i.e. downloading data, writing a job script, prepare some data etc. But you mustn\u2019t run computations on submit nodes as the server is quite a weak machines that you are sharing with others. So, you have to bring the computations to the compute nodes - by generating a job script and sending it to the cluster. Working interactively on a compute node When developing stuff it\u2019s often useful to have short iterations of try-error. Therefore it\u2019s also possible to work interactively on a compute node for a certain amount of time without having to send jobs to the cluster and wait until they finish just to see it didn\u2019t work. See Interactive Jobs for more information about this topic. It\u2019s now time for your first job script. To do some work on the cluster, you require certain resources (e.g. CPUs and memory) and a description of the computations to be done. A job consists of instructions to the scheduler in the form of option flags, and statements that describe the actual tasks. Let\u2019s start with the instructions to the scheduler: #!/bin/bash #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1GB # Put your code below this line ... The first line makes sure that the file is executed using the bash shell. The remaining lines are option flags used by the sbatch command. The page Jobs Submission outlines the most important options of sbatch . Now, let\u2019s write a simple \u201chello, world\u201d-task: ... # Put your code below this line module load Workspace/home mkdir -p $SCRATCH /my_first_job cd $SCRATCH /my_first_job echo \"Hello, UBELIX from node $( hostname ) \" > hello.txt After loading the Workspace module, we create a new directory \u2018my_first_job\u2019 within our \u201cpersonal\u201d SCRATCH directory. The variable $SCRATCH expands to /storage/scratch/users/<your_username> . Then, we change directory to the newly created directory. In the third line we print the line Hello, Ubelix from node <hostname_of_the_executing_node> and redirect the output to a file named hello.txt . The expression $(hostname) means, run the command hostname and put its output here. Save the content to a file named first.sh . The complete job script looks like this: #!/bin/bash #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1GB # Put your code below this line module load Workspace/home mkdir -p $SCRATCH /my_first_job cd $SCRATCH /my_first_job echo \"Hello, UBELIX from node $( hostname ) \" > hello.txt Schedule Your Job We can now submit our first job to the scheduler. The scheduler will then provide the requested resources to the job. If all requested resources are already available, then your job can start immediately. Otherwise your job will wait until enough resources are available. We submit our job to the scheduler using the sbatch command: sbatch first.sh Submitted batch job 32490640 If the job is submitted successfully, the command outputs a job-ID with which you can refer to your job later on. There are various options for different types of jobs provided in the scheduler. See sections Array Jobs , GPUs , and Interactive Jobs for more information Monitor Your Job You can inspect the state of our active jobs (running or pending) with the squeue command: squeue --job = 32490640 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 32490640 epyc2 job01 testuser R 0:22 1 bnode23 Here you can see that the job \u2018job01\u2019 with job-ID 32490640 is in state RUNNING (R). The job is running in the \u2018epyc2\u2019 partition (default partition) on bnode23 for 22 seconds. It is also possible that the job can not start immediately after submitting it to SLURM because the requested resources are not yet available. In this case, the output could look like this: squeue --job = 32490640 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 32490640 epyc2 job01 testuser PD 0:00 1 (Priority) Here you can see that the job is in state PENDING (PD) and a reason why the job is pending. In this example, the job has to wait for at least one other job with higher priority. See here for a list of other reasons why a job might be pending. You can always list all your active (pending or running) jobs with squeue: squeue --user = testuser JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 34651451 epyc2 slurm.sh testuser PD 0:00 2 (Priority) 34651453 epyc2 slurm.sh testuser PD 0:00 2 (Priority) 29143227 epyc2 Rjob testuser PD 0:00 4 (JobHeldUser) 37856328 bdw mpi.sh testuser R 4:38 2 anode[012-014] 32634559 bdw fast.sh testuser R 2:52:37 1 anode12 32634558 bdw fast.sh testuser R 3:00:54 1 anode14 32634554 bdw fast.sh testuser R 4:11:26 1 anode08 32633556 bdw fast.sh testuser R 4:36:10 1 anode08 Further information on on job monitoring you find on page Monitoring Jobs . Furthermore, in the Job handling section you find additional information about Investigating a Job Failure and Check-pointing . Training Courses Science IT Support (ScITS) regularly conducts introductory and advanced courses on Linux, UBELIX and other topics. Details outlined on their pages .","title":"Quick Start"},{"location":"quick-start.html#quick-start","text":"This section is intended as a brief introduction into HPC, especially to the present system UBELIX. This page is an summary, a hands-on introduction, which targets primarily users without prior knowledge in high-performance computing. However, basic Linux knowledge is a prerequisite. If you are not familiar with basic Linux commands, there are many beginner tutorials available online. After reading this page you will have composed and submitted your first job successfully to the cluster. Links are provided throughout the text to point you to more in-depth information on the topic.","title":"Quick Start"},{"location":"quick-start.html#cluster-rules","text":"Before we start: as everywhere where people come together, a common sense is needed to allow for a good cooperation and to enable a positive HPC experience. Be always aware that you are working on a shared system where your behaviour could have a negative impact on the workflow of other users. Please find the list of the most important guidelines in our code of conduct .","title":"Cluster Rules"},{"location":"quick-start.html#request-an-account","text":"Before you can start working on the HPCs, staff and students of the University of Bern must have their Campus Account (CA) registered for the HPCs. External researchers that collaborate with an institute of the University of Bern must apply for a CA through that institute. See Accounts and Activation for more information getting access to UBELIX.","title":"Request an Account"},{"location":"quick-start.html#hpc-workspace","text":"Workspaces provide are a collaborative environment, including group based access to permanent and temporary storage, as well as group based compute resource accounting. Research group leaders need to apply for an workspace, see Workspace Management . For an introduction to HPC Workspaces see Workspace Overview","title":"HPC Workspace"},{"location":"quick-start.html#login","text":"To connect to the cluster, you must log in to the submit host from inside the university network (e.g. from a workstation on the campus). If you want to connect from a remote location (e.g. from your computer at home) you must first establish a VPN connection to get access to the university network. To connect from a UNIX-like system (Linux, Mac OS X, MobaXterm on Windows) simply use a secure shell (SSH) to log in to the submit host: ssh <username>@submit.unibe.ch # the following is equivalent ssh -l <username> submit.unibe.ch","title":"Login"},{"location":"quick-start.html#welcome-home","text":"After successful login to the cluster, your will find yourself in the directory /storage/homefs/$USER , where $USER is your Campus Account username. This is your home directory and serves as the repository for your personal files, and configurations. You can reference your home directory by ~ or $HOME . Your home directory is located on a shared file system. Therefore, all files and directories are always available on all cluster nodes and must hence not be copied between those nodes. HOME directories have a daily snapshot and backup procedures. Disk space is managed by quotas . By default, each user has 1TB of disk space available. Keep your home directory clean by regularly deleting old data or by moving data to a private storage. migration change No backup service for data in your home directory will be provided until the migration and Workspace introduction phase is finished. It is your own responsibility to backup important data to a private location. Home directories created before 2021, may still be located at /home/ubelix/<your_institute>/<your_campus_account>/ . Previous extended HOME quota will be kept until the Workspace introduction phase is finished. You can always print the current working directory using the pwd (present working directory) command: pwd /storage/homefs/<username>","title":"Welcome $HOME"},{"location":"quick-start.html#copy-data","text":"At some point, you will probably need to copy files between your local computer and the cluster. There are different ways to achieve this, depending on your local operating system (OS). To copy a file from your local computer running a UNIX-like OS use the secure copy command scp on your local workstation: scp /path/to/file <username>@submit.unibe.ch:/path/to/target_dir/ To copy a file from the cluster to your local computer running a UNIX-like OS also use the secure copy command scp on your local workstation: scp <username>@submit.unibe.ch:/path/to/file /path/to/target_dir/ More information about file transfer can be found on the page File Transfer to/from UBELIX .","title":"Copy Data"},{"location":"quick-start.html#use-software","text":"On our HPCs you can make use of already pre-installed software or you can compile and install your own software. We use a module system to manage software packages, even different versions of the same software. This allows you to focus on getting your work done instead of compiling software. E.g. to get a list of all provided packages: module avail A package name can be added to list all packages containing that string. The module spider command encountering also results from the VitalIT software stack. Workspace software stacks module spider or module avail will only find packages in a Workspace software stack if the Workspace module for that workspace is loaded Furthermore, we are suggesting to work with so called toolchains. These are collections of modules build on top of each other. E.g. setting the environment for compiling an scientific application with math. libraries, OpenMPI and GCC, load: $ module load foss $ module list module list Currently Loaded Modules: 1 ) GCCcore/9.3.0 4 ) GCC/9.3.0 7 ) libxml2/.2.9.10-GCCcore-9.3.0 ( H ) 10 ) UCX/1.8.0-GCCcore-9.3.0 13 ) gompi/2020a 16 ) foss/2020a 2 ) zlib/.1.2.11-GCCcore-9.3.0 ( H ) 5 ) numactl/2.0.13-GCCcore-9.3.0 8 ) libpciaccess/.0.16-GCCcore-9.3.0 ( H ) 11 ) OpenMPI/4.0.3-GCC-9.3.0 14 ) FFTW/3.3.8-gompi-2020a 3 ) binutils/.2.34-GCCcore-9.3.0 ( H ) 6 ) XZ/.5.2.5-GCCcore-9.3.0 ( H ) 9 ) hwloc/2.2.0-GCCcore-9.3.0 12 ) OpenBLAS/0.3.9-GCC-9.3.0 15 ) ScaLAPACK/2.1.0-gompi-2020a Where: H: Hidden Module You can also specify version numbers there. Scope The loaded version of a software is only active in your current session. If you open a new shell you are again using the default version of the software. Therefore, it is crucial to load the required modules from within your job script. But also keep in mind that the current environment will get forwarded into a job submitted from it. This may lead to conflicting versions of loaded modules and modules loaded in the script. With the module environment you can also easily install, maintain and provide software packages in your workspaces and share with your collaborators. The Software section is dedicated to this topic. More information can be found there.","title":"Use Software"},{"location":"quick-start.html#hello-world","text":"Doing useful computations consists of running commands that work on data and generate a result. These computations are resource-intensive. That is what the compute nodes are there for. These over 300 servers do the heavy lifting as soon as resources are free for you. Currently you are on a submit server also known as login server. This server is for preparing the computations, i.e. downloading data, writing a job script, prepare some data etc. But you mustn\u2019t run computations on submit nodes as the server is quite a weak machines that you are sharing with others. So, you have to bring the computations to the compute nodes - by generating a job script and sending it to the cluster. Working interactively on a compute node When developing stuff it\u2019s often useful to have short iterations of try-error. Therefore it\u2019s also possible to work interactively on a compute node for a certain amount of time without having to send jobs to the cluster and wait until they finish just to see it didn\u2019t work. See Interactive Jobs for more information about this topic. It\u2019s now time for your first job script. To do some work on the cluster, you require certain resources (e.g. CPUs and memory) and a description of the computations to be done. A job consists of instructions to the scheduler in the form of option flags, and statements that describe the actual tasks. Let\u2019s start with the instructions to the scheduler: #!/bin/bash #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1GB # Put your code below this line ... The first line makes sure that the file is executed using the bash shell. The remaining lines are option flags used by the sbatch command. The page Jobs Submission outlines the most important options of sbatch . Now, let\u2019s write a simple \u201chello, world\u201d-task: ... # Put your code below this line module load Workspace/home mkdir -p $SCRATCH /my_first_job cd $SCRATCH /my_first_job echo \"Hello, UBELIX from node $( hostname ) \" > hello.txt After loading the Workspace module, we create a new directory \u2018my_first_job\u2019 within our \u201cpersonal\u201d SCRATCH directory. The variable $SCRATCH expands to /storage/scratch/users/<your_username> . Then, we change directory to the newly created directory. In the third line we print the line Hello, Ubelix from node <hostname_of_the_executing_node> and redirect the output to a file named hello.txt . The expression $(hostname) means, run the command hostname and put its output here. Save the content to a file named first.sh . The complete job script looks like this: #!/bin/bash #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1GB # Put your code below this line module load Workspace/home mkdir -p $SCRATCH /my_first_job cd $SCRATCH /my_first_job echo \"Hello, UBELIX from node $( hostname ) \" > hello.txt","title":"Hello World"},{"location":"quick-start.html#schedule-your-job","text":"We can now submit our first job to the scheduler. The scheduler will then provide the requested resources to the job. If all requested resources are already available, then your job can start immediately. Otherwise your job will wait until enough resources are available. We submit our job to the scheduler using the sbatch command: sbatch first.sh Submitted batch job 32490640 If the job is submitted successfully, the command outputs a job-ID with which you can refer to your job later on. There are various options for different types of jobs provided in the scheduler. See sections Array Jobs , GPUs , and Interactive Jobs for more information","title":"Schedule Your Job"},{"location":"quick-start.html#monitor-your-job","text":"You can inspect the state of our active jobs (running or pending) with the squeue command: squeue --job = 32490640 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 32490640 epyc2 job01 testuser R 0:22 1 bnode23 Here you can see that the job \u2018job01\u2019 with job-ID 32490640 is in state RUNNING (R). The job is running in the \u2018epyc2\u2019 partition (default partition) on bnode23 for 22 seconds. It is also possible that the job can not start immediately after submitting it to SLURM because the requested resources are not yet available. In this case, the output could look like this: squeue --job = 32490640 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 32490640 epyc2 job01 testuser PD 0:00 1 (Priority) Here you can see that the job is in state PENDING (PD) and a reason why the job is pending. In this example, the job has to wait for at least one other job with higher priority. See here for a list of other reasons why a job might be pending. You can always list all your active (pending or running) jobs with squeue: squeue --user = testuser JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 34651451 epyc2 slurm.sh testuser PD 0:00 2 (Priority) 34651453 epyc2 slurm.sh testuser PD 0:00 2 (Priority) 29143227 epyc2 Rjob testuser PD 0:00 4 (JobHeldUser) 37856328 bdw mpi.sh testuser R 4:38 2 anode[012-014] 32634559 bdw fast.sh testuser R 2:52:37 1 anode12 32634558 bdw fast.sh testuser R 3:00:54 1 anode14 32634554 bdw fast.sh testuser R 4:11:26 1 anode08 32633556 bdw fast.sh testuser R 4:36:10 1 anode08 Further information on on job monitoring you find on page Monitoring Jobs . Furthermore, in the Job handling section you find additional information about Investigating a Job Failure and Check-pointing .","title":"Monitor Your Job"},{"location":"quick-start.html#training-courses","text":"Science IT Support (ScITS) regularly conducts introductory and advanced courses on Linux, UBELIX and other topics. Details outlined on their pages .","title":"Training Courses"},{"location":"file-system/file-transfer.html","text":"File Transfer from/to UBELIX Description This page contains some basic information about moving files between your local workstation and the cluster. Mac/Linux/Windows You can use different protocols/programs for transferring files from/to the cluster, depending on your need: Sftp, SCP, Rsync, Wget, and others. The following commands are from on your local workstation as indicated by \u201clocal$\u201d If you have customized your SSH environment as described here, you can substitute your host alias for @submit.unibe.ch in the following commands Secure Copy (SCP) - Mac/Linux Secure Copy is a program (also a protocol) that allows you to securely transfer files between local and remote hosts. SCP uses SSH for transferring data and managing authentication. SCP performs a plain linear copy of the specified files, while replacing already existing files with the same name. If you need more sophisticated control over your copy process, consider Rsync . Syntax scp [options] source destination Some common options -r : copy directories recursively (Note that SCP follows symbolic links encountered in the tree traversal) -p : preserve modification time, access time, and modes from the original file -v : verbose mode Copying Files from Your Local Workstation to UBELIX Copy the file ~/dir/file01 to your remote home directory: $ scp ~/dir/file01 <username>@submit.unibe.ch: Copy multiple files to the remote directory ~/bar : The destination directory must already exist. You can create a directory from remote with: ssh @submit.unibe.ch \u2018mkdir -p ~/bar\u2019 $ scp ~/dir/file01 ~/dir/file02 ~/dir/file03 <username>@submit.unibe.ch:bar Copy all files within directory ~/dir to the remote directory ~/bar : Add the -r option (recursive) to also copy all subdirectories of ~/dir $ scp -r ~/dir/* <username>@submit.unibe.ch:bar Copy the directory ~/dir to your remote home directory: This will create a new directory ~/dir on the remote host. If the directory ~/dir already exists, the following command adds the content of the source directory to the destination directory $ scp -r ~/dir <username>@submit.unibe.ch: Copying Files from UBELIX to Your Local Workstation Copy the remote file ~/bar/file01 to the current working directory on your local workstation: $ scp <username>@submit.unibe.ch:bar/file01 . Copy multiple remote files to the local directory ~/dir : The local directory ~/dir will be automatically created if it does not already exist $ scp <username>@submit.unibe.ch:bar/ \\{ file02,file03,file04 \\} ~/dir Copy the remote directory ~/bar to the current working directory on your local workstation: $ scp -r <username>@submit.unibe.ch:bar . Remote Sync (Rsync) - Mac/Linux Rsync implements a sophisticated algorithm that allows to transfer only missing/non-matching parts of a source file to update a target file. With this the process of transferring data may be significantly faster than simply replacing all data. Among other things, Rsync also allows you to specify complex filter rules to exclude certain files or directories located inside a directory that you want to sync. Syntax rsync [options] source destination Some common options -r : copy directories recursively (does not preserve timestamps and permissions) -a : archive mode (like -r, but also preserves timestamps, permissions, ownership, and copies symlinks as symlinks) -z : compress data -v : verbose mode (additional v\u2019s will increase verbosity level) -n : dry-run -h : output numbers in a human readable format Copying Files from Your Local Workstation to UBELIX Copy the file ~/dir/file01 to your remote home directory: $ rsync ~/dir/file01 <username>@submit.unibe.ch: Copy multiple files to your remote home directory: $ rsync file01 file02 file03 <username>@submit.unibe.ch: Copy the local directory ~/dir to the remote directory ~/bar: With a trailing slash (/) after the source directory only the content of the source directory is copied to the destination directory. Without a trailing slash both the source directory and the content of the directory are copied to the destination directory $ rsync -az ~/dir/ <username>@submit.unibe.ch:bar Copying Files from UBELIX to Your Local Workstation Copy the remote file ~/foo/file01 to your current working directory: $ rsync <username>@submit.unibe.ch:foo/file01 . Copy the remote files ~/foo/file01 and ~/bar/file02 to your the local directory ~/dir : $ rsync <username>@submit.unibe.ch: \\{ foo/file01,bar/file02 \\} ~/dir Copy the remote directory ~/foo to the local directory ~/dir : With a trailing slash (/) after the source directory only the content of the source directory is copied to the destination directory. Without a trailing slash both the source directory and the content of the directory are copied to the destination directory. $ rsync -az <username>@submit.unibe.ch:foo/ ~/dir Including/Excluding Files With the --include / --exclude options you can specify patterns, that describe which files are not excluded/excluded from the copy process. Use the -n option with the -v option to perform a dry-run while listing the files that would be copied Exclude a specific directory: rsync -av --exclude \"subdir1\" ~/dir/ <username>@submit.unibe.ch: Copy only files with suffix .txt and .m : rsync -av --include \"*.txt\" --include \"*.m\" --exclude \"*\" ~/dir/ <username>@submit.unibe.ch: Copy all files with suffix .m within the source directory ~/dir (including matching files within subdirectories) to the remote destination directory ~/foo : Use the --prune-empty-dirs option to omit copying empty directories $ rsync -av --prune-empty-dirs --include \"*/\" --include \"*.m\" --exclude \"*\" ~/dir/ <username>@submit.unibe.ch:foo Deleting Files None of the following commands will delete any files in your source folder This delete options can be dangerous if used incorrectly! Perform a dry-run ( -n option) first and verify that important files are not listed ( -v option) for deletion Use the --delete option to delete files/directories from the destination directory that are not/no more present in the source directory: $ rsync -av --delete ~/dir/ <username>@submit.unibe.ch:mfiles With the --delete-excluded option you can additionally delete files from the destination directory that are excluded from transferring/syncing (not in the generated file list): $ rsync -av --prune-empty-dirs --delete-excluded --include \"*/\" --include \"*.m\" --exclude \"*\" ~/dir/ <username>@submit.unibe.ch:foo WinSCP - Windows We use WinSCP to illustrate file transfers from Windows. There are of course other tools that serve the same purpose. Type You can download WinSCP from https://winscp.net/eng/index.php Open WinSCP and select \u201cNew Site\u201d in the left field to define the session parameters: File protocol: SFTP Host name: submit.unibe.ch Port number: 22 User name: (Enter your username here) Click the \u201cSave\u201d button to save your site Specify an alias for the site: Site name: (Choose your own alias, e.g. ubelix) Press the \u201cOK\u201d button to save the session as a site In the left field, double-click on your alias to open the connection, enter your Campus Account password and click \u201cOK\u201d: Now you can move (drag and drop) files between your work station and the submit host using the two views:","title":"Moving files to and from the HPCs"},{"location":"file-system/file-transfer.html#file-transfer-fromto-ubelix","text":"","title":"File Transfer from/to UBELIX"},{"location":"file-system/file-transfer.html#description","text":"This page contains some basic information about moving files between your local workstation and the cluster.","title":"Description"},{"location":"file-system/file-transfer.html#maclinuxwindows","text":"You can use different protocols/programs for transferring files from/to the cluster, depending on your need: Sftp, SCP, Rsync, Wget, and others. The following commands are from on your local workstation as indicated by \u201clocal$\u201d If you have customized your SSH environment as described here, you can substitute your host alias for @submit.unibe.ch in the following commands","title":"Mac/Linux/Windows"},{"location":"file-system/file-transfer.html#secure-copy-scp-maclinux","text":"Secure Copy is a program (also a protocol) that allows you to securely transfer files between local and remote hosts. SCP uses SSH for transferring data and managing authentication. SCP performs a plain linear copy of the specified files, while replacing already existing files with the same name. If you need more sophisticated control over your copy process, consider Rsync . Syntax scp [options] source destination Some common options -r : copy directories recursively (Note that SCP follows symbolic links encountered in the tree traversal) -p : preserve modification time, access time, and modes from the original file -v : verbose mode","title":"Secure Copy (SCP) - Mac/Linux"},{"location":"file-system/file-transfer.html#copying-files-from-your-local-workstation-to-ubelix","text":"Copy the file ~/dir/file01 to your remote home directory: $ scp ~/dir/file01 <username>@submit.unibe.ch: Copy multiple files to the remote directory ~/bar : The destination directory must already exist. You can create a directory from remote with: ssh @submit.unibe.ch \u2018mkdir -p ~/bar\u2019 $ scp ~/dir/file01 ~/dir/file02 ~/dir/file03 <username>@submit.unibe.ch:bar Copy all files within directory ~/dir to the remote directory ~/bar : Add the -r option (recursive) to also copy all subdirectories of ~/dir $ scp -r ~/dir/* <username>@submit.unibe.ch:bar Copy the directory ~/dir to your remote home directory: This will create a new directory ~/dir on the remote host. If the directory ~/dir already exists, the following command adds the content of the source directory to the destination directory $ scp -r ~/dir <username>@submit.unibe.ch:","title":"Copying Files from Your Local Workstation to UBELIX"},{"location":"file-system/file-transfer.html#copying-files-from-ubelix-to-your-local-workstation","text":"Copy the remote file ~/bar/file01 to the current working directory on your local workstation: $ scp <username>@submit.unibe.ch:bar/file01 . Copy multiple remote files to the local directory ~/dir : The local directory ~/dir will be automatically created if it does not already exist $ scp <username>@submit.unibe.ch:bar/ \\{ file02,file03,file04 \\} ~/dir Copy the remote directory ~/bar to the current working directory on your local workstation: $ scp -r <username>@submit.unibe.ch:bar .","title":"Copying Files from UBELIX to Your Local Workstation"},{"location":"file-system/file-transfer.html#remote-sync-rsync-maclinux","text":"Rsync implements a sophisticated algorithm that allows to transfer only missing/non-matching parts of a source file to update a target file. With this the process of transferring data may be significantly faster than simply replacing all data. Among other things, Rsync also allows you to specify complex filter rules to exclude certain files or directories located inside a directory that you want to sync. Syntax rsync [options] source destination Some common options -r : copy directories recursively (does not preserve timestamps and permissions) -a : archive mode (like -r, but also preserves timestamps, permissions, ownership, and copies symlinks as symlinks) -z : compress data -v : verbose mode (additional v\u2019s will increase verbosity level) -n : dry-run -h : output numbers in a human readable format","title":"Remote Sync (Rsync) - Mac/Linux"},{"location":"file-system/file-transfer.html#copying-files-from-your-local-workstation-to-ubelix_1","text":"Copy the file ~/dir/file01 to your remote home directory: $ rsync ~/dir/file01 <username>@submit.unibe.ch: Copy multiple files to your remote home directory: $ rsync file01 file02 file03 <username>@submit.unibe.ch: Copy the local directory ~/dir to the remote directory ~/bar: With a trailing slash (/) after the source directory only the content of the source directory is copied to the destination directory. Without a trailing slash both the source directory and the content of the directory are copied to the destination directory $ rsync -az ~/dir/ <username>@submit.unibe.ch:bar","title":"Copying Files from Your Local Workstation to UBELIX"},{"location":"file-system/file-transfer.html#copying-files-from-ubelix-to-your-local-workstation_1","text":"Copy the remote file ~/foo/file01 to your current working directory: $ rsync <username>@submit.unibe.ch:foo/file01 . Copy the remote files ~/foo/file01 and ~/bar/file02 to your the local directory ~/dir : $ rsync <username>@submit.unibe.ch: \\{ foo/file01,bar/file02 \\} ~/dir Copy the remote directory ~/foo to the local directory ~/dir : With a trailing slash (/) after the source directory only the content of the source directory is copied to the destination directory. Without a trailing slash both the source directory and the content of the directory are copied to the destination directory. $ rsync -az <username>@submit.unibe.ch:foo/ ~/dir","title":"Copying Files from UBELIX to Your Local Workstation"},{"location":"file-system/file-transfer.html#includingexcluding-files","text":"With the --include / --exclude options you can specify patterns, that describe which files are not excluded/excluded from the copy process. Use the -n option with the -v option to perform a dry-run while listing the files that would be copied Exclude a specific directory: rsync -av --exclude \"subdir1\" ~/dir/ <username>@submit.unibe.ch: Copy only files with suffix .txt and .m : rsync -av --include \"*.txt\" --include \"*.m\" --exclude \"*\" ~/dir/ <username>@submit.unibe.ch: Copy all files with suffix .m within the source directory ~/dir (including matching files within subdirectories) to the remote destination directory ~/foo : Use the --prune-empty-dirs option to omit copying empty directories $ rsync -av --prune-empty-dirs --include \"*/\" --include \"*.m\" --exclude \"*\" ~/dir/ <username>@submit.unibe.ch:foo","title":"Including/Excluding Files"},{"location":"file-system/file-transfer.html#deleting-files","text":"None of the following commands will delete any files in your source folder This delete options can be dangerous if used incorrectly! Perform a dry-run ( -n option) first and verify that important files are not listed ( -v option) for deletion Use the --delete option to delete files/directories from the destination directory that are not/no more present in the source directory: $ rsync -av --delete ~/dir/ <username>@submit.unibe.ch:mfiles With the --delete-excluded option you can additionally delete files from the destination directory that are excluded from transferring/syncing (not in the generated file list): $ rsync -av --prune-empty-dirs --delete-excluded --include \"*/\" --include \"*.m\" --exclude \"*\" ~/dir/ <username>@submit.unibe.ch:foo","title":"Deleting Files"},{"location":"file-system/file-transfer.html#winscp-windows","text":"We use WinSCP to illustrate file transfers from Windows. There are of course other tools that serve the same purpose. Type You can download WinSCP from https://winscp.net/eng/index.php Open WinSCP and select \u201cNew Site\u201d in the left field to define the session parameters: File protocol: SFTP Host name: submit.unibe.ch Port number: 22 User name: (Enter your username here) Click the \u201cSave\u201d button to save your site Specify an alias for the site: Site name: (Choose your own alias, e.g. ubelix) Press the \u201cOK\u201d button to save the session as a site In the left field, double-click on your alias to open the connection, enter your Campus Account password and click \u201cOK\u201d: Now you can move (drag and drop) files between your work station and the submit host using the two views:","title":"WinSCP - Windows"},{"location":"file-system/filesystem-overview.html","text":"File Systems Overview HOME: $HOME directories are located under /home/$USER , where $USER is the campus account. $HOME is limited to maximum 1TB and is meant for private and configuration data. There is no data sharing workflow considerred for $HOME . If you want to share data with collaborators please ask your research group leader to request a HPC Workspace. Reqular Snapshots provide possibility to recover accidentally modified or deleted data. Some application, by default, use the $HOME file system even for larger amount of data, e.g. user packages in Python or R. Often this can be redirected, e.g. using --prefix option or in worse case using a symbolic link to a project directory. Get in touch with us if you need assistance. HPC Workspaces In HPC Workspaces access is defined in two user defined access groups. A primary read/write group is meant to be the data owner, while a secondary has only read access. The sizes of these spaces are managed by the Workspace owners. Beside a free of charge quota, the space can be extended with costs. For more details see Workspace management Research Storage shares The predecessor of HPC Workspaces are Research Storage Shares. These file spaces are located under /storage/research/... . These are fully managed by HPC support team. All modifications need to be requested via support request on the Service Portal. We aim to migrate these spaces and their data into HPC Workspaces. Therewith users can use all the tools and framework around HPC Workspaces and also the free of charge quota. SCRATCH SCRATCH is a temporary space with less restrictive limitations in size, but more restrictive limitation in time. There is no snapshot or backup service implemented in that space. Cleaning Policy under Construction detailed information will follow soon Quota For HOME and WORKSPACES total storage size and amount of files are limited. The current used amount and limits can be displayed using the quota tool, see File System Quota","title":"Overview"},{"location":"file-system/filesystem-overview.html#file-systems-overview","text":"","title":"File Systems Overview"},{"location":"file-system/filesystem-overview.html#home","text":"$HOME directories are located under /home/$USER , where $USER is the campus account. $HOME is limited to maximum 1TB and is meant for private and configuration data. There is no data sharing workflow considerred for $HOME . If you want to share data with collaborators please ask your research group leader to request a HPC Workspace. Reqular Snapshots provide possibility to recover accidentally modified or deleted data. Some application, by default, use the $HOME file system even for larger amount of data, e.g. user packages in Python or R. Often this can be redirected, e.g. using --prefix option or in worse case using a symbolic link to a project directory. Get in touch with us if you need assistance.","title":"HOME:"},{"location":"file-system/filesystem-overview.html#hpc-workspaces","text":"In HPC Workspaces access is defined in two user defined access groups. A primary read/write group is meant to be the data owner, while a secondary has only read access. The sizes of these spaces are managed by the Workspace owners. Beside a free of charge quota, the space can be extended with costs. For more details see Workspace management","title":"HPC Workspaces"},{"location":"file-system/filesystem-overview.html#research-storage-shares","text":"The predecessor of HPC Workspaces are Research Storage Shares. These file spaces are located under /storage/research/... . These are fully managed by HPC support team. All modifications need to be requested via support request on the Service Portal. We aim to migrate these spaces and their data into HPC Workspaces. Therewith users can use all the tools and framework around HPC Workspaces and also the free of charge quota.","title":"Research Storage shares"},{"location":"file-system/filesystem-overview.html#scratch","text":"SCRATCH is a temporary space with less restrictive limitations in size, but more restrictive limitation in time. There is no snapshot or backup service implemented in that space.","title":"SCRATCH"},{"location":"file-system/filesystem-overview.html#cleaning-policy","text":"under Construction detailed information will follow soon","title":"Cleaning Policy"},{"location":"file-system/filesystem-overview.html#quota","text":"For HOME and WORKSPACES total storage size and amount of files are limited. The current used amount and limits can be displayed using the quota tool, see File System Quota","title":"Quota"},{"location":"file-system/quota.html","text":"File System Quota Description This page contains information about quota limits on the parallel file system. Quotas are enabled to control the file system usage. migration change Previously HOME directories had limits of 3 TB per user, file quota limits of 2 million files. In some cases this was extended. The previous HOME quota will kept active until the Workspace introduction phase is finished. Job abortion Jobs will fail if no more disk space can be allocated, or if no more files can be created because the respective quota hard limit is exceeded Quotas space quota file quota backup expiration HOME 1TB 1 1M 1 yes 1 - WORKSPACE free: up to 10TB per research group 2 1M per TB yes 1 year 3 SCRATCH 50TB 50M no 1 month Display quota information A quota tool is delivered with the Workspace module: $ quota UniBE Workspace Quota report ============================ : used ( GB )( % ) , quota ( GB ) | files used ( % ) , quota ================================================================================== HOME : 420 ( 41 % ) , 1024 | 773425 ( 77 % ) , 1000000 Workspace1 : 101 ( 1 % ) , 10240 | 3 ( 0 % ) , 10000000 Furthermore, there is a more detailed version using the -l or --long option $ quota -l UniBE Workspace Quota report ============================ : free quota, used ( GB )( % ) , quota ( GB ) | files used ( % ) , quota | start date ( 1 ) , average quota ( 2 ) ================================================================================================================================ HOME : all, 421 ( 41 % ) , 1024 | 796058 ( 79 % ) , 1000000 | , Workspace1 : 5 , 101 ( 1 % ) , 10240 | 4 ( 0 % ) , 10000000 | 2021 -02-25, 7 .5833 ( 1 ) accounting period start date, The date from which the average usage is computed. ( 2 ) file space average quota ( not files ) , calculated by the average of messured values in the actual accounting period. In the last example the workspace Workspace1 has 5TB of free quota, and a total of 10TB of quota ( 5TB additional storage requested). The start date defines the start of the accounting period and the average quota is computed as average over all datapoints starting from start date . data gathering Workspaces: Workspace quota information is gathered twice a day. Thus the presented data may not completely represent the current state. HOME: values presented are actual values directly from the file system Note: the coloring of the relative values is green (<70%), yellow (70% < x < 90%), red (>90%). advanced quota method The following mmlsquota command present you actual values from the file system. For $HOME : $ mmlsquota --block-size = G -u $USER rs_gpfs:svc_homefs Block Limits | File Limits Filesystem Fileset type KB quota limit in_doubt grace | files quota limit in_doubt grace Remarks rs_gpfs svc_homefs USR 444181792 1073741824 1073741824 6072144 none | 815985 1000000 1000000 2462 none migration change If your HOME is not yet migrated, you can check your quota on the old file system using: $mmlsquota gpfs . The quota limits for the institute directories can be gathered using: mmlsquota -g id --block-size T , for the institute id . The --block-size option specify the unit {K , M, G, T} in which the numbers of blocks are displayed: mmlsquota --block-size = G -j workspace1 rs_gpfs Block Limits | File Limits Filesystem type GB quota limit in_doubt grace | files quota limit in_doubt grace Remarks rs_gpfs FILESET 57 10240 11264 0 none | 5 10000000 11000000 0 none The output shows the quotas for a worspace called Workspace1 . The quotas are set to a soft limit of 10240 GB, and a hard limit of 11264 GB. 57 GB is currently allocated to the workspace. An in_doubt value greater than zero means that the quota system has not yet been updated as to whether the space that is in doubt is still available or not. If the user/workspace exceeds the soft limit, the grace period will be set to one week. If usage is not reduced to a level below the soft limit during that time, the quota system interprets the soft limit as the hard limit and no further allocation is allowed. The user(s) can reset this condition by reducing usage enough to fall below the soft limit. The maximum amount of disk space the workspace/user can accumulate during the grace period is defined by the hard limit. The same information is also displayed for the file limits (number of files). Request Higher Quota Limits clean before ask Make sure to clean up your directories before requesting additional storage space. There will be no quota increase for HOME directories. Additional storage for workspaces can be requested by the workspace owner or the deputy, see Workspace Management ATTENTION: there is no Backup or snapshot on HOME directories until the Workspace introduction phase is finished. Previous extended quota keep active until then. \u21a9 \u21a9 \u21a9 Each research group can use up to 10TB of free disk storage in multiple Workspaces free of charge. Quota increase can be purchased, see Workspace Management . \u21a9 Workspaces are meant to be active directories and no archive. Workspace are active by default for one year. The duration can every time be extended to \u201ccurrent date plus one year\u201d. \u21a9","title":"Quota"},{"location":"file-system/quota.html#file-system-quota","text":"","title":"File System Quota"},{"location":"file-system/quota.html#description","text":"This page contains information about quota limits on the parallel file system. Quotas are enabled to control the file system usage. migration change Previously HOME directories had limits of 3 TB per user, file quota limits of 2 million files. In some cases this was extended. The previous HOME quota will kept active until the Workspace introduction phase is finished. Job abortion Jobs will fail if no more disk space can be allocated, or if no more files can be created because the respective quota hard limit is exceeded","title":"Description"},{"location":"file-system/quota.html#quotas","text":"space quota file quota backup expiration HOME 1TB 1 1M 1 yes 1 - WORKSPACE free: up to 10TB per research group 2 1M per TB yes 1 year 3 SCRATCH 50TB 50M no 1 month","title":"Quotas"},{"location":"file-system/quota.html#display-quota-information","text":"A quota tool is delivered with the Workspace module: $ quota UniBE Workspace Quota report ============================ : used ( GB )( % ) , quota ( GB ) | files used ( % ) , quota ================================================================================== HOME : 420 ( 41 % ) , 1024 | 773425 ( 77 % ) , 1000000 Workspace1 : 101 ( 1 % ) , 10240 | 3 ( 0 % ) , 10000000 Furthermore, there is a more detailed version using the -l or --long option $ quota -l UniBE Workspace Quota report ============================ : free quota, used ( GB )( % ) , quota ( GB ) | files used ( % ) , quota | start date ( 1 ) , average quota ( 2 ) ================================================================================================================================ HOME : all, 421 ( 41 % ) , 1024 | 796058 ( 79 % ) , 1000000 | , Workspace1 : 5 , 101 ( 1 % ) , 10240 | 4 ( 0 % ) , 10000000 | 2021 -02-25, 7 .5833 ( 1 ) accounting period start date, The date from which the average usage is computed. ( 2 ) file space average quota ( not files ) , calculated by the average of messured values in the actual accounting period. In the last example the workspace Workspace1 has 5TB of free quota, and a total of 10TB of quota ( 5TB additional storage requested). The start date defines the start of the accounting period and the average quota is computed as average over all datapoints starting from start date . data gathering Workspaces: Workspace quota information is gathered twice a day. Thus the presented data may not completely represent the current state. HOME: values presented are actual values directly from the file system Note: the coloring of the relative values is green (<70%), yellow (70% < x < 90%), red (>90%).","title":"Display quota information"},{"location":"file-system/quota.html#advanced-quota-method","text":"The following mmlsquota command present you actual values from the file system. For $HOME : $ mmlsquota --block-size = G -u $USER rs_gpfs:svc_homefs Block Limits | File Limits Filesystem Fileset type KB quota limit in_doubt grace | files quota limit in_doubt grace Remarks rs_gpfs svc_homefs USR 444181792 1073741824 1073741824 6072144 none | 815985 1000000 1000000 2462 none migration change If your HOME is not yet migrated, you can check your quota on the old file system using: $mmlsquota gpfs . The quota limits for the institute directories can be gathered using: mmlsquota -g id --block-size T , for the institute id . The --block-size option specify the unit {K , M, G, T} in which the numbers of blocks are displayed: mmlsquota --block-size = G -j workspace1 rs_gpfs Block Limits | File Limits Filesystem type GB quota limit in_doubt grace | files quota limit in_doubt grace Remarks rs_gpfs FILESET 57 10240 11264 0 none | 5 10000000 11000000 0 none The output shows the quotas for a worspace called Workspace1 . The quotas are set to a soft limit of 10240 GB, and a hard limit of 11264 GB. 57 GB is currently allocated to the workspace. An in_doubt value greater than zero means that the quota system has not yet been updated as to whether the space that is in doubt is still available or not. If the user/workspace exceeds the soft limit, the grace period will be set to one week. If usage is not reduced to a level below the soft limit during that time, the quota system interprets the soft limit as the hard limit and no further allocation is allowed. The user(s) can reset this condition by reducing usage enough to fall below the soft limit. The maximum amount of disk space the workspace/user can accumulate during the grace period is defined by the hard limit. The same information is also displayed for the file limits (number of files).","title":"advanced quota method"},{"location":"file-system/quota.html#request-higher-quota-limits","text":"clean before ask Make sure to clean up your directories before requesting additional storage space. There will be no quota increase for HOME directories. Additional storage for workspaces can be requested by the workspace owner or the deputy, see Workspace Management ATTENTION: there is no Backup or snapshot on HOME directories until the Workspace introduction phase is finished. Previous extended quota keep active until then. \u21a9 \u21a9 \u21a9 Each research group can use up to 10TB of free disk storage in multiple Workspaces free of charge. Quota increase can be purchased, see Workspace Management . \u21a9 Workspaces are meant to be active directories and no archive. Workspace are active by default for one year. The duration can every time be extended to \u201ccurrent date plus one year\u201d. \u21a9","title":"Request Higher Quota Limits"},{"location":"file-system/scratch.html","text":"Scratch - temporary file space Description Scratch file space are meant for temporary data storage. Interim computational data should be located there. File and size quota is less restrictive. Every user can use up to 20TB and 30M files. There is no snapshot and no backup feature available. Furthermore, an automatic deletion policy is planned, deleting files which are older than 30 days . Scratch file space can be accessed using the Workspace module and the $SCRATCH environment variable. module load Workspace cd $SCRATCH For personal Scratch see below Workspace Scratch Each Workspace has a $SCRATCH space with the same access permissions like the permanent Workspace directory (using primary and secondary groups). The Workspace can be accessed using $SCRATCH variable (after loading the Workspace module). It will point to /storage/scratch/<researchGroupID>/<WorkspaceID> . Please use $SCRATCH to access it. personal Scratch Users without a Workspace can also use \u201cpersonal\u201d Scratch. This space does need to be created initially: module load Workspace/home mkdir $SCRATCH cd $SCRATCH Please note that this space is per default no private space. If you want to restrict access you can change permissions using: chmod 700 $SCRATCH","title":"Scratch"},{"location":"file-system/scratch.html#description","text":"Scratch file space are meant for temporary data storage. Interim computational data should be located there. File and size quota is less restrictive. Every user can use up to 20TB and 30M files. There is no snapshot and no backup feature available. Furthermore, an automatic deletion policy is planned, deleting files which are older than 30 days . Scratch file space can be accessed using the Workspace module and the $SCRATCH environment variable. module load Workspace cd $SCRATCH For personal Scratch see below","title":"Description"},{"location":"file-system/scratch.html#workspace-scratch","text":"Each Workspace has a $SCRATCH space with the same access permissions like the permanent Workspace directory (using primary and secondary groups). The Workspace can be accessed using $SCRATCH variable (after loading the Workspace module). It will point to /storage/scratch/<researchGroupID>/<WorkspaceID> . Please use $SCRATCH to access it.","title":"Workspace Scratch"},{"location":"file-system/scratch.html#personal-scratch","text":"Users without a Workspace can also use \u201cpersonal\u201d Scratch. This space does need to be created initially: module load Workspace/home mkdir $SCRATCH cd $SCRATCH Please note that this space is per default no private space. If you want to restrict access you can change permissions using: chmod 700 $SCRATCH","title":"personal Scratch"},{"location":"general/faq.html","text":"FAQ Description This page provides a collection of frequently asked questions. File system Is my HOME already migrated? You can check using the command pwd . The location in the new file system is: /storage/homefs/$USER , where $USER is your user name. How does the HOMEs migration work? What can I do during the migration? When the migration of your institute starts (T=0), you receive an email. You can still use your home directory then. Ideally, you already cleaned your home directory before that, if not, last chance to cleanup. Anaconda installations need special treatment see this blog entry for a starting point. On the 5th day after the initial mail (T=0+5d), the real migration takes place. You get another email upon starting your individual migration and We lock you out of the cluster. The final synchronization takes place. This phase lasts between several minutes up to a few hours depending on the amount of date you have in your home directory. All running jobs and processes that are running at that time, will be canceled or killed. Therefore please try to only start jobs that may finish before that time. When we finished your home directory, you receive another email and you can log in again and check your home directroy and start to use UBELIX again. Finally a last mail we reach you when the migration for your whole institute has been finished. I read: HOME quota will be 1TB, what now? Previously, HOME quota was mostly 3TB. Now we will shift to group shared spaces, where each research group has 10TB in Workspaces free of charge and personal HOME 1TB. The HOME quota in the new location will be increased temporarily until the Workspaces are established in production (end of May). After the introduction of Workspaces there will be a transition period of one month, where you/your research group manager can create a Workspace and you can migrate the data. Afterwards, the quota will be fixed to 1TB in HOMEs. What if my HOME is full? If you reached your quota, you will get strange warning about not being able to write temporary files etc. You can check your quota using the 1. Decluttering: Check for unnecessary data. This could be: unused application packages, e.g. Python(2) packages in $HOME/.local/lib/python*/site-packages/* temporary computational data, like already post processed output files duplicated data \u2026 Pack and archive: The HPC storage is a high performance parallel storage and not meant to be an archive. Data not used in the short to midterm should be packed and moved to an archive storage. In general, we consider data on our HPC systems as research data. Further we consider research data to be shared sooner or later. And we aim to support and enhance collaborations. Therefore, we introduce group shared spaces, called HPC Workspaces. Ask your research group manager to add you to an existing Workspace or create a new one. There will be no quota increase for HOME directories. Workspaces I need to share data with my colleges. What can I do? HPC Workspaces are meant to host shared data. See HPC Workspaces Where can I get a Workspace? A research group manager need to create the Workspace, since there are possibilities for charged extensions. If you want to join an existing Workspace. Ask the Workspace manager or its deputy to add you. See HPC Workspaces How much does a Workspace cost? Workspaces itself are free of charge. Every research group has 10TB disk space free of charge, which can be used in multiple Workspaces. If necessary, additional storage can be purchased per Workspace, where only the actual usage will be charged, see Workspace Management What if our 10TB free of charge research group quota is full? Your Research group manager or a registered deputy can apply for an additional quota. Actual used quota will be charged. Software issues Why is my private conda installation broken after migration Unfortunately, Anaconda hard wires absolute paths into almost all files (including scripts and binary files). A proper migration process may have included conda pack . There is a way you may access your old environments and create new ones with the same specification: export CONDA_ENVS_PATH=${HOME}/anaconda3/envs ## or where you had your old envs module load Anaconda3 eval \"$(conda shell.bash hook)\" conda info --envs conda activate oldEnvName ## choose your old environment name conda list --explicit > spec-list.txt unset CONDA_ENVS_PATH conda create --name myEnvName --file spec-list.txt # select a name Please, also note that there is a system wide Anaconda installation, so no need for your own separate one. Finally, after recreating your environments please delete all old Anaconda installations and environments. These are not only big but also a ton of files. Environment issues I am using zsh, but some commands and tools fail, what can I do? There are known caveates with LMOD (or module system) and Bash scripts in zsh environments. Bash scripts do not source any system or user files. To initialize the (module) environment properly, you need to set export BASH_ENV=/etc/bashrc in your zsh profile ( .zshrc ). Job issues Why is my job still pending? The REASON column of the squeue output gives you a hint why your job is not running. (Resources) The job is waiting for resources to become available so that the jobs resource request can be fulfilled. (Priority) The job is not allowed to run because at least one higher prioritized job is waiting for resources. (Dependency) The job is waiting for another job to finish first (\u2013dependency=\u2026 option). (DependencyNeverSatisfied) The job is waiting for a dependency that can never be satisfied. Such a job will remain pending forever. Please cancel such jobs. (QOSMaxCpuPerUserLimit) The job is not allowed to start because your currently running jobs consume all allowed CPU resources for your user in a specific partition. Wait for jobs to finish. (AssocGrpCpuLimit) dito. (AssocGrpJobsLimit) The job is not allowed to start because you have reached the maximum of allowed running jobs for your user in a specific partition. Wait for jobs to finish. (ReqNodeNotAvail, UnavailableNodes:\u2026) Some node required by the job is currently not available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN , DRAINED , or not responding. Most probably there is an active reservation for all nodes due to an upcoming maintenance downtime (see output of scontrol show reservation ) and your job is not able to finish before the start of the downtime. Another reason why you should specify the duration of a job (\u2013time) as accurately as possible. Your job will start after the downtime has finished. You can list all active reservations using scontrol show reservation . Why can\u2019t I submit further jobs? sbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user\u2019s size and/or time limits) \u2026 means that you have reached the maximum of allowed jobs to be submitted to a specific partition. Job in state FAILED although job completed successfully Slurm captures the return value of the batch script/last command and reports this value as the completion status of the job/job step. Slurm indicates status FAILED if the value captured is non-zero. The following simplified example illustrates the issue: simple.c #include <unistd.h> #include <stdio.h> int main ( int argc, char *argv []) { char hostname [ 128 ] ; gethostname ( hostname, sizeof ( hostname )) ; printf ( \"%s says: Hello World.\\n\" , hostname ) ; } job.sh #!/bin/bash # Slurm options #SBATCH --mail-user=foo@bar.unibe.ch #SBATCH --mail-type=END #SBATCH --job-name=\"Simple Hello World\" #SBATCH --time=00:05:00 #SBATCH --nodes=1 # Put your code below this line ./simple bash$ sbatch job.sh Submitted batch job 104 Although the job finished successfully\u2026 slurm-104.out knlnode02.ubelix.unibe.ch says: Hello World. \u2026Slurm reports job FAILED: bash$ sacct -j 104 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 104 Simple He+ all 1 FAILED 45 :0 104 .batch batch 1 FAILED 45 :0 Problem: The exit code of the job is the exit status of batch script (job.sh) which in turn returns the exit status of the last command executed (simple) which in turn returns the return value of the last statement (printf()). Since printf() returns the number of characters printed (45), the exit code of the batch script is non-zero and consequently Slurm reports job FAILED although the job produces the desired output. Solution: Explicitly return a value: #include <unistd.h> #include <stdio.h> int main ( int argc, char *argv []) { char hostname [ 128 ] ; int n ; gethostname ( hostname, sizeof ( hostname )) ; // If successful, the total number of characters written is returned. On failure, a negative number is returned. n = printf ( \"%s says: Hello World.\\n\" , hostname ) ; if ( n < 0 ) return 1 ; return 0 ; } bash$ sacct -j 105 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 105 Simple He+ all 1 COMPLETED 0 :0 105 .batch batch 1 COMPLETED 0 :0","title":"FAQ"},{"location":"general/faq.html#faq","text":"","title":"FAQ"},{"location":"general/faq.html#description","text":"This page provides a collection of frequently asked questions.","title":"Description"},{"location":"general/faq.html#file-system","text":"","title":"File system"},{"location":"general/faq.html#is-my-home-already-migrated","text":"You can check using the command pwd . The location in the new file system is: /storage/homefs/$USER , where $USER is your user name.","title":"Is my HOME already migrated?"},{"location":"general/faq.html#how-does-the-homes-migration-work-what-can-i-do-during-the-migration","text":"When the migration of your institute starts (T=0), you receive an email. You can still use your home directory then. Ideally, you already cleaned your home directory before that, if not, last chance to cleanup. Anaconda installations need special treatment see this blog entry for a starting point. On the 5th day after the initial mail (T=0+5d), the real migration takes place. You get another email upon starting your individual migration and We lock you out of the cluster. The final synchronization takes place. This phase lasts between several minutes up to a few hours depending on the amount of date you have in your home directory. All running jobs and processes that are running at that time, will be canceled or killed. Therefore please try to only start jobs that may finish before that time. When we finished your home directory, you receive another email and you can log in again and check your home directroy and start to use UBELIX again. Finally a last mail we reach you when the migration for your whole institute has been finished.","title":"How does the HOMEs migration work? What can I do during the migration?"},{"location":"general/faq.html#i-read-home-quota-will-be-1tb-what-now","text":"Previously, HOME quota was mostly 3TB. Now we will shift to group shared spaces, where each research group has 10TB in Workspaces free of charge and personal HOME 1TB. The HOME quota in the new location will be increased temporarily until the Workspaces are established in production (end of May). After the introduction of Workspaces there will be a transition period of one month, where you/your research group manager can create a Workspace and you can migrate the data. Afterwards, the quota will be fixed to 1TB in HOMEs.","title":"I read: HOME quota will be 1TB, what now?"},{"location":"general/faq.html#what-if-my-home-is-full","text":"If you reached your quota, you will get strange warning about not being able to write temporary files etc. You can check your quota using the 1. Decluttering: Check for unnecessary data. This could be: unused application packages, e.g. Python(2) packages in $HOME/.local/lib/python*/site-packages/* temporary computational data, like already post processed output files duplicated data \u2026 Pack and archive: The HPC storage is a high performance parallel storage and not meant to be an archive. Data not used in the short to midterm should be packed and moved to an archive storage. In general, we consider data on our HPC systems as research data. Further we consider research data to be shared sooner or later. And we aim to support and enhance collaborations. Therefore, we introduce group shared spaces, called HPC Workspaces. Ask your research group manager to add you to an existing Workspace or create a new one. There will be no quota increase for HOME directories.","title":"What if my HOME is full?"},{"location":"general/faq.html#workspaces","text":"","title":"Workspaces"},{"location":"general/faq.html#i-need-to-share-data-with-my-colleges-what-can-i-do","text":"HPC Workspaces are meant to host shared data. See HPC Workspaces","title":"I need to share data with my colleges. What can I do?"},{"location":"general/faq.html#where-can-i-get-a-workspace","text":"A research group manager need to create the Workspace, since there are possibilities for charged extensions. If you want to join an existing Workspace. Ask the Workspace manager or its deputy to add you. See HPC Workspaces","title":"Where can I get a Workspace?"},{"location":"general/faq.html#how-much-does-a-workspace-cost","text":"Workspaces itself are free of charge. Every research group has 10TB disk space free of charge, which can be used in multiple Workspaces. If necessary, additional storage can be purchased per Workspace, where only the actual usage will be charged, see Workspace Management","title":"How much does a Workspace cost?"},{"location":"general/faq.html#what-if-our-10tb-free-of-charge-research-group-quota-is-full","text":"Your Research group manager or a registered deputy can apply for an additional quota. Actual used quota will be charged.","title":"What if our 10TB free of charge research group quota is full?"},{"location":"general/faq.html#software-issues","text":"","title":"Software issues"},{"location":"general/faq.html#why-is-my-private-conda-installation-broken-after-migration","text":"Unfortunately, Anaconda hard wires absolute paths into almost all files (including scripts and binary files). A proper migration process may have included conda pack . There is a way you may access your old environments and create new ones with the same specification: export CONDA_ENVS_PATH=${HOME}/anaconda3/envs ## or where you had your old envs module load Anaconda3 eval \"$(conda shell.bash hook)\" conda info --envs conda activate oldEnvName ## choose your old environment name conda list --explicit > spec-list.txt unset CONDA_ENVS_PATH conda create --name myEnvName --file spec-list.txt # select a name Please, also note that there is a system wide Anaconda installation, so no need for your own separate one. Finally, after recreating your environments please delete all old Anaconda installations and environments. These are not only big but also a ton of files.","title":"Why is my private conda installation broken after migration"},{"location":"general/faq.html#environment-issues","text":"","title":"Environment issues"},{"location":"general/faq.html#i-am-using-zsh-but-some-commands-and-tools-fail-what-can-i-do","text":"There are known caveates with LMOD (or module system) and Bash scripts in zsh environments. Bash scripts do not source any system or user files. To initialize the (module) environment properly, you need to set export BASH_ENV=/etc/bashrc in your zsh profile ( .zshrc ).","title":"I am using zsh, but some commands and tools fail, what can I do?"},{"location":"general/faq.html#job-issues","text":"","title":"Job issues"},{"location":"general/faq.html#why-is-my-job-still-pending","text":"The REASON column of the squeue output gives you a hint why your job is not running. (Resources) The job is waiting for resources to become available so that the jobs resource request can be fulfilled. (Priority) The job is not allowed to run because at least one higher prioritized job is waiting for resources. (Dependency) The job is waiting for another job to finish first (\u2013dependency=\u2026 option). (DependencyNeverSatisfied) The job is waiting for a dependency that can never be satisfied. Such a job will remain pending forever. Please cancel such jobs. (QOSMaxCpuPerUserLimit) The job is not allowed to start because your currently running jobs consume all allowed CPU resources for your user in a specific partition. Wait for jobs to finish. (AssocGrpCpuLimit) dito. (AssocGrpJobsLimit) The job is not allowed to start because you have reached the maximum of allowed running jobs for your user in a specific partition. Wait for jobs to finish. (ReqNodeNotAvail, UnavailableNodes:\u2026) Some node required by the job is currently not available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN , DRAINED , or not responding. Most probably there is an active reservation for all nodes due to an upcoming maintenance downtime (see output of scontrol show reservation ) and your job is not able to finish before the start of the downtime. Another reason why you should specify the duration of a job (\u2013time) as accurately as possible. Your job will start after the downtime has finished. You can list all active reservations using scontrol show reservation .","title":"Why is my job still pending?"},{"location":"general/faq.html#why-cant-i-submit-further-jobs","text":"sbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user\u2019s size and/or time limits) \u2026 means that you have reached the maximum of allowed jobs to be submitted to a specific partition.","title":"Why can't I submit further jobs?"},{"location":"general/faq.html#job-in-state-failed-although-job-completed-successfully","text":"Slurm captures the return value of the batch script/last command and reports this value as the completion status of the job/job step. Slurm indicates status FAILED if the value captured is non-zero. The following simplified example illustrates the issue: simple.c #include <unistd.h> #include <stdio.h> int main ( int argc, char *argv []) { char hostname [ 128 ] ; gethostname ( hostname, sizeof ( hostname )) ; printf ( \"%s says: Hello World.\\n\" , hostname ) ; } job.sh #!/bin/bash # Slurm options #SBATCH --mail-user=foo@bar.unibe.ch #SBATCH --mail-type=END #SBATCH --job-name=\"Simple Hello World\" #SBATCH --time=00:05:00 #SBATCH --nodes=1 # Put your code below this line ./simple bash$ sbatch job.sh Submitted batch job 104 Although the job finished successfully\u2026 slurm-104.out knlnode02.ubelix.unibe.ch says: Hello World. \u2026Slurm reports job FAILED: bash$ sacct -j 104 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 104 Simple He+ all 1 FAILED 45 :0 104 .batch batch 1 FAILED 45 :0 Problem: The exit code of the job is the exit status of batch script (job.sh) which in turn returns the exit status of the last command executed (simple) which in turn returns the return value of the last statement (printf()). Since printf() returns the number of characters printed (45), the exit code of the batch script is non-zero and consequently Slurm reports job FAILED although the job produces the desired output. Solution: Explicitly return a value: #include <unistd.h> #include <stdio.h> int main ( int argc, char *argv []) { char hostname [ 128 ] ; int n ; gethostname ( hostname, sizeof ( hostname )) ; // If successful, the total number of characters written is returned. On failure, a negative number is returned. n = printf ( \"%s says: Hello World.\\n\" , hostname ) ; if ( n < 0 ) return 1 ; return 0 ; } bash$ sacct -j 105 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 105 Simple He+ all 1 COMPLETED 0 :0 105 .batch batch 1 COMPLETED 0 :0","title":"Job in state FAILED although job completed successfully"},{"location":"general/investment.html","text":"Investment UBELIX is financed and maintained by the IT services of the University of Bern. Interested users can invest into GPUs, CPUs and storage usage. The investors get elevated access to specific resources. Furthermore, additional resoures will be purchased with the additional budget. If you are interested, get in touch with us: open a request at the Service Portal . Under Construction This page is still under construction. Details of possible ways to invest will follow.","title":"Investment"},{"location":"general/investment.html#investment","text":"UBELIX is financed and maintained by the IT services of the University of Bern. Interested users can invest into GPUs, CPUs and storage usage. The investors get elevated access to specific resources. Furthermore, additional resoures will be purchased with the additional budget. If you are interested, get in touch with us: open a request at the Service Portal . Under Construction This page is still under construction. Details of possible ways to invest will follow.","title":"Investment"},{"location":"general/news.html","text":"News 04-05-2021: major SLUM partition restructure, see Slurm partitions . Job scripts may need to be adapted. HPC Workspace officially in production HPC Workspace Overview Kernel, CUDA driver, SLURM, and Spectrum Scale update in June: HOME quota fixed to 1TB, removal of GPFS institute shared directories, see Home Migration . Please create Workspaces and migrate data beforehand . 17-02-2021: Home migration: User HOMEs started to get migrated to the newer Spectrum Scale System storage HPC Workspaces: Beta Phase of custom group shared file spaces with tools and Slurm accounting","title":"News"},{"location":"general/news.html#news","text":"04-05-2021: major SLUM partition restructure, see Slurm partitions . Job scripts may need to be adapted. HPC Workspace officially in production HPC Workspace Overview Kernel, CUDA driver, SLURM, and Spectrum Scale update in June: HOME quota fixed to 1TB, removal of GPFS institute shared directories, see Home Migration . Please create Workspaces and migrate data beforehand . 17-02-2021: Home migration: User HOMEs started to get migrated to the newer Spectrum Scale System storage HPC Workspaces: Beta Phase of custom group shared file spaces with tools and Slurm accounting","title":"News"},{"location":"general/support.html","text":"Support UniBE HPC support In case of questions, comments or issues get in touch with us using the Service Portal -> Ask a question -> Tech.Serv.: HPC Documentation If you found issues in this documentation or want to help improving it, open an issue or pull request on out Github repository: https://github.com/hpc-unibe-ch/hpc-docs Investment The HPCs are mainly financed by the UniBE IT services. Furthermore, interested users can buy elevated access to resources. Additional resources are purchased with this additional budget. See Investment Page .","title":"Support"},{"location":"general/support.html#support","text":"","title":"Support"},{"location":"general/support.html#unibe-hpc-support","text":"In case of questions, comments or issues get in touch with us using the Service Portal -> Ask a question -> Tech.Serv.: HPC","title":"UniBE HPC support"},{"location":"general/support.html#documentation","text":"If you found issues in this documentation or want to help improving it, open an issue or pull request on out Github repository: https://github.com/hpc-unibe-ch/hpc-docs","title":"Documentation"},{"location":"general/support.html#investment","text":"The HPCs are mainly financed by the UniBE IT services. Furthermore, interested users can buy elevated access to resources. Additional resources are purchased with this additional budget. See Investment Page .","title":"Investment"},{"location":"getting-Started/account.html","text":"Account and Activation Description UBELIX is available to everybody with a valid Campus Account (CA) of the University of Bern. The cluster is meant to be used for research related to the University of Bern. Before you can use this service your CA need to be activated for UBELIX . Please respect the Code of Conduct On this page you will find useful information regarding the activation of your CA and the login procedure. Furthermore, the provided service structure is outlined. Account activation Request for activation To request the activation of your Campus Account, please send an email to hpc@id.unibe.ch including: a brief description of what you want to use the cluster for your Campus Account username Students must additionally provide: the name of the institute (e.g. Mathematical Institute) if available, the name of the research group (e.g. Numerical Analysis) If you possess multiple Campus Accounts (staff and student) use your staff account since this one is more specific. As soon as we get your email we will activate your account for UBELIX. Once activated, you will receive a confirmation email containing initial instructions You cannot choose a new username for UBELIX. The username/password combination will be the same as for your Campus Account that you also use to access other services provided by the University of Bern (e.g: email, Ilias). Apply for a Campus Account for external coworkers If you do not have a Campus Account of the University of Bern, but you need access to the cluster for your cooperative scientific research with an UniBE institute, the account manager of the institute has to request a Campus Account from the IT department of the University of Bern. Please ask your coworker at the institute to arrange this for you. The responsible account manager at your institute can be found from the following link: Account managers Mailing List The official channel for informing the UBELIX community about upcoming events (e.g. maintenance) and other important news is our mailing list. Sign up to receive information on what\u2019s going on on the cluster: https://listserv.unibe.ch/mailman/listinfo/hpc-users","title":"Accounts and Activation"},{"location":"getting-Started/account.html#account-and-activation","text":"","title":"Account and Activation"},{"location":"getting-Started/account.html#description","text":"UBELIX is available to everybody with a valid Campus Account (CA) of the University of Bern. The cluster is meant to be used for research related to the University of Bern. Before you can use this service your CA need to be activated for UBELIX . Please respect the Code of Conduct On this page you will find useful information regarding the activation of your CA and the login procedure. Furthermore, the provided service structure is outlined.","title":"Description"},{"location":"getting-Started/account.html#account-activation","text":"Request for activation To request the activation of your Campus Account, please send an email to hpc@id.unibe.ch including: a brief description of what you want to use the cluster for your Campus Account username Students must additionally provide: the name of the institute (e.g. Mathematical Institute) if available, the name of the research group (e.g. Numerical Analysis) If you possess multiple Campus Accounts (staff and student) use your staff account since this one is more specific. As soon as we get your email we will activate your account for UBELIX. Once activated, you will receive a confirmation email containing initial instructions You cannot choose a new username for UBELIX. The username/password combination will be the same as for your Campus Account that you also use to access other services provided by the University of Bern (e.g: email, Ilias).","title":"Account activation"},{"location":"getting-Started/account.html#apply-for-a-campus-account-for-external-coworkers","text":"If you do not have a Campus Account of the University of Bern, but you need access to the cluster for your cooperative scientific research with an UniBE institute, the account manager of the institute has to request a Campus Account from the IT department of the University of Bern. Please ask your coworker at the institute to arrange this for you. The responsible account manager at your institute can be found from the following link: Account managers","title":"Apply for a Campus Account for external coworkers"},{"location":"getting-Started/account.html#mailing-list","text":"The official channel for informing the UBELIX community about upcoming events (e.g. maintenance) and other important news is our mailing list. Sign up to receive information on what\u2019s going on on the cluster: https://listserv.unibe.ch/mailman/listinfo/hpc-users","title":"Mailing List"},{"location":"getting-Started/login-ssh.html","text":"Login Description UBELIX is available to everybody with a valid Campus Account (CA) of the University of Bern. The cluster is meant to be used for research related to the University of Bern. Before you can use this service we have to activate your CA for UBELIX , see Accounts and Activation . This page contains information on how to configure your SSH environment for a simplified login procedure and information regarding the application of a CA for external researchers. Log in to UBELIX Before proceeding make sure that: you have your Campus Account activated for UBELIX (see above) you have a working SSH client Linux/Mac: e.g ssh command in a terminal Microsoft Windows: you can use PuTTY, MobaXterm, or the Linux subsystem. Alternatively, a flavor of Linux can be installed on Microsoft Windows using virtualization software (e.g VirtualBox). We strongly encourage you to familiarize with a Unix-based operating system. Requirement Login to UBELIX is only possible from within the UniBE network. If you want to connect from outside, you must first establish a VPN connection. For VPN profiles and instructions see the official tutorial . Mac/Linux/Unix Run the following commands in a terminal. Open an SSH connection to the submit host: $ ssh <username>@submit.unibe.ch OR $ ssh -l <username> submit.unibe.ch At the password prompt enter your Campus Account password: $ ssh <username>@submit.unibe.ch Password: Usually there is no indication of typing when entering your password (not even asterisks or bullets). That\u2019s intended. Just enter your password and press \u2018enter\u2019. After log in successfully you will see the welcome message and the command prompt: Last login: Tue Apr 21 16 :17:26 2020 CentOS 7 .7.1908.x86_64 FQDN: submit01.ubelix.unibe.ch ( 10 .1.129.21 ) Processor: 24x Intel ( R ) Xeon ( R ) CPU E5-2630 v2 @ 2 .60GHz Kernel: 3 .10.0-1062.9.1.el7.x86_64 Memory: 62 .73 GiB Congratulations, you just logged in to the cluster! You can immediately start using UBELIX. Microsoft Windows We use PuTTY to illustrate how to establish a SSH connection from a Windows client to UBELIX. There are of course other SSH clients for Windows available that serve the same purpose. Download You can download PuTTY from http://www.putty.org In category \u201cSession\u201d specify: Connection type: SSH Host name (or IP address): submit.unibe.ch Port: 22 In category \u201cConnection\u201d/\u201dData\u201d specify: Auto-login username: (Enter your username here) To save your session, in category \u201cSession\u201d specify: Saved Session: ubelix (you can choose your own session name) and click the \u201cSave\u201d button Click \u201cOpen\u201d to establish a new connection. At the password prompt enter your password: Usually there is no indication of typing when entering your password (not even asterisks or bullets). That\u2019s intended. Just enter your password and press \u2018enter\u2019. After log in successfully you will see the welcome message and the command prompt: Congratulations, you just logged in to the cluster! You can immediately start using UBELIX. Customize your SSH session Useful feartures like SSH alias, X and port forwarding are described on our page SSH customization . Create a SSH alias Mac/Linux/Unix To simplify the login procedure you can define an alias for the user-/hostname combination. Add a host declaration to ~/.ssh/config (substitute your own alias and username): ~/.ssh/config Host <alias> Hostname submit.unibe.ch User <username> From now on you can log in to the cluster by using the specified alias: $ ssh <alias> You still have to provide your password! SSH session timeout Mac/Linux/Unix If a SSH connection goes idle for a specific amount of time (default 10 minutes), you may be confronted with a \u201cWrite failed: Broken pipe\u201d error message or the connection is simply frozen, and you are forced to log in again. To prevent this from happening, configure the client to periodically (e.g. every 60 seconds) send a message to trigger a response from the remote server. To do so, add the following line to the SSH configuration file: ServerAliveInterval 60 The host declaration may now look like this: ~/.ssh/config Host <alias> Hostname submit.unibe.ch User <username> ServerAliveInterval 60 SSH key pairs Mac/Linux/Unix SSH keys serve as a means of identifying a user to a SSH server. When using SSH keys your password will never be send over the network. Remember to always keep your private key private! Only share your public key, never share your private key. If you already have a valid private/public key pair that you also want to use for UBELIX, you can omit the rest of this section and continue with \u201cAdding a public key to your UBELIX account\u201d. First, generate a private/public key pair. You can substitute your own comment (-C). To accept the default name/location simply press Enter, otherwise specify a different name/location: $ ssh-keygen -t rsa -b 4096 -C \"ubelix\" Generating public/private rsa key pair. Enter file in which to save the key ( /Users/faerber/.ssh/id_rsa ) : Enter and confirm a secure passphrase: If you do not specify a passphrase and someone else gets a copy of your private key, then he will be able to login with your identity on any account that uses the corresponding public key! Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Adding a public key to your UBELIX account If you have specified a custom name/location for your SSH keys, you can tell your SSH client to use this key for connecting to UBELIX by specifying the private key on the command line: $ ssh -i ~/.ssh/id_rsa_ubelix <alias> or even better, add the key to your host declaration in your ssh configuration: ~/.ssh/config Host <alias> Hostname submit.unibe.ch User <username> ServerAliveInterval 60 IdentityFile ~/.ssh/id_rsa_ubelix Now, login to UBELIX and append your public key (content of id_rsa.pub) to the file ~/.ssh/authorized_keys. This step can also be done by simply issuing ssh-copy-id -i ~/.ssh/id_rsa_ubelix.pub . If everything was correct, you will now be able to login without providing you Campus Account password upon your next login attempt. However, if you have secured your key with a passphrase, you will get prompted for your passphrase instead. You can use ssh-agent to securely save your passphrase, so you do not have to re-enter it all the time. Adding your Key to SSH-Agent The behavior of ssh-agent depends on the flavor and version of your operating system. On OS X Leopard or later your keys can be saved in the system\u2019s keychain. Most Linux installations will automatically start ssh-agent when you log in. Add the key to ssh-agent: $ ssh-add ~/.ssh/id_rsa_ubelix Passwordless ssh within the HPCs if you want to ssh passwordless to other nodes within the HPCs or want to use services like JupyterLab, you need to generate and register a new key within the HPC. Thus run the following commands on e.g. the submit node: ssh-keygen -t rsa -b 4096 cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys chmod 640 .ssh/authorized_keys","title":"Accessing the HPCs"},{"location":"getting-Started/login-ssh.html#login","text":"","title":"Login"},{"location":"getting-Started/login-ssh.html#description","text":"UBELIX is available to everybody with a valid Campus Account (CA) of the University of Bern. The cluster is meant to be used for research related to the University of Bern. Before you can use this service we have to activate your CA for UBELIX , see Accounts and Activation . This page contains information on how to configure your SSH environment for a simplified login procedure and information regarding the application of a CA for external researchers.","title":"Description"},{"location":"getting-Started/login-ssh.html#log-in-to-ubelix","text":"Before proceeding make sure that: you have your Campus Account activated for UBELIX (see above) you have a working SSH client Linux/Mac: e.g ssh command in a terminal Microsoft Windows: you can use PuTTY, MobaXterm, or the Linux subsystem. Alternatively, a flavor of Linux can be installed on Microsoft Windows using virtualization software (e.g VirtualBox). We strongly encourage you to familiarize with a Unix-based operating system. Requirement Login to UBELIX is only possible from within the UniBE network. If you want to connect from outside, you must first establish a VPN connection. For VPN profiles and instructions see the official tutorial .","title":"Log in to UBELIX"},{"location":"getting-Started/login-ssh.html#maclinuxunix","text":"Run the following commands in a terminal. Open an SSH connection to the submit host: $ ssh <username>@submit.unibe.ch OR $ ssh -l <username> submit.unibe.ch At the password prompt enter your Campus Account password: $ ssh <username>@submit.unibe.ch Password: Usually there is no indication of typing when entering your password (not even asterisks or bullets). That\u2019s intended. Just enter your password and press \u2018enter\u2019. After log in successfully you will see the welcome message and the command prompt: Last login: Tue Apr 21 16 :17:26 2020 CentOS 7 .7.1908.x86_64 FQDN: submit01.ubelix.unibe.ch ( 10 .1.129.21 ) Processor: 24x Intel ( R ) Xeon ( R ) CPU E5-2630 v2 @ 2 .60GHz Kernel: 3 .10.0-1062.9.1.el7.x86_64 Memory: 62 .73 GiB Congratulations, you just logged in to the cluster! You can immediately start using UBELIX.","title":"Mac/Linux/Unix"},{"location":"getting-Started/login-ssh.html#microsoft-windows","text":"We use PuTTY to illustrate how to establish a SSH connection from a Windows client to UBELIX. There are of course other SSH clients for Windows available that serve the same purpose. Download You can download PuTTY from http://www.putty.org In category \u201cSession\u201d specify: Connection type: SSH Host name (or IP address): submit.unibe.ch Port: 22 In category \u201cConnection\u201d/\u201dData\u201d specify: Auto-login username: (Enter your username here) To save your session, in category \u201cSession\u201d specify: Saved Session: ubelix (you can choose your own session name) and click the \u201cSave\u201d button Click \u201cOpen\u201d to establish a new connection. At the password prompt enter your password: Usually there is no indication of typing when entering your password (not even asterisks or bullets). That\u2019s intended. Just enter your password and press \u2018enter\u2019. After log in successfully you will see the welcome message and the command prompt: Congratulations, you just logged in to the cluster! You can immediately start using UBELIX.","title":"Microsoft Windows"},{"location":"getting-Started/login-ssh.html#customize-your-ssh-session","text":"Useful feartures like SSH alias, X and port forwarding are described on our page SSH customization .","title":"Customize your SSH session"},{"location":"getting-Started/login-ssh.html#create-a-ssh-alias","text":"Mac/Linux/Unix To simplify the login procedure you can define an alias for the user-/hostname combination. Add a host declaration to ~/.ssh/config (substitute your own alias and username): ~/.ssh/config Host <alias> Hostname submit.unibe.ch User <username> From now on you can log in to the cluster by using the specified alias: $ ssh <alias> You still have to provide your password!","title":"Create a SSH alias"},{"location":"getting-Started/login-ssh.html#ssh-session-timeout","text":"Mac/Linux/Unix If a SSH connection goes idle for a specific amount of time (default 10 minutes), you may be confronted with a \u201cWrite failed: Broken pipe\u201d error message or the connection is simply frozen, and you are forced to log in again. To prevent this from happening, configure the client to periodically (e.g. every 60 seconds) send a message to trigger a response from the remote server. To do so, add the following line to the SSH configuration file: ServerAliveInterval 60 The host declaration may now look like this: ~/.ssh/config Host <alias> Hostname submit.unibe.ch User <username> ServerAliveInterval 60","title":"SSH session timeout"},{"location":"getting-Started/login-ssh.html#ssh-key-pairs","text":"Mac/Linux/Unix SSH keys serve as a means of identifying a user to a SSH server. When using SSH keys your password will never be send over the network. Remember to always keep your private key private! Only share your public key, never share your private key. If you already have a valid private/public key pair that you also want to use for UBELIX, you can omit the rest of this section and continue with \u201cAdding a public key to your UBELIX account\u201d. First, generate a private/public key pair. You can substitute your own comment (-C). To accept the default name/location simply press Enter, otherwise specify a different name/location: $ ssh-keygen -t rsa -b 4096 -C \"ubelix\" Generating public/private rsa key pair. Enter file in which to save the key ( /Users/faerber/.ssh/id_rsa ) : Enter and confirm a secure passphrase: If you do not specify a passphrase and someone else gets a copy of your private key, then he will be able to login with your identity on any account that uses the corresponding public key! Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Adding a public key to your UBELIX account If you have specified a custom name/location for your SSH keys, you can tell your SSH client to use this key for connecting to UBELIX by specifying the private key on the command line: $ ssh -i ~/.ssh/id_rsa_ubelix <alias> or even better, add the key to your host declaration in your ssh configuration: ~/.ssh/config Host <alias> Hostname submit.unibe.ch User <username> ServerAliveInterval 60 IdentityFile ~/.ssh/id_rsa_ubelix Now, login to UBELIX and append your public key (content of id_rsa.pub) to the file ~/.ssh/authorized_keys. This step can also be done by simply issuing ssh-copy-id -i ~/.ssh/id_rsa_ubelix.pub . If everything was correct, you will now be able to login without providing you Campus Account password upon your next login attempt. However, if you have secured your key with a passphrase, you will get prompted for your passphrase instead. You can use ssh-agent to securely save your passphrase, so you do not have to re-enter it all the time. Adding your Key to SSH-Agent The behavior of ssh-agent depends on the flavor and version of your operating system. On OS X Leopard or later your keys can be saved in the system\u2019s keychain. Most Linux installations will automatically start ssh-agent when you log in. Add the key to ssh-agent: $ ssh-add ~/.ssh/id_rsa_ubelix Passwordless ssh within the HPCs if you want to ssh passwordless to other nodes within the HPCs or want to use services like JupyterLab, you need to generate and register a new key within the HPC. Thus run the following commands on e.g. the submit node: ssh-keygen -t rsa -b 4096 cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys chmod 640 .ssh/authorized_keys","title":"SSH key pairs"},{"location":"getting-Started/ssh-customization.html","text":"Customize your SSH environment Description This page is listing useful tricks and features with SSH connections. Create a SSH alias Mac/Linux/Unix To simplify the login procedure you can define an alias for the user-/hostname combination. Add a host declaration to ~/.ssh/config (substitute your own alias and username): ~/.ssh/config Host <alias> Hostname submit.unibe.ch User <username> From now on you can log in to the cluster by using the specified alias: $ ssh <alias> You still have to provide your password! SSH session timeout Mac/Linux/Unix If a SSH connection goes idle for a specific amount of time (default 10 minutes), you may be confronted with a \u201cWrite failed: Broken pipe\u201d error message or the connection is simply frozen, and you are forced to log in again. To prevent this from happening, configure the client to periodically (e.g. every 60 seconds) send a message to trigger a response from the remote server. To do so, add the following line to the SSH configuration file: ServerAliveInterval 60 The host declaration may now look like this: ~/.ssh/config Host <alias> Hostname submit.unibe.ch User <username> ServerAliveInterval 60 SSH key pairs Mac/Linux/Unix SSH keys serve as a means of identifying a user to a SSH server. When using SSH keys your password will never be send over the network. Remember to always keep your private keys private! Share only public keys, never share your private key. If you already have a valid private/public key pair that you also want to use for UBELIX, you can omit the rest of this section and continue with \u201cAdding a public key to your UBELIX account\u201d. First, generate a private/public key pair. You can substitute your own comment (-C). To accept the default name/location simply press Enter, otherwise specify a different name/location: $ ssh-keygen -t rsa -b 4096 -C \"ubelix\" Generating public/private rsa key pair. Enter file in which to save the key ( /Users/faerber/.ssh/id_rsa ) : Enter and confirm a secure passphrase: If you do not specify a passphrase and someone else gets a copy of your private key, then he will be able to login with your identity on any account that uses the corresponding public key! Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Adding a public key to your UBELIX account If you have specified a custom name/location for your SSH keys, you can tell your SSH client to use this key for connecting to UBELIX by specifying the private key on the command line: $ ssh -i ~/.ssh/id_rsa_ubelix <alias> or even better, add the key to your host declaration in your ssh configuration: ~/.ssh/config Host <alias> Hostname submit.unibe.ch User <username> ServerAliveInterval 60 IdentityFile ~/.ssh/id_rsa_ubelix Now, login to UBELIX and append your public key (content of id_rsa.pub) to the file ~/.ssh/authorized_keys. This step can also be done by simply issuing ssh-copy-id -i ~/.ssh/id_rsa_ubelix.pub <alias> . If everything was correct, you will now be able to login without providing you Campus Account password upon your next login attempt. However, if you have secured your key with a passphrase, you will get prompted for your passphrase instead. You can use ssh-agent to securely save your passphrase, so you do not have to re-enter it all the time. Adding your Key to SSH-Agent The behavior of ssh-agent depends on the flavor and version of your operating system. On OS X Leopard or later your keys can be saved in the system\u2019s keychain. Most Linux installations will automatically start ssh-agent when you log in. Add the key to ssh-agent: $ ssh-add ~/.ssh/id_rsa_ubelix X11 - forwarding For applications with graphical interfaces X11-forwarding is sometimes necessary. You can enable X11-forwarding by using -Y option during your login process: ssh -Y <alias> The success can be tested e.g. by calling xterm on the login node, which should open a new window. Keep in mind your local operating system need to have a X server running. E.g. Xming on Windows or XQuartz for Mac. Port forwarding Some application like JupyterLab require port forwarding, where a port on the remote machine gets connected with a port on the local machine. The ssh command need to be called with additional arguments: ssh -Y -L 15051 :localhost:15051 submit.unibe.ch Here port 15051 is selected for both sides. Ports are numbers between 2000 and 65000, which needs to be unique on the present machine. The default port for JupyterLab is 8888, but only one user can use this port on the machine at a time. To avoid the need for modifying your workflow again and again, we suggest to (once) select a unique number (between 2000 and 65000), which hopfully and most likely will not be used by another user.","title":"SSH customization"},{"location":"getting-Started/ssh-customization.html#customize-your-ssh-environment","text":"","title":"Customize your SSH environment"},{"location":"getting-Started/ssh-customization.html#description","text":"This page is listing useful tricks and features with SSH connections.","title":"Description"},{"location":"getting-Started/ssh-customization.html#create-a-ssh-alias","text":"Mac/Linux/Unix To simplify the login procedure you can define an alias for the user-/hostname combination. Add a host declaration to ~/.ssh/config (substitute your own alias and username): ~/.ssh/config Host <alias> Hostname submit.unibe.ch User <username> From now on you can log in to the cluster by using the specified alias: $ ssh <alias> You still have to provide your password!","title":"Create a SSH alias"},{"location":"getting-Started/ssh-customization.html#ssh-session-timeout","text":"Mac/Linux/Unix If a SSH connection goes idle for a specific amount of time (default 10 minutes), you may be confronted with a \u201cWrite failed: Broken pipe\u201d error message or the connection is simply frozen, and you are forced to log in again. To prevent this from happening, configure the client to periodically (e.g. every 60 seconds) send a message to trigger a response from the remote server. To do so, add the following line to the SSH configuration file: ServerAliveInterval 60 The host declaration may now look like this: ~/.ssh/config Host <alias> Hostname submit.unibe.ch User <username> ServerAliveInterval 60","title":"SSH session timeout"},{"location":"getting-Started/ssh-customization.html#ssh-key-pairs","text":"Mac/Linux/Unix SSH keys serve as a means of identifying a user to a SSH server. When using SSH keys your password will never be send over the network. Remember to always keep your private keys private! Share only public keys, never share your private key. If you already have a valid private/public key pair that you also want to use for UBELIX, you can omit the rest of this section and continue with \u201cAdding a public key to your UBELIX account\u201d. First, generate a private/public key pair. You can substitute your own comment (-C). To accept the default name/location simply press Enter, otherwise specify a different name/location: $ ssh-keygen -t rsa -b 4096 -C \"ubelix\" Generating public/private rsa key pair. Enter file in which to save the key ( /Users/faerber/.ssh/id_rsa ) : Enter and confirm a secure passphrase: If you do not specify a passphrase and someone else gets a copy of your private key, then he will be able to login with your identity on any account that uses the corresponding public key! Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Adding a public key to your UBELIX account If you have specified a custom name/location for your SSH keys, you can tell your SSH client to use this key for connecting to UBELIX by specifying the private key on the command line: $ ssh -i ~/.ssh/id_rsa_ubelix <alias> or even better, add the key to your host declaration in your ssh configuration: ~/.ssh/config Host <alias> Hostname submit.unibe.ch User <username> ServerAliveInterval 60 IdentityFile ~/.ssh/id_rsa_ubelix Now, login to UBELIX and append your public key (content of id_rsa.pub) to the file ~/.ssh/authorized_keys. This step can also be done by simply issuing ssh-copy-id -i ~/.ssh/id_rsa_ubelix.pub <alias> . If everything was correct, you will now be able to login without providing you Campus Account password upon your next login attempt. However, if you have secured your key with a passphrase, you will get prompted for your passphrase instead. You can use ssh-agent to securely save your passphrase, so you do not have to re-enter it all the time. Adding your Key to SSH-Agent The behavior of ssh-agent depends on the flavor and version of your operating system. On OS X Leopard or later your keys can be saved in the system\u2019s keychain. Most Linux installations will automatically start ssh-agent when you log in. Add the key to ssh-agent: $ ssh-add ~/.ssh/id_rsa_ubelix","title":"SSH key pairs"},{"location":"getting-Started/ssh-customization.html#x11-forwarding","text":"For applications with graphical interfaces X11-forwarding is sometimes necessary. You can enable X11-forwarding by using -Y option during your login process: ssh -Y <alias> The success can be tested e.g. by calling xterm on the login node, which should open a new window. Keep in mind your local operating system need to have a X server running. E.g. Xming on Windows or XQuartz for Mac.","title":"X11 - forwarding"},{"location":"getting-Started/ssh-customization.html#port-forwarding","text":"Some application like JupyterLab require port forwarding, where a port on the remote machine gets connected with a port on the local machine. The ssh command need to be called with additional arguments: ssh -Y -L 15051 :localhost:15051 submit.unibe.ch Here port 15051 is selected for both sides. Ports are numbers between 2000 and 65000, which needs to be unique on the present machine. The default port for JupyterLab is 8888, but only one user can use this port on the machine at a time. To avoid the need for modifying your workflow again and again, we suggest to (once) select a unique number (between 2000 and 65000), which hopfully and most likely will not be used by another user.","title":"Port forwarding"},{"location":"getting-Started/ubelix-overview.html","text":"UBELIX - Overview Description This page provides a high-level system overview of a HPC cluster such as UBELIX. It describes the different hardware components that constitute the cluster and gives a quantitative list of the different generations of compute nodes in UBELIX. UBELIX (University of Bern Linux Cluster) is a HPC cluster that currently consists of about 340 compute nodes featuring almost 11\u2018500 CPU cores and 136 GPUs and a software-defined storage infrastructure providing ~3 PB of disk storage net. UBELIX is a heterogeneous cluster, meaning UBELIX consists of different generations of compute nodes with different instruction sets. Compute nodes, front-end servers and the storage are interconnected through a high speed Infiniband network. The front-end servers also provide a link to the outside world. UBELIX is used by various institutes and research groups within chemistry, biology, physics, astronomy, computer science, geography, medical radiology and others for scientific research and by students working on their thesis. High-level system overview The HPCs can only be reached within the UniBE network. User landing point are the login nodes, where jobs can be prepared and submitted. Computational tasks are scheduled and managed on the compute nodes using SLURM. All compute nodes as well as the login nodes have access to the parallel file system. Login node aka. Submit node A user connects to the cluster by logging into the submit host via SSH. You can use this host for medium-performance tasks, e.g. to edit files or to compile smaller programs. Resource-demanding/high-performance tasks must be submitted to the batch queuing system as jobs, and will finally run on one or multiple compute nodes. Even long running compile tasks should be submitted as a job on a compute node instead of running it on the submit host. SLURM Batch-Queueing System On UBELIX we use the open-source batch-queueing system Slurm , managing all jobs on the compute nodes. The job submission is described in detail in the Job handling section, starting with Submitting jobs . The procedure look like: resource definition : resources required for your job need to be defined, including numbers of CPU cores , time limit, memory , partitions, QOS, etc.. These resources can be defined in the batch script or as command line arguments. Not explicitly specified parameters are chosen with default values . submitting : job can be submitted using sbatch (using a batch script), srun (directly running the executable), or salloc (interactive submission). The submission is checked from SLURM if it is within the specification and limits. scheduling : Slurm is finding the optimal spots for the registered jobs on the resources and time. This also includes priority handling and optimizing for best coverage. launch : Slurm prepares the environment on the selected compute resources. This also includes setting up the MPI environment, if requested interactive sessions, etc., and launching your batch script. serial/parallel tasks : per default all the tasks defined in your batch script are run on the first core of your allocation. Compute tasks should be started with \u2018srun\u2019. Parallel task are launched on all (or as defined) job related resources. cancelling/completing : When tasks finished, wall time limit or memory limit is reached the job ant its environment gets removed from the resources. All output is written into file(s) (except of interactive sessions) Cluster Partitions (Queues) and their Compute Nodes UBELIX is a heterogeneous machine, consisting of different architectures. There are CPU compute nodes with: Intel Ivybridge Intel Broadwell AMD Epyc2 and GPU nodes with: Nvidia Geforce GTX 1080 Ti Nvidia Geforce RTX 2080 Ti Nvidia Geforce RTX 3090 Nvidia Tesla P100 Without explicit changes, jobs are scheduled in the AMD Epyc2 partition, running up to 3 days. Partitions group nodes into logical sets, which share the same limits. Furthermore, specific limits and privileges are managed using Quality Of Service (QOS), like high priorities or node limitations for long running jobs. Different partitions and QOS are listed in the SLURM Partition/QOS article Storage Infrastructure A modular, software-defined storage system (IBM Spectrum Scale) provides a shared, parallel file system that is mounted on all frontend servers and compute nodes. For more information see storage infrastructure and File System Quota .","title":"UniBE HPC system \"UBELIX\""},{"location":"getting-Started/ubelix-overview.html#ubelix-overview","text":"","title":"UBELIX - Overview"},{"location":"getting-Started/ubelix-overview.html#description","text":"This page provides a high-level system overview of a HPC cluster such as UBELIX. It describes the different hardware components that constitute the cluster and gives a quantitative list of the different generations of compute nodes in UBELIX. UBELIX (University of Bern Linux Cluster) is a HPC cluster that currently consists of about 340 compute nodes featuring almost 11\u2018500 CPU cores and 136 GPUs and a software-defined storage infrastructure providing ~3 PB of disk storage net. UBELIX is a heterogeneous cluster, meaning UBELIX consists of different generations of compute nodes with different instruction sets. Compute nodes, front-end servers and the storage are interconnected through a high speed Infiniband network. The front-end servers also provide a link to the outside world. UBELIX is used by various institutes and research groups within chemistry, biology, physics, astronomy, computer science, geography, medical radiology and others for scientific research and by students working on their thesis.","title":"Description"},{"location":"getting-Started/ubelix-overview.html#high-level-system-overview","text":"The HPCs can only be reached within the UniBE network. User landing point are the login nodes, where jobs can be prepared and submitted. Computational tasks are scheduled and managed on the compute nodes using SLURM. All compute nodes as well as the login nodes have access to the parallel file system.","title":"High-level system overview"},{"location":"getting-Started/ubelix-overview.html#login-node-aka-submit-node","text":"A user connects to the cluster by logging into the submit host via SSH. You can use this host for medium-performance tasks, e.g. to edit files or to compile smaller programs. Resource-demanding/high-performance tasks must be submitted to the batch queuing system as jobs, and will finally run on one or multiple compute nodes. Even long running compile tasks should be submitted as a job on a compute node instead of running it on the submit host.","title":"Login node aka. Submit node"},{"location":"getting-Started/ubelix-overview.html#slurm-batch-queueing-system","text":"On UBELIX we use the open-source batch-queueing system Slurm , managing all jobs on the compute nodes. The job submission is described in detail in the Job handling section, starting with Submitting jobs . The procedure look like: resource definition : resources required for your job need to be defined, including numbers of CPU cores , time limit, memory , partitions, QOS, etc.. These resources can be defined in the batch script or as command line arguments. Not explicitly specified parameters are chosen with default values . submitting : job can be submitted using sbatch (using a batch script), srun (directly running the executable), or salloc (interactive submission). The submission is checked from SLURM if it is within the specification and limits. scheduling : Slurm is finding the optimal spots for the registered jobs on the resources and time. This also includes priority handling and optimizing for best coverage. launch : Slurm prepares the environment on the selected compute resources. This also includes setting up the MPI environment, if requested interactive sessions, etc., and launching your batch script. serial/parallel tasks : per default all the tasks defined in your batch script are run on the first core of your allocation. Compute tasks should be started with \u2018srun\u2019. Parallel task are launched on all (or as defined) job related resources. cancelling/completing : When tasks finished, wall time limit or memory limit is reached the job ant its environment gets removed from the resources. All output is written into file(s) (except of interactive sessions)","title":"SLURM Batch-Queueing System"},{"location":"getting-Started/ubelix-overview.html#cluster-partitions-queues-and-their-compute-nodes","text":"UBELIX is a heterogeneous machine, consisting of different architectures. There are CPU compute nodes with: Intel Ivybridge Intel Broadwell AMD Epyc2 and GPU nodes with: Nvidia Geforce GTX 1080 Ti Nvidia Geforce RTX 2080 Ti Nvidia Geforce RTX 3090 Nvidia Tesla P100 Without explicit changes, jobs are scheduled in the AMD Epyc2 partition, running up to 3 days. Partitions group nodes into logical sets, which share the same limits. Furthermore, specific limits and privileges are managed using Quality Of Service (QOS), like high priorities or node limitations for long running jobs. Different partitions and QOS are listed in the SLURM Partition/QOS article","title":"Cluster Partitions (Queues) and their Compute Nodes"},{"location":"getting-Started/ubelix-overview.html#storage-infrastructure","text":"A modular, software-defined storage system (IBM Spectrum Scale) provides a shared, parallel file system that is mounted on all frontend servers and compute nodes. For more information see storage infrastructure and File System Quota .","title":"Storage Infrastructure"},{"location":"getting-Started/workspaces.html","text":"HPC Workspaces under Construction Workspaces are still in testing phase and not publicly available yet. detailed information will follow soon Description The HPC Workspaces provide a group shared environment, including storage with user-defined access, SLURM accounting, and tools. A HPC Workspace belong to a research group and need to be requested by the research group leader, see Workspace management . Short Summary Workspaces provide a collaborative environment with user defined access groups: a primary group with read/write access and a secondary group with read only access Each Workspace provide: permanent storage ( /storage/workspaces/<researchGroupID>/<workspaceID> ) temporary storage ( /storage/scratch/<researchGroupID>/<workspaceID> ) user-friendly access to a custom software repositories and monitoring tools and SLURM accounting to that Workspace. Fair share between research groups. Usage Please always load the Workspace module when using it, even if you only copy files into it. The module provides you with shortcuts (e.g. $WORKSPACE ), the custom Software stack (if existing) and SLURM settings. Application Workspaces need to be created by registered research group lead/managers, see Workspace Application More Details For more details see: Workspace Introduction , Workspace Software Environment , and Workspace Monitoring","title":"Workspaces"},{"location":"getting-Started/workspaces.html#hpc-workspaces","text":"under Construction Workspaces are still in testing phase and not publicly available yet. detailed information will follow soon","title":"HPC Workspaces"},{"location":"getting-Started/workspaces.html#description","text":"The HPC Workspaces provide a group shared environment, including storage with user-defined access, SLURM accounting, and tools. A HPC Workspace belong to a research group and need to be requested by the research group leader, see Workspace management .","title":"Description"},{"location":"getting-Started/workspaces.html#short-summary","text":"Workspaces provide a collaborative environment with user defined access groups: a primary group with read/write access and a secondary group with read only access Each Workspace provide: permanent storage ( /storage/workspaces/<researchGroupID>/<workspaceID> ) temporary storage ( /storage/scratch/<researchGroupID>/<workspaceID> ) user-friendly access to a custom software repositories and monitoring tools and SLURM accounting to that Workspace. Fair share between research groups.","title":"Short Summary"},{"location":"getting-Started/workspaces.html#usage","text":"Please always load the Workspace module when using it, even if you only copy files into it. The module provides you with shortcuts (e.g. $WORKSPACE ), the custom Software stack (if existing) and SLURM settings.","title":"Usage"},{"location":"getting-Started/workspaces.html#application","text":"Workspaces need to be created by registered research group lead/managers, see Workspace Application","title":"Application"},{"location":"getting-Started/workspaces.html#more-details","text":"For more details see: Workspace Introduction , Workspace Software Environment , and Workspace Monitoring","title":"More Details"},{"location":"hpc-workspaces/environment.html","text":"HPC Workspace Data and Software Tools Description HPC Workspace module provides support for user-friendly file system access, custom software stacks in HPC Workspaces, and SLURM accounting. The module can also be used to set up HOME for a custom software stack. Workspace module The Workspace module adjust the environment to work in a specific HPC Workspace. module load Workspace sets the following environment variables ( Shortcuts ) and Software stacks There are the following possibilities: you belong to no Workspace: load module load Workspace/home to use your software stack in your HOME directory you belong to one Workspace: this Workspace gets loaded when module load Workspace you belong to multiple Workspaces: you need to specify the Workspace to load using the variable $HPC_WORKSPACE . The module presents the possible options, e.g.: $ module load Workspace Workspaces are available: HPC_SW_test, hpc_training, Please select and load ONE of the following: export HPC_WORKSPACE=HPC_SW_test; module load Workspace export HPC_WORKSPACE=hpc_training; module load Workspace Thus you load a specific Workspace using: export HPC_WORKSPACE=<WorkspaceName>; module load Workspace Shortcuts The workspace module provides the following variables: Variable Function $WORKSPACE full path to the Workspace. Thus, you can access the workspace using: cd $WORKSPACE $SCRATCH full path to the Workspace SCRATCH directory. Thus you can access it using: cd $SCRATCH Additional Settings the module provides the following settings for a more user-friendly usage of applications. You may not need to use them directly, but tools like SLURM, Singularity, Python, and R will use them. Variable Function $SBATCH_ACCOUNT sets the SLURM account to the Workspace account. Thus all submitted jobs with that module are accounted to the Workspace account automatically. No need to set it in the sbatch script $SINGULARITY_BINDPATH using singularity, the Workspace directory will be bind into the container without manual specification. The WORKSPACE variable as well as the SCRATCH variable will also be ported into the container. Thus, you can specify locations with $WORKSPACE or $SCRATCH within the container. $PYTHONPATH if Python or Anaconda is loaded beforehand, it is set to: $WORKSPACE/PyPackages/lib/pythonXXX/site-packages where XXX is the Python major and minor version. And also add the bin directory to $PATH . $PYTHONPACKAGEPATH if Python or Anaconda is loaded beforehand, it is set to: $WORKSPACE/PyPackages . This can be used for e.g. pip install --prefix $PYTHONPACKAGEPATH $CONDA_ENVS_PATH therewith conda environments can be created and shared within the Workspace $R_LIBS therewith additional R packages can be installed and searched in the shared Workspace. The directory need to be created first. See R page Software stacks Beside, a set of software packages we provide for our different CPU architecture, the Workspace module provides tools to install custom software stacks within your Workspace. Especially with EasyBuild shortcuts are provided to install and use custom software stacks easily build for all architectures. For installing packages with EasyBuild, see EasyBuild description . Manual package can also be installed in the similar manner. Adding a Modulefile provides the users to load packages as used to. Please see Installing Custom Software . As a result all users of the Workspace can use the software packages by loading the Workspace module and the software product module. First load Python/Anaconda3 Python or Anaconda3 module need to be loaded before loading the Workspace module, since variables to be set depend on Python version. Workspace module can also be reloaded, e.g.: export HPC_WORKSPACE=HPC_SW_test; module load Workspace module load Python module load Workspace Conda environments The Workspace module provides support for creating and using conda environments in the shared Workspace. See Anaconda Conda environments . R packages The Workspace module provides support for installing and using additional R packages in the shared Workspace. Thus a package once installed by one user can be used by all Workspace members. See R installing packages . UMASK The Workspace module sets the umask to 002. Thus files and directories get group-writeable, e.g.: -rw-rw-r-- 1 user group 0 Mar 15 15 :15 /path/to/file","title":"Software environment"},{"location":"hpc-workspaces/environment.html#hpc-workspace-data-and-software-tools","text":"","title":"HPC Workspace Data and Software Tools"},{"location":"hpc-workspaces/environment.html#description","text":"HPC Workspace module provides support for user-friendly file system access, custom software stacks in HPC Workspaces, and SLURM accounting. The module can also be used to set up HOME for a custom software stack.","title":"Description"},{"location":"hpc-workspaces/environment.html#workspace-module","text":"The Workspace module adjust the environment to work in a specific HPC Workspace. module load Workspace sets the following environment variables ( Shortcuts ) and Software stacks There are the following possibilities: you belong to no Workspace: load module load Workspace/home to use your software stack in your HOME directory you belong to one Workspace: this Workspace gets loaded when module load Workspace you belong to multiple Workspaces: you need to specify the Workspace to load using the variable $HPC_WORKSPACE . The module presents the possible options, e.g.: $ module load Workspace Workspaces are available: HPC_SW_test, hpc_training, Please select and load ONE of the following: export HPC_WORKSPACE=HPC_SW_test; module load Workspace export HPC_WORKSPACE=hpc_training; module load Workspace Thus you load a specific Workspace using: export HPC_WORKSPACE=<WorkspaceName>; module load Workspace","title":"Workspace module"},{"location":"hpc-workspaces/environment.html#shortcuts","text":"The workspace module provides the following variables: Variable Function $WORKSPACE full path to the Workspace. Thus, you can access the workspace using: cd $WORKSPACE $SCRATCH full path to the Workspace SCRATCH directory. Thus you can access it using: cd $SCRATCH","title":"Shortcuts"},{"location":"hpc-workspaces/environment.html#additional-settings","text":"the module provides the following settings for a more user-friendly usage of applications. You may not need to use them directly, but tools like SLURM, Singularity, Python, and R will use them. Variable Function $SBATCH_ACCOUNT sets the SLURM account to the Workspace account. Thus all submitted jobs with that module are accounted to the Workspace account automatically. No need to set it in the sbatch script $SINGULARITY_BINDPATH using singularity, the Workspace directory will be bind into the container without manual specification. The WORKSPACE variable as well as the SCRATCH variable will also be ported into the container. Thus, you can specify locations with $WORKSPACE or $SCRATCH within the container. $PYTHONPATH if Python or Anaconda is loaded beforehand, it is set to: $WORKSPACE/PyPackages/lib/pythonXXX/site-packages where XXX is the Python major and minor version. And also add the bin directory to $PATH . $PYTHONPACKAGEPATH if Python or Anaconda is loaded beforehand, it is set to: $WORKSPACE/PyPackages . This can be used for e.g. pip install --prefix $PYTHONPACKAGEPATH $CONDA_ENVS_PATH therewith conda environments can be created and shared within the Workspace $R_LIBS therewith additional R packages can be installed and searched in the shared Workspace. The directory need to be created first. See R page","title":"Additional Settings"},{"location":"hpc-workspaces/environment.html#software-stacks","text":"Beside, a set of software packages we provide for our different CPU architecture, the Workspace module provides tools to install custom software stacks within your Workspace. Especially with EasyBuild shortcuts are provided to install and use custom software stacks easily build for all architectures. For installing packages with EasyBuild, see EasyBuild description . Manual package can also be installed in the similar manner. Adding a Modulefile provides the users to load packages as used to. Please see Installing Custom Software . As a result all users of the Workspace can use the software packages by loading the Workspace module and the software product module. First load Python/Anaconda3 Python or Anaconda3 module need to be loaded before loading the Workspace module, since variables to be set depend on Python version. Workspace module can also be reloaded, e.g.: export HPC_WORKSPACE=HPC_SW_test; module load Workspace module load Python module load Workspace","title":"Software stacks"},{"location":"hpc-workspaces/environment.html#conda-environments","text":"The Workspace module provides support for creating and using conda environments in the shared Workspace. See Anaconda Conda environments .","title":"Conda environments"},{"location":"hpc-workspaces/environment.html#r-packages","text":"The Workspace module provides support for installing and using additional R packages in the shared Workspace. Thus a package once installed by one user can be used by all Workspace members. See R installing packages .","title":"R packages"},{"location":"hpc-workspaces/environment.html#umask","text":"The Workspace module sets the umask to 002. Thus files and directories get group-writeable, e.g.: -rw-rw-r-- 1 user group 0 Mar 15 15 :15 /path/to/file","title":"UMASK"},{"location":"hpc-workspaces/management.html","text":"Workspace management Description This article targets Workspace managers. It covers applications, Workspace properties, up to modifications and suggested usage of Workspaces. Workspace Intention Workspaces are group shared resources. This group could be for example: whole research group working together on shared data and software packages student(s) an their supervisor collaborating researchers from different research groups/institutes sub-group within the research group, which requires data separation Workspace Properties Ownership HPC Workspaces manage compute and storage resources. Especially the freely available resources are meant to be shared between participating research groups. Therefore, the Workspaces belong to a beforehand registered research group. A HPC Workspace can be only requested by a research group leader , who responsible and accountable, since costly extensions can be added to the Workspace. Additionally, a deputy can be nominated, who also can manage the Workspace (see Workspace Modification ). Owners and deputies are called Workspace managers . Deputies Deputies have the same privileges than the owner, up to purchasing costly resources without additional notification. If the workspace is meant to be for a collaboration of researchers from different research groups, you need to agree to one research group which is responsibilities and gets accounted for the resources. This research group leader need to request the workspace. Member groups Each Workspace has two member groups: primary users with read and write access and secondary users with read only access Only members of the primary group can create and modify data, belonging to the Workspace, as well as submitting jobs to the Workspace account. The member lists are defined at Workspace application time and can be modified later. Members can be anyone with an UniBE Campus Account. Free Of Charge Quota Every research group has 10TB free of charge quota. This can be used within one or more Workspaces. The amount used per Workspace is set at application time and can be changed later within the limitation. Additional Storage Additional storage can be purchased for 50CHF per TB per year. On the application or modification form an quota upper limit can be set. Accounted will be the actual usage. Therefore, the actual usage is monitored and twice a day. The average value of all data points is used for accounting. Availability Storage is a limited and expensive resource. Abandoned, unused workspaces should be prevented by design. Therefore, an HPC Workspace has a default live time of one year. A notification will be send before the Workspace expiry. The Workspace expiry date can be changed at any time to any date within the next 365 days by any Workspace manager. ServicePortal -> Shop -> HPC -> Edit HPC Workspace -> Workspace Duratio Application Prerequisite The research group need to be registered first on the Service Portal once: Service Portal -> Register Research Group The request need to be verified manually. This may take some time. Application Form A HPC Workspace can (only) be requested by a registered research group lead/manager using the ServicePortal application form: Service Portal -> Create HPC Workspace The following information are required: Workspace ID (max. 20 characters) Workspace Name Workspace Description registered research group (see prerequisites) Deputy (permissions for managing the Workspace) (optional) Free Quota (see above) additional Storage (optional): an upper limit of quota, where the actual quota will be charged. When selected this requires a valid cost center for accounting. Cost Center (necessary when requesting Additional Storage) primary group members secondary group members (optional) member lists can be selected one by one or as a comma separated list of Campus accounts (see Import Bulk User ) Notification After requesting the Workspace creation, a notification will be send. The content with \u201cdata point not found\u201d may be confusing, but still your request is successfully submitted. processing time The Workspace creation for now relies on a temporary automatic process which is running only once a day at 20:00. In future the process will be much faster. Workspace modifications After creation, owners and deputies can modify Workspace properties using the Service Portal Form: Service Portal -> Edit HPC Workspace Properties to change are: - adding/removing members to/from primary and secondary group - storage extension - Workspace live time extension - Workspace closure (so far you need to \u201cdeactivate\u201d AND THEN \u201cdelete\u201d the workspace) Note During the processing of a modification no other modification can be requested. The Workspace is even not visible in the ServicePortal for that time. Most modification will be processed within few minutes, but adding non-free-of-charge features like additional storage, need human approval, which may delay the process. The Workspace itself (file storage and Slurm, etc.) will not be interrupted by a change. Investor QoS Investors get elevated priviledges for specific queues. These are managed in so called Slurm QoS (Quality of Service). Where in the past the investors specified a list of users, who can use the QoS, now with Workspaces we are able to manage the QoS on Workspace level. Therefore, you need to open a request to add an existing QoS to a (list of) Workspace(s). The membership managment is done with the Workspace. Import Bulk Users The \u201cImport Bulk Users\u201d field provides the possibility to list a larger set of members for primary or secondary group without selecting them one by one. There the members need to be specified as a comma seperated list of Campus Accounts (not full names). Keep in mind: you need to specify the full list of members in these field. After leaving the field the upper primary/secondary member list will be replaced with this list.","title":"Workspace management"},{"location":"hpc-workspaces/management.html#workspace-management","text":"","title":"Workspace management"},{"location":"hpc-workspaces/management.html#description","text":"This article targets Workspace managers. It covers applications, Workspace properties, up to modifications and suggested usage of Workspaces.","title":"Description"},{"location":"hpc-workspaces/management.html#workspace-intention","text":"Workspaces are group shared resources. This group could be for example: whole research group working together on shared data and software packages student(s) an their supervisor collaborating researchers from different research groups/institutes sub-group within the research group, which requires data separation","title":"Workspace Intention"},{"location":"hpc-workspaces/management.html#workspace-properties","text":"","title":"Workspace Properties"},{"location":"hpc-workspaces/management.html#ownership","text":"HPC Workspaces manage compute and storage resources. Especially the freely available resources are meant to be shared between participating research groups. Therefore, the Workspaces belong to a beforehand registered research group. A HPC Workspace can be only requested by a research group leader , who responsible and accountable, since costly extensions can be added to the Workspace. Additionally, a deputy can be nominated, who also can manage the Workspace (see Workspace Modification ). Owners and deputies are called Workspace managers . Deputies Deputies have the same privileges than the owner, up to purchasing costly resources without additional notification. If the workspace is meant to be for a collaboration of researchers from different research groups, you need to agree to one research group which is responsibilities and gets accounted for the resources. This research group leader need to request the workspace.","title":"Ownership"},{"location":"hpc-workspaces/management.html#member-groups","text":"Each Workspace has two member groups: primary users with read and write access and secondary users with read only access Only members of the primary group can create and modify data, belonging to the Workspace, as well as submitting jobs to the Workspace account. The member lists are defined at Workspace application time and can be modified later. Members can be anyone with an UniBE Campus Account.","title":"Member groups"},{"location":"hpc-workspaces/management.html#free-of-charge-quota","text":"Every research group has 10TB free of charge quota. This can be used within one or more Workspaces. The amount used per Workspace is set at application time and can be changed later within the limitation.","title":"Free Of Charge Quota"},{"location":"hpc-workspaces/management.html#additional-storage","text":"Additional storage can be purchased for 50CHF per TB per year. On the application or modification form an quota upper limit can be set. Accounted will be the actual usage. Therefore, the actual usage is monitored and twice a day. The average value of all data points is used for accounting.","title":"Additional Storage"},{"location":"hpc-workspaces/management.html#availability","text":"Storage is a limited and expensive resource. Abandoned, unused workspaces should be prevented by design. Therefore, an HPC Workspace has a default live time of one year. A notification will be send before the Workspace expiry. The Workspace expiry date can be changed at any time to any date within the next 365 days by any Workspace manager. ServicePortal -> Shop -> HPC -> Edit HPC Workspace -> Workspace Duratio","title":"Availability"},{"location":"hpc-workspaces/management.html#application","text":"","title":"Application"},{"location":"hpc-workspaces/management.html#prerequisite","text":"The research group need to be registered first on the Service Portal once: Service Portal -> Register Research Group The request need to be verified manually. This may take some time.","title":"Prerequisite"},{"location":"hpc-workspaces/management.html#application-form","text":"A HPC Workspace can (only) be requested by a registered research group lead/manager using the ServicePortal application form: Service Portal -> Create HPC Workspace The following information are required: Workspace ID (max. 20 characters) Workspace Name Workspace Description registered research group (see prerequisites) Deputy (permissions for managing the Workspace) (optional) Free Quota (see above) additional Storage (optional): an upper limit of quota, where the actual quota will be charged. When selected this requires a valid cost center for accounting. Cost Center (necessary when requesting Additional Storage) primary group members secondary group members (optional) member lists can be selected one by one or as a comma separated list of Campus accounts (see Import Bulk User ) Notification After requesting the Workspace creation, a notification will be send. The content with \u201cdata point not found\u201d may be confusing, but still your request is successfully submitted. processing time The Workspace creation for now relies on a temporary automatic process which is running only once a day at 20:00. In future the process will be much faster.","title":"Application Form"},{"location":"hpc-workspaces/management.html#workspace-modifications","text":"After creation, owners and deputies can modify Workspace properties using the Service Portal Form: Service Portal -> Edit HPC Workspace Properties to change are: - adding/removing members to/from primary and secondary group - storage extension - Workspace live time extension - Workspace closure (so far you need to \u201cdeactivate\u201d AND THEN \u201cdelete\u201d the workspace) Note During the processing of a modification no other modification can be requested. The Workspace is even not visible in the ServicePortal for that time. Most modification will be processed within few minutes, but adding non-free-of-charge features like additional storage, need human approval, which may delay the process. The Workspace itself (file storage and Slurm, etc.) will not be interrupted by a change.","title":"Workspace modifications"},{"location":"hpc-workspaces/management.html#investor-qos","text":"Investors get elevated priviledges for specific queues. These are managed in so called Slurm QoS (Quality of Service). Where in the past the investors specified a list of users, who can use the QoS, now with Workspaces we are able to manage the QoS on Workspace level. Therefore, you need to open a request to add an existing QoS to a (list of) Workspace(s). The membership managment is done with the Workspace.","title":"Investor QoS"},{"location":"hpc-workspaces/management.html#import-bulk-users","text":"The \u201cImport Bulk Users\u201d field provides the possibility to list a larger set of members for primary or secondary group without selecting them one by one. There the members need to be specified as a comma seperated list of Campus Accounts (not full names). Keep in mind: you need to specify the full list of members in these field. After leaving the field the upper primary/secondary member list will be replaced with this list.","title":"Import Bulk Users"},{"location":"hpc-workspaces/monitoring.html","text":"HPC Workspace Monitoring Tools Description Workspaces are consting of different features. Depending on the feature you can check settings using: Feature tool permission Storage call quota on a ubelix login node, see Quota Tool , accounting information using quota -l everyone belonging to the Workspace prim./sec. group membership Service Portal -> Edit HPC Workspace Workspace owner and deputy SLURM fair share still under construction, comming soon everyone belonging to the Workspace Workspace properties can be changed by Workspace managers, see Workspace Management","title":"Monitoring"},{"location":"hpc-workspaces/monitoring.html#hpc-workspace-monitoring-tools","text":"","title":"HPC Workspace Monitoring Tools"},{"location":"hpc-workspaces/monitoring.html#description","text":"Workspaces are consting of different features. Depending on the feature you can check settings using: Feature tool permission Storage call quota on a ubelix login node, see Quota Tool , accounting information using quota -l everyone belonging to the Workspace prim./sec. group membership Service Portal -> Edit HPC Workspace Workspace owner and deputy SLURM fair share still under construction, comming soon everyone belonging to the Workspace Workspace properties can be changed by Workspace managers, see Workspace Management","title":"Description"},{"location":"hpc-workspaces/workspaces.html","text":"Workspaces Introduction Description This article introduces HPC workspaces with the main aspects. A guideline for Workspace managers including application and modification can be found at Workspace Management . If you want to join an existing Workspace , please ask the Workspace owner or manager to add your account. A HPC workspace consists of: 2 access groups, read/write and read only permanent and temporary storage Slurm accounting fair share on research group level Attention Please always load the Workspace module, even if only just copying files into it. The module corrects the umask and therewith the created file and directory permissions. Furthermore, it is good practice to use $WORKSPACE/file1 instead of absolute path /path/to/Workspace/file1 Motivation HPC data typically is shared data. This could be between students and supervisors, between researchers of a research group, or even between researchers of different institutes. These data needs to be accessible even if people leave the team. Furthermore, these data is usually processed with a set of custom software tools, which need to be easy accessible, share, between the collaborators. Advantages: group based storage access (data and software) enhanced collaborations between researchers, even across institutes user-friendly access control by Workspace managers high bandwidth storage and backup temporary space with less restricted quota research group based compute resource sharing in-line with other HPC centres Storage Access Permissions HOME is meant to be a private space, mainly for configurational data. All group oriented data is meant to be located in a HPC Workspace directories: /storage/workspaces/<researchGroupID>/<workspaceID> for permanent storage /storage/scratch/ for temporary storage, see File Systems All files and directories in the group shared spaces are meant to stay with default permissions, like -rw-rw-r-- : read and write for owner and primary group members read only access for others. Since the Workspace directory itself is limited to primary and secondary group members using ACLs, secondary group members get read permissions. Members The Workspace owner can define a deputy who gets same permissions as owner, including booking at cost. Workspace permissions are managed with two lists of members : primary users, with read and write access to all data in the spaces, permission to account to that Workspace secondary users, with read only access Thus, the Workspace and SCRATCH space are accessible to all members of the both lists of users, but only the members of the primary list can write or modify data. Members can be anyone with an active UniBE Campus Account. The Workspace owner or its deputies can manage the lists using the Service Portal Backup All data in the permanent space ( /storage/workspaces/ ) is protected using daily snapshots and backups Scratch will not be protected. Quota For default and actual quota information use quota tool. More details see File System Quota . SLURM: Computational work is accounted to a Workspace account. Every workspace belongs to a research group. The freely available resources are shared between research groups. In contrast to the previous implementation, the priorities are based on research group usage and shared between all workspaces related to this research group.","title":"Overview"},{"location":"hpc-workspaces/workspaces.html#workspaces-introduction","text":"","title":"Workspaces Introduction"},{"location":"hpc-workspaces/workspaces.html#description","text":"This article introduces HPC workspaces with the main aspects. A guideline for Workspace managers including application and modification can be found at Workspace Management . If you want to join an existing Workspace , please ask the Workspace owner or manager to add your account. A HPC workspace consists of: 2 access groups, read/write and read only permanent and temporary storage Slurm accounting fair share on research group level Attention Please always load the Workspace module, even if only just copying files into it. The module corrects the umask and therewith the created file and directory permissions. Furthermore, it is good practice to use $WORKSPACE/file1 instead of absolute path /path/to/Workspace/file1","title":"Description"},{"location":"hpc-workspaces/workspaces.html#motivation","text":"HPC data typically is shared data. This could be between students and supervisors, between researchers of a research group, or even between researchers of different institutes. These data needs to be accessible even if people leave the team. Furthermore, these data is usually processed with a set of custom software tools, which need to be easy accessible, share, between the collaborators.","title":"Motivation"},{"location":"hpc-workspaces/workspaces.html#advantages","text":"group based storage access (data and software) enhanced collaborations between researchers, even across institutes user-friendly access control by Workspace managers high bandwidth storage and backup temporary space with less restricted quota research group based compute resource sharing in-line with other HPC centres","title":"Advantages:"},{"location":"hpc-workspaces/workspaces.html#storage-access-permissions","text":"HOME is meant to be a private space, mainly for configurational data. All group oriented data is meant to be located in a HPC Workspace directories: /storage/workspaces/<researchGroupID>/<workspaceID> for permanent storage /storage/scratch/ for temporary storage, see File Systems All files and directories in the group shared spaces are meant to stay with default permissions, like -rw-rw-r-- : read and write for owner and primary group members read only access for others. Since the Workspace directory itself is limited to primary and secondary group members using ACLs, secondary group members get read permissions.","title":"Storage Access Permissions"},{"location":"hpc-workspaces/workspaces.html#members","text":"The Workspace owner can define a deputy who gets same permissions as owner, including booking at cost. Workspace permissions are managed with two lists of members : primary users, with read and write access to all data in the spaces, permission to account to that Workspace secondary users, with read only access Thus, the Workspace and SCRATCH space are accessible to all members of the both lists of users, but only the members of the primary list can write or modify data. Members can be anyone with an active UniBE Campus Account. The Workspace owner or its deputies can manage the lists using the Service Portal","title":"Members"},{"location":"hpc-workspaces/workspaces.html#backup","text":"All data in the permanent space ( /storage/workspaces/ ) is protected using daily snapshots and backups Scratch will not be protected.","title":"Backup"},{"location":"hpc-workspaces/workspaces.html#quota","text":"For default and actual quota information use quota tool. More details see File System Quota .","title":"Quota"},{"location":"hpc-workspaces/workspaces.html#slurm","text":"Computational work is accounted to a Workspace account. Every workspace belongs to a research group. The freely available resources are shared between research groups. In contrast to the previous implementation, the priorities are based on research group usage and shared between all workspaces related to this research group.","title":"SLURM:"},{"location":"slurm/array-jobs.html","text":"Array Jobs with Slurm Description You want to submit multiple jobs that are identical or differ only in some arguments. Instead of submitting N jobs independently, you can submit one array job unifying N tasks. Submitting an Array To submit an array job, specify the number of tasks as a range of task ids using the \u2013array option: #SBATCH --array=n[,k[,...]][-m[:s]] The task id range specified in the option argument may be a single number, a simple range of the form n-m, a range with a step size s, a comma separated list of values, or a combination thereof. The task ids will be exported to the job tasks via the environment variable SLURM_ARRAY_TASK_ID. Other variables available in the context of the job describing the task range are: SLURM_ARRAY_TASK_MAX, SLURM_ARRAY_TASK_MIN, SLURM_ARRAY_TASK_STEP. Specifying \u2013array=10 will not submit an array job with 10 tasks, but an array job with a single task with task id 10. To run an array job with multiple tasks you must specify a range or a comma separated list of task ids. Limit the Number of Concurrently Running Tasks You may want to limit the number of concurrently running tasks if the tasks are very resource demanding and too many of them running concurrently would lower the overall performance of the cluster. To limit the number of tasks that are allowed to run concurrently, use a \u201c%\u201d separator: #SBATCH --array=n[,k[,...]][-m[:s]]%<max_tasks> Canceling Individual Tasks You can cancel individual tasks of an array job by indicating tasks ids to the scancel command: $ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 79265_ [ 49 -99:2%20 ] test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_41 test Simple H foo R 0 :10 1 fnode03 79265_43 test Simple H foo R 0 :10 1 fnode03 79265_45 test Simple H foo R 0 :10 1 fnode03 79265_47 test Simple H foo R 0 :10 1 fnode03 Use the \u2013array option to the squeue command to display one tasks per line: $ squeue --array JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 79265_65 test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_67 test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_69 test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_97 test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_57 test Simple H foo R 0 :47 1 fnode03 79265_59 test Simple H foo R 0 :47 1 fnode03 79265_61 test Simple H foo R 0 :47 1 fnode03 79265_63 test Simple H foo R 0 :47 1 fnode03 Examples Use case 1: 1000 computations, same resource requirements, different input/output arguments Instead of submitting 1000 individual jobs, submit a single array jobs with 1000 tasks: #!/bin/bash #SBATCH --time=00:30:00 # Each task takes max 30 minutes #SBATCH --mem-per-cpu=2G # Each task uses max 2G of memory #SBATCH --array=1-1000 # Submit 1000 tasks with task ID 1,2,...,1000. # The name of the input files must reflect the task ID! ./foo input_data_ ${ SLURM_ARRAY_TASK_ID } .txt > output_ ${ SLURM_ARRAY_TASK_ID } .txt Task with ID 20 will run the program foo with the following arguments: ./foo input_data_20.txt > output_20.txt Use case 2: Read arguments from file Submit an array job with 1000 tasks. Each task executes the program foo with different arguments: #!/bin/bash #SBATCH --time=00:30:00 # Each task takes max 30 minutes #SBATCH --mem-per-cpu=2G # Each task uses max 2G of memory ### Submit 1000 tasks with task ID 1,2,...,1000. Run max 20 tasks concurrently #SBATCH --array=1-1000%20 data_dir = $HOME /projects/example/input_data result_dir = $HOME /projects/example/results param_store = $HOME /projects/example/args.txt ### args.txt contains 1000 lines with 2 arguments per line. ### Line <i> contains arguments for run <i> # Get first argument param_a = $( cat $param_store | awk -v var = $SLURM_ARRAY_TASK_ID 'NR==var {print $1}' ) # Get second argument param_b = $( cat $param_store | awk -v var = $SLURM_ARRAY_TASK_ID 'NR==var {print $2}' ) ### Input files are named input_run_0001.txt,...input_run_1000.txt ### Zero pad the task ID to match the numbering of the input files n = $( printf \"%04d\" $SLURM_ARRAY_TASK_ID ) srun ./foo -c $param_a -p $param_b -i ${ data_dir } /input_run_ ${ n } .txt -o ${ result_dir } /result_run_ ${ n } .txt","title":"Array Jobs with Slurm"},{"location":"slurm/array-jobs.html#array-jobs-with-slurm","text":"","title":"Array Jobs with Slurm"},{"location":"slurm/array-jobs.html#description","text":"You want to submit multiple jobs that are identical or differ only in some arguments. Instead of submitting N jobs independently, you can submit one array job unifying N tasks.","title":"Description"},{"location":"slurm/array-jobs.html#submitting-an-array","text":"To submit an array job, specify the number of tasks as a range of task ids using the \u2013array option: #SBATCH --array=n[,k[,...]][-m[:s]] The task id range specified in the option argument may be a single number, a simple range of the form n-m, a range with a step size s, a comma separated list of values, or a combination thereof. The task ids will be exported to the job tasks via the environment variable SLURM_ARRAY_TASK_ID. Other variables available in the context of the job describing the task range are: SLURM_ARRAY_TASK_MAX, SLURM_ARRAY_TASK_MIN, SLURM_ARRAY_TASK_STEP. Specifying \u2013array=10 will not submit an array job with 10 tasks, but an array job with a single task with task id 10. To run an array job with multiple tasks you must specify a range or a comma separated list of task ids.","title":"Submitting an Array"},{"location":"slurm/array-jobs.html#limit-the-number-of-concurrently-running-tasks","text":"You may want to limit the number of concurrently running tasks if the tasks are very resource demanding and too many of them running concurrently would lower the overall performance of the cluster. To limit the number of tasks that are allowed to run concurrently, use a \u201c%\u201d separator: #SBATCH --array=n[,k[,...]][-m[:s]]%<max_tasks>","title":"Limit the Number of Concurrently Running Tasks"},{"location":"slurm/array-jobs.html#canceling-individual-tasks","text":"You can cancel individual tasks of an array job by indicating tasks ids to the scancel command: $ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 79265_ [ 49 -99:2%20 ] test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_41 test Simple H foo R 0 :10 1 fnode03 79265_43 test Simple H foo R 0 :10 1 fnode03 79265_45 test Simple H foo R 0 :10 1 fnode03 79265_47 test Simple H foo R 0 :10 1 fnode03 Use the \u2013array option to the squeue command to display one tasks per line: $ squeue --array JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) 79265_65 test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_67 test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_69 test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_97 test Simple H foo PD 0 :00 1 ( QOSMaxCpuPerUserLimit ) 79265_57 test Simple H foo R 0 :47 1 fnode03 79265_59 test Simple H foo R 0 :47 1 fnode03 79265_61 test Simple H foo R 0 :47 1 fnode03 79265_63 test Simple H foo R 0 :47 1 fnode03","title":"Canceling Individual Tasks"},{"location":"slurm/array-jobs.html#examples","text":"","title":"Examples"},{"location":"slurm/array-jobs.html#use-case-1-1000-computations-same-resource-requirements-different-inputoutput-arguments","text":"Instead of submitting 1000 individual jobs, submit a single array jobs with 1000 tasks: #!/bin/bash #SBATCH --time=00:30:00 # Each task takes max 30 minutes #SBATCH --mem-per-cpu=2G # Each task uses max 2G of memory #SBATCH --array=1-1000 # Submit 1000 tasks with task ID 1,2,...,1000. # The name of the input files must reflect the task ID! ./foo input_data_ ${ SLURM_ARRAY_TASK_ID } .txt > output_ ${ SLURM_ARRAY_TASK_ID } .txt Task with ID 20 will run the program foo with the following arguments: ./foo input_data_20.txt > output_20.txt","title":"Use case 1: 1000 computations, same resource requirements, different input/output arguments"},{"location":"slurm/array-jobs.html#use-case-2-read-arguments-from-file","text":"Submit an array job with 1000 tasks. Each task executes the program foo with different arguments: #!/bin/bash #SBATCH --time=00:30:00 # Each task takes max 30 minutes #SBATCH --mem-per-cpu=2G # Each task uses max 2G of memory ### Submit 1000 tasks with task ID 1,2,...,1000. Run max 20 tasks concurrently #SBATCH --array=1-1000%20 data_dir = $HOME /projects/example/input_data result_dir = $HOME /projects/example/results param_store = $HOME /projects/example/args.txt ### args.txt contains 1000 lines with 2 arguments per line. ### Line <i> contains arguments for run <i> # Get first argument param_a = $( cat $param_store | awk -v var = $SLURM_ARRAY_TASK_ID 'NR==var {print $1}' ) # Get second argument param_b = $( cat $param_store | awk -v var = $SLURM_ARRAY_TASK_ID 'NR==var {print $2}' ) ### Input files are named input_run_0001.txt,...input_run_1000.txt ### Zero pad the task ID to match the numbering of the input files n = $( printf \"%04d\" $SLURM_ARRAY_TASK_ID ) srun ./foo -c $param_a -p $param_b -i ${ data_dir } /input_run_ ${ n } .txt -o ${ result_dir } /result_run_ ${ n } .txt","title":"Use case 2: Read arguments from file"},{"location":"slurm/checkpointing.html","text":"Checkpointing Description Checkpointing: Saving the state of a computation so that it can be resumed later. On this page we provide some useful information for making your own code checkpoint-able. Currently, we do not provide and support programs and libraries (e.g. BLCR) that allow to checkpoint proprietary (closed source) software. Some applications provide built-in checkpoint/restart mechanisms: Gaussian, Quantum Espresso, CP2K and more. Why checkpointing Imagine a job is already running for several hours when an event occurs which leads to the abortion of the job. Such events can be: Exceeding the time limit Exceeding allocated memory Job gets preempted by another job (gpu partition only!) Node failure Checkpointing a job means that you frequently save the job state so that you can resume computation from the last checkpoint in case of a disastrous event. General recipe for checkpointing your own code Introducing checkpointing logic in your code consists of 3 steps 1. Look for a state file containing a previously saved state. 2. If a state file exists, then restore the state. Else, start from scratch. 3. Periodically save the state. Using UNIX signals You can save the state of your job at specific points in time, after certain iterations, or at whatever event you choose to trigger a state saving. You can also trap specific UNIX signals and act as soon as the signal occurs. The following table lists common signals that you might want to trap in your program: Signal Name Signal Number Description Default Disposition SIGTERM 15 SIGTERM initiates the termination of a process Term - Terminate the process SIGCONT 18 SIGCONT continues a stopped process Cont - Continue the process if stopped SIGUSR1 10 User-defined signals. SIGUSR1/SIGUSR2 are never sent by the kernel Term - Terminate the process SIGUSR2 12 User-defined signals. SIGUSR1/SIGUSR2 are never sent by the kernel Term - Terminate the process kill -l for a list of all supported signals. Note that some signals cannot be trapped, e.g SIGKILL Slurm sends SIGCONT followed by SIGTERM just before a job is canceled. Trapping the signal (e.g. SIGTERM) gives you 60 seconds for housekeeping tasks, e.g. save current state. At the latest after that your job is canceled with SIGKILL. This is true for jobs canceled by the owner using scancel and jobs canceled by Slurm, e.g. because of exceeding time limit. Register a signal handler for a UNIX signal The following examples show how to register a signal handler in different languages, but omit the logic for creating a checkpoint and restart a job from an existing checkpoint. We will provide a working example further down below on this page. Bash #!/bin/bash function signal_handler { # Save program state and exit ( ... ) exit } trap signal_handler TERM ( ... ) C/C++ #include <signal.h> // C #include <csignal> // C++ void signal_handler ( int signal ) { // Save program state and exit (...) exit ( 0 ); } // Register signal handler for SIGTERM signal ( SIGTERM , signal_handler ); // signal_handler: function to handle signal (...) Python #! /usr/bin/env python import signal import sys def signal_handler ( sig , frame ): # Save program state and exit ( ... ) sys . exit ( 0 ) signal . signal ( signal . SIGTERM , signal_handler ) ( ... ) Signaling checkpoint creation without canceling the job You can use a UNIX signal to trigger the creation of a checkpoint of a running job. For example, consider a job that traps SIGUSR1 and saves intermediate results as soon as the signal occurs. You can then create a checkpoint by signaling SIGUSR1 to the job using scancel : scancel --signal = USR1 <jobid> Use \u2013batch option to signal the batch step (shell script), but not any other associated job step (srun) or child processes of the shell script. Use \u2013full option to signal all steps associated with the job including the shell script and its child processes. Using scancel with the \u2013signal option won\u2019t terminate the job or job step.","title":"Checkpointing"},{"location":"slurm/checkpointing.html#checkpointing","text":"","title":"Checkpointing"},{"location":"slurm/checkpointing.html#description","text":"Checkpointing: Saving the state of a computation so that it can be resumed later. On this page we provide some useful information for making your own code checkpoint-able. Currently, we do not provide and support programs and libraries (e.g. BLCR) that allow to checkpoint proprietary (closed source) software. Some applications provide built-in checkpoint/restart mechanisms: Gaussian, Quantum Espresso, CP2K and more.","title":"Description"},{"location":"slurm/checkpointing.html#why-checkpointing","text":"Imagine a job is already running for several hours when an event occurs which leads to the abortion of the job. Such events can be: Exceeding the time limit Exceeding allocated memory Job gets preempted by another job (gpu partition only!) Node failure Checkpointing a job means that you frequently save the job state so that you can resume computation from the last checkpoint in case of a disastrous event.","title":"Why checkpointing"},{"location":"slurm/checkpointing.html#general-recipe-for-checkpointing-your-own-code","text":"Introducing checkpointing logic in your code consists of 3 steps 1. Look for a state file containing a previously saved state. 2. If a state file exists, then restore the state. Else, start from scratch. 3. Periodically save the state.","title":"General recipe for checkpointing your own code"},{"location":"slurm/checkpointing.html#using-unix-signals","text":"You can save the state of your job at specific points in time, after certain iterations, or at whatever event you choose to trigger a state saving. You can also trap specific UNIX signals and act as soon as the signal occurs. The following table lists common signals that you might want to trap in your program: Signal Name Signal Number Description Default Disposition SIGTERM 15 SIGTERM initiates the termination of a process Term - Terminate the process SIGCONT 18 SIGCONT continues a stopped process Cont - Continue the process if stopped SIGUSR1 10 User-defined signals. SIGUSR1/SIGUSR2 are never sent by the kernel Term - Terminate the process SIGUSR2 12 User-defined signals. SIGUSR1/SIGUSR2 are never sent by the kernel Term - Terminate the process kill -l for a list of all supported signals. Note that some signals cannot be trapped, e.g SIGKILL Slurm sends SIGCONT followed by SIGTERM just before a job is canceled. Trapping the signal (e.g. SIGTERM) gives you 60 seconds for housekeeping tasks, e.g. save current state. At the latest after that your job is canceled with SIGKILL. This is true for jobs canceled by the owner using scancel and jobs canceled by Slurm, e.g. because of exceeding time limit.","title":"Using UNIX signals"},{"location":"slurm/checkpointing.html#register-a-signal-handler-for-a-unix-signal","text":"The following examples show how to register a signal handler in different languages, but omit the logic for creating a checkpoint and restart a job from an existing checkpoint. We will provide a working example further down below on this page. Bash #!/bin/bash function signal_handler { # Save program state and exit ( ... ) exit } trap signal_handler TERM ( ... ) C/C++ #include <signal.h> // C #include <csignal> // C++ void signal_handler ( int signal ) { // Save program state and exit (...) exit ( 0 ); } // Register signal handler for SIGTERM signal ( SIGTERM , signal_handler ); // signal_handler: function to handle signal (...) Python #! /usr/bin/env python import signal import sys def signal_handler ( sig , frame ): # Save program state and exit ( ... ) sys . exit ( 0 ) signal . signal ( signal . SIGTERM , signal_handler ) ( ... )","title":"Register a signal handler for a UNIX signal"},{"location":"slurm/checkpointing.html#signaling-checkpoint-creation-without-canceling-the-job","text":"You can use a UNIX signal to trigger the creation of a checkpoint of a running job. For example, consider a job that traps SIGUSR1 and saves intermediate results as soon as the signal occurs. You can then create a checkpoint by signaling SIGUSR1 to the job using scancel : scancel --signal = USR1 <jobid> Use \u2013batch option to signal the batch step (shell script), but not any other associated job step (srun) or child processes of the shell script. Use \u2013full option to signal all steps associated with the job including the shell script and its child processes. Using scancel with the \u2013signal option won\u2019t terminate the job or job step.","title":"Signaling checkpoint creation without canceling the job"},{"location":"slurm/deleting-jobs.html","text":"Deleting Jobs Description This page provides information on how to delete jobs. scancel Use the scancel command to delete active jobs Syntax scancel [ options ] <jobid> ... Common options --account Restrict the scancel operation to jobs under this charge account --jobname Restrict the scancel operation to jobs with this job name --partition Restrict the scancel operation to jobs in this partition --state Restrict the scancel operation to jobs in this state Examples Delete specific job: scancel 12345678 Delete all running jobs: scancel --state = R","title":"Deleting Jobs"},{"location":"slurm/deleting-jobs.html#deleting-jobs","text":"","title":"Deleting Jobs"},{"location":"slurm/deleting-jobs.html#description","text":"This page provides information on how to delete jobs.","title":"Description"},{"location":"slurm/deleting-jobs.html#scancel","text":"Use the scancel command to delete active jobs Syntax scancel [ options ] <jobid> ... Common options --account Restrict the scancel operation to jobs under this charge account --jobname Restrict the scancel operation to jobs with this job name --partition Restrict the scancel operation to jobs in this partition --state Restrict the scancel operation to jobs in this state","title":"scancel"},{"location":"slurm/deleting-jobs.html#examples","text":"Delete specific job: scancel 12345678 Delete all running jobs: scancel --state = R","title":"Examples"},{"location":"slurm/fair-share.html","text":"Fair Share Description The provided resources of UBELIX are meant to be provided in a fair faishon between all interested research groups. Therefore, Workspaces belong to research groups. Every participating research group is entitled for the same amount of resources. This entitlement is shared between all Workspace accounts and its members using that accounts. The fair-share system is designed to encourage users to balance their use of resources over time and de-prioritize users/accounts with over average usage. Thus no user group/research group gets the opportunity to over-dominate on the systems, while others request resources. Fair-share is the largest factor in determining priority, but not the only one. Fair Share Score Your Fair Share score is a number between 0 and 1. Projects with a larger Fair Share score receive a higher priority in the queue. If an entity \u2014 a research group or its Workspace account \u2014 is using the machine more slowly than expected, for Fair Share purposes it is considered a light user. By contrast, one using the machine faster than expected is a heavy user. Workspaces at lightly using research groups get a higher Fair Share score than those at heavily using research groups. Within each research group, lightly using Workspaces get a higher Fair Share score than heavily using Workspaces. Using faster than your expected rate of usage will usually cause your Fair Share score to decrease . The more extreme the overuse, the more severe the likely drop. Using slower than your expected rate of usage will usually cause your Fair Share score to increase . The more extreme the underuse, the greater the Fair Share bonus. Using the cluster unevenly will cause your Fair Share score to decrease . Priorities The Fair Share score is used to set the job priorities. It is based on a share of the cluster, which is a fraction of the cluster\u2019s overall computing capacity. Fair Share under Construction detailed information will follow soon For investors there priorities may vary.","title":"Fair Share"},{"location":"slurm/fair-share.html#fair-share","text":"","title":"Fair Share"},{"location":"slurm/fair-share.html#description","text":"The provided resources of UBELIX are meant to be provided in a fair faishon between all interested research groups. Therefore, Workspaces belong to research groups. Every participating research group is entitled for the same amount of resources. This entitlement is shared between all Workspace accounts and its members using that accounts. The fair-share system is designed to encourage users to balance their use of resources over time and de-prioritize users/accounts with over average usage. Thus no user group/research group gets the opportunity to over-dominate on the systems, while others request resources. Fair-share is the largest factor in determining priority, but not the only one.","title":"Description"},{"location":"slurm/fair-share.html#fair-share-score","text":"Your Fair Share score is a number between 0 and 1. Projects with a larger Fair Share score receive a higher priority in the queue. If an entity \u2014 a research group or its Workspace account \u2014 is using the machine more slowly than expected, for Fair Share purposes it is considered a light user. By contrast, one using the machine faster than expected is a heavy user. Workspaces at lightly using research groups get a higher Fair Share score than those at heavily using research groups. Within each research group, lightly using Workspaces get a higher Fair Share score than heavily using Workspaces. Using faster than your expected rate of usage will usually cause your Fair Share score to decrease . The more extreme the overuse, the more severe the likely drop. Using slower than your expected rate of usage will usually cause your Fair Share score to increase . The more extreme the underuse, the greater the Fair Share bonus. Using the cluster unevenly will cause your Fair Share score to decrease .","title":"Fair Share Score"},{"location":"slurm/fair-share.html#priorities","text":"The Fair Share score is used to set the job priorities. It is based on a share of the cluster, which is a fraction of the cluster\u2019s overall computing capacity.","title":"Priorities"},{"location":"slurm/fair-share.html#fair-share_1","text":"under Construction detailed information will follow soon For investors there priorities may vary.","title":"Fair Share"},{"location":"slurm/gpus.html","text":"GPUs Description This page contains all information you need to submit GPU-jobs successfully on Ubelix. Important Information on GPU Usage Code that runs on the CPU will not magically make use of GPUs by simply submitting a job to the \u2018gpu\u2019 partition! You have to explicitly adapt your code to run on the GPU. Also, code that runs on a GPU will not necessarily run faster than it runs on the CPU. For example, GPUs are not suited to handle tasks that are not highly parallelizable. In other words, you must understand the characteristics of your job, and make sure that you only submit jobs to the \u2018gpu\u2019 partition that can actually benefit from GPUs. GPU Type Ubelix currently features four types of GPUs. You have to choose an architecture and use the following --gres option to select it. Type SLURM gres option Nvidia Geforce GTX 1080 Ti --gres=gpu:gtx1080ti:<number_of_gpus> Nvidia Geforce RTX 2080 Ti --gres=gpu:rtx2080ti:<number_of_gpus> Nvidia Geforce RTX 3090 --gres=gpu:rtx3090:<number_of_gpus> Nvidia Tesla P100 --gres=gpu:teslaP100:<number_of_gpus> Job Submission Use the following options to submit a job to the gpu partition using the default job QoS: #SBATCH --partition=gpu #SBATCH --gres=gpu:<type>:<number_of_gpus> job_gpu_preempt For investors we provides investor partitions with specific QoS for each investor, defining the purchased resources. In case of GPU we want/need to provide instant access to purchased GPU resources. Nevertheless, to efficiently use all resources, the job_gpu_preemt exists in the gpu partition. Jobs, submitted with this QoS, may interrupted if resources are required for investors. Short jobs, and jobs with checkpointing benefit from these additional resources. For example requesting 4 RTX2080Ti #SBATCH --partition=gpu #SBATCH --qos=job_gpu_preempt #SBATCH --gres=gpu:rtx2080ti:4 Use the following option to ensure that the job, if preempted, won\u2019t be requeued but canceled instead: #SBATCH --no-requeue CUDA CUDA versions are now managed through modules. Run module avail to see which versions are available: module avail CUDA ---- /software.el7/modulefiles/all ---- CUDA/8.0.61 cuDNN/7.1.4-CUDA-9.2.88 CUDA/9.0.176 cuDNN/7.6.0.64-gcccuda-2019a ( D ) CUDA/9.1.85 fosscuda/2019a CUDA/9.2.88 fosscuda/2019b ( D ) CUDA/10.1.105-GCC-8.2.0-2.31.1 gcccuda/2019a CUDA/10.1.243 ( D ) gcccuda/2019b ( D ) cuDNN/6.0-CUDA-8.0.61 OpenMPI/3.1.3-gcccuda-2019a cuDNN/7.0.5-CUDA-9.0.176 OpenMPI/3.1.4-gcccuda-2019b cuDNN/7.0.5-CUDA-9.1.85 Run module load to load a specific version of CUDA: module load cuDNN/7.1.4-CUDA-9.2.88 If you need cuDNN you must load the cuDNN module. The appropriate CUDA version is then loaded as a dependency. Further Information CUDA: https://developer.nvidia.com/cuda-zone CUDA C/C++ Basics: http://www.nvidia.com/docs/IO/116711/sc11-cuda-c-basics.pdf Nvidia Geforce GTX 1080 Ti: https://www.nvidia.com/en-us/geforce/products/10series/geforce-gtx-1080-ti Nvidia Geforce RTX 2080 Ti: https://www.nvidia.com/de-de/geforce/graphics-cards/rtx-2080-ti/ Nvidia Geforce RTX 3090: https://www.nvidia.com/de-de/geforce/graphics-cards/30-series/rtx-3090/ Nvidia Tesla P100: http://www.nvidia.com/object/tesla-p100.html","title":"GPUs"},{"location":"slurm/gpus.html#gpus","text":"","title":"GPUs"},{"location":"slurm/gpus.html#description","text":"This page contains all information you need to submit GPU-jobs successfully on Ubelix.","title":"Description"},{"location":"slurm/gpus.html#important-information-on-gpu-usage","text":"Code that runs on the CPU will not magically make use of GPUs by simply submitting a job to the \u2018gpu\u2019 partition! You have to explicitly adapt your code to run on the GPU. Also, code that runs on a GPU will not necessarily run faster than it runs on the CPU. For example, GPUs are not suited to handle tasks that are not highly parallelizable. In other words, you must understand the characteristics of your job, and make sure that you only submit jobs to the \u2018gpu\u2019 partition that can actually benefit from GPUs.","title":"Important Information on GPU Usage"},{"location":"slurm/gpus.html#gpu-type","text":"Ubelix currently features four types of GPUs. You have to choose an architecture and use the following --gres option to select it. Type SLURM gres option Nvidia Geforce GTX 1080 Ti --gres=gpu:gtx1080ti:<number_of_gpus> Nvidia Geforce RTX 2080 Ti --gres=gpu:rtx2080ti:<number_of_gpus> Nvidia Geforce RTX 3090 --gres=gpu:rtx3090:<number_of_gpus> Nvidia Tesla P100 --gres=gpu:teslaP100:<number_of_gpus>","title":"GPU Type"},{"location":"slurm/gpus.html#job-submission","text":"Use the following options to submit a job to the gpu partition using the default job QoS: #SBATCH --partition=gpu #SBATCH --gres=gpu:<type>:<number_of_gpus>","title":"Job Submission"},{"location":"slurm/gpus.html#job_gpu_preempt","text":"For investors we provides investor partitions with specific QoS for each investor, defining the purchased resources. In case of GPU we want/need to provide instant access to purchased GPU resources. Nevertheless, to efficiently use all resources, the job_gpu_preemt exists in the gpu partition. Jobs, submitted with this QoS, may interrupted if resources are required for investors. Short jobs, and jobs with checkpointing benefit from these additional resources. For example requesting 4 RTX2080Ti #SBATCH --partition=gpu #SBATCH --qos=job_gpu_preempt #SBATCH --gres=gpu:rtx2080ti:4 Use the following option to ensure that the job, if preempted, won\u2019t be requeued but canceled instead: #SBATCH --no-requeue","title":"job_gpu_preempt"},{"location":"slurm/gpus.html#cuda","text":"CUDA versions are now managed through modules. Run module avail to see which versions are available: module avail CUDA ---- /software.el7/modulefiles/all ---- CUDA/8.0.61 cuDNN/7.1.4-CUDA-9.2.88 CUDA/9.0.176 cuDNN/7.6.0.64-gcccuda-2019a ( D ) CUDA/9.1.85 fosscuda/2019a CUDA/9.2.88 fosscuda/2019b ( D ) CUDA/10.1.105-GCC-8.2.0-2.31.1 gcccuda/2019a CUDA/10.1.243 ( D ) gcccuda/2019b ( D ) cuDNN/6.0-CUDA-8.0.61 OpenMPI/3.1.3-gcccuda-2019a cuDNN/7.0.5-CUDA-9.0.176 OpenMPI/3.1.4-gcccuda-2019b cuDNN/7.0.5-CUDA-9.1.85 Run module load to load a specific version of CUDA: module load cuDNN/7.1.4-CUDA-9.2.88 If you need cuDNN you must load the cuDNN module. The appropriate CUDA version is then loaded as a dependency.","title":"CUDA"},{"location":"slurm/gpus.html#further-information","text":"CUDA: https://developer.nvidia.com/cuda-zone CUDA C/C++ Basics: http://www.nvidia.com/docs/IO/116711/sc11-cuda-c-basics.pdf Nvidia Geforce GTX 1080 Ti: https://www.nvidia.com/en-us/geforce/products/10series/geforce-gtx-1080-ti Nvidia Geforce RTX 2080 Ti: https://www.nvidia.com/de-de/geforce/graphics-cards/rtx-2080-ti/ Nvidia Geforce RTX 3090: https://www.nvidia.com/de-de/geforce/graphics-cards/30-series/rtx-3090/ Nvidia Tesla P100: http://www.nvidia.com/object/tesla-p100.html","title":"Further Information"},{"location":"slurm/interactive-jobs.html","text":"Interactive Jobs Description This page describes how to request resources for interactive jobs and how to use the allocated resources when working interactively. Requesting resources Use salloc [options] to allocate resources for an interactive job. The command will block until sufficient resources are available. After the resources have been successfully allocated to your job your are still located on the submit host. Applications started now will not run within the allocation! bash-4.2$ salloc --nodes = 1 --ntasks-per-node = 4 --mem-per-cpu = 2G --time = 01 :00:00 salloc: Pending job allocation 63752579 salloc: job 63752579 queued and waiting for resources salloc: job 63752579 has been allocated resources salloc: Granted job allocation 63752579 bash-4.2$ hostname submit01.ubelix.unibe.ch Use an allocation Use srun [options] \u2013jobid= to start a job step under an existing job/allocation. bash-4.2$ srun -n1 --jobid = 63752579 hostname knode02 To work interactively on an allocated compute node: bash-4.2$ srun -n1 --jobid = 63752579 --pty bash bash-4.2$ hostname knode02 X11 Forwarding Requirements You must login to UBELIX with X11 forwarding enabled: ssh -X @submit.unibe.ch . Make this the default with ForwardX11 yes in ~/.ssh/config . Password-less communication between all nodes within UBELIX. In order to make this possible, generate a new SSH key (without passphrase) on the login node (submit) and add the public key to ~/.ssh/authorized_keys : ssh-keygen -t rsa -b 4096 cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys A X-Server on your local workstation, e.g. MAC: Xquartz (X11 no longer included in macOS) Windows: MobaXTerm or Xming DO NOT reuse an existing SSH key for this purpose, i.e. do not copy an existing private key from your local machine to UBELIX. With all the requirements in place you can now submit an interactive job and export an X11 display on the allocated compute node, e.g: srun -n 1 --pty --x11 xterm You can also use X11 forwarding with non interactive jobs adding the option #SBATCH --x11 in your job script and using again srun --x11 to launch your application.","title":"Interactive Jobs"},{"location":"slurm/interactive-jobs.html#interactive-jobs","text":"","title":"Interactive Jobs"},{"location":"slurm/interactive-jobs.html#description","text":"This page describes how to request resources for interactive jobs and how to use the allocated resources when working interactively.","title":"Description"},{"location":"slurm/interactive-jobs.html#requesting-resources","text":"Use salloc [options] to allocate resources for an interactive job. The command will block until sufficient resources are available. After the resources have been successfully allocated to your job your are still located on the submit host. Applications started now will not run within the allocation! bash-4.2$ salloc --nodes = 1 --ntasks-per-node = 4 --mem-per-cpu = 2G --time = 01 :00:00 salloc: Pending job allocation 63752579 salloc: job 63752579 queued and waiting for resources salloc: job 63752579 has been allocated resources salloc: Granted job allocation 63752579 bash-4.2$ hostname submit01.ubelix.unibe.ch","title":"Requesting resources"},{"location":"slurm/interactive-jobs.html#use-an-allocation","text":"Use srun [options] \u2013jobid= to start a job step under an existing job/allocation. bash-4.2$ srun -n1 --jobid = 63752579 hostname knode02 To work interactively on an allocated compute node: bash-4.2$ srun -n1 --jobid = 63752579 --pty bash bash-4.2$ hostname knode02","title":"Use an allocation"},{"location":"slurm/interactive-jobs.html#x11-forwarding","text":"Requirements You must login to UBELIX with X11 forwarding enabled: ssh -X @submit.unibe.ch . Make this the default with ForwardX11 yes in ~/.ssh/config . Password-less communication between all nodes within UBELIX. In order to make this possible, generate a new SSH key (without passphrase) on the login node (submit) and add the public key to ~/.ssh/authorized_keys : ssh-keygen -t rsa -b 4096 cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys A X-Server on your local workstation, e.g. MAC: Xquartz (X11 no longer included in macOS) Windows: MobaXTerm or Xming DO NOT reuse an existing SSH key for this purpose, i.e. do not copy an existing private key from your local machine to UBELIX. With all the requirements in place you can now submit an interactive job and export an X11 display on the allocated compute node, e.g: srun -n 1 --pty --x11 xterm You can also use X11 forwarding with non interactive jobs adding the option #SBATCH --x11 in your job script and using again srun --x11 to launch your application.","title":"X11 Forwarding"},{"location":"slurm/investigating-job-failure.html","text":"Investigating a Job Failure Description This page provides some useful information on investigating a job failure. It is important to collect error/output messages either by writing such information to the default location or by specifying specific locations using the \u2013error/\u2013output option. Do not redirect the error/output stream to /dev/null unless you know what you are doing. Error and output messages are the starting point for investigating a job failure. A job may fail due to a hardware failure on a node involved in the computation, a failure of a Slurm daemon, exceeding a resource limit, or a software specific error. The most common causes are exceeding resource limits and software-specific errors which we will discuss here. Exceeding Resource Limits Each partition limits the maximal allowed runtime of a job and provides default values for the estimated job runtime and memory usage per core. A job should request appropriate values for those resources using the \u2013time and \u2013mem-per-core options. A job is killed if one of these limits is exceeded. In both cases, the error file provides appropriate information: Time limit: ( ... ) slurmstepd: error: *** JOB 41239 ON fnode01 CANCELLED AT 2016 -11-30T11:22:57 DUE TO TIME LIMIT *** ( ... ) `````` Memory limit: ``` Bash ( ... ) slurmstepd: error: Job 41176 exceeded memory limit ( 3940736 > 2068480 ) , being killed slurmstepd: error: Exceeded job memory limit slurmstepd: error: *** JOB 41176 ON fnode01 CANCELLED AT 2016 -11-30T10:21:37 *** ( ... ) Software Errors The exit code of a job is captured by Slurm and saved as part of the job record. For sbatch jobs the exit code of the batch script is captured. For srun, the exit code will be the return value of the executed command. Any non-zero exit code is considered a job failure, and results in job state of FAILED. When a signal was responsible for a job/step termination, the signal number will also be captured, and displayed after the exit code (separated by a colon). Depending on the execution order of the commands in the batch script, it is possible that a specific command fails but the batch script will return zero indicating success. Consider the following simplified example: fail.R var<-sq ( 1 ,1000000000 ) job.sbatch #!/bin/bash # Slurm options #SBATCH --mail-user=nico.faerber@id.unibe.ch #SBATCH --mail-type=begin,end,fail #SBATCH --job-name=\"Simple Example\" #SBATCH --time=00:05:00 #SBATCH --mem-per-cpu=2G # Put your code below this line R CMD BATCH --vanilla fail.R echo \"Script finished\" The exit code and state wrongly indicates that the job finished successfully: $ sbatch job_slurm.sh Submitted batch job 41585 $ sacct -j 41585 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 41585 Simple E + all id 1 COMPLETED 0 :0 41585 .batch batch id 1 COMPLETED 0 :0 Only the R-specific output file shows the error: fail.Rout ( ... ) > var<-sq ( 1 ,1000000000 ) Error: could not find function \"sq\" Execution halted You can bypass this problem by exiting with a proper exit code as soon as the command failed: jobsbatch #!/bin/bash # Slurm options #SBATCH --mail-user=nico.faerber@id.unibe.ch #SBATCH --mail-type=begin,end,fail #SBATCH --job-name=\"Simple Example\" #SBATCH --time=00:05:00 #SBATCH --mem-per-cpu=2G # Put your code below this line R CMD BATCH --vanilla fail.R || exit 91 echo \"Script finished\" Now, the exit code and state matches the true outcome: $ sbatch job_slurm.sh Submitted batch job 41925 $ sacct -j 41925 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 41925 Simple E + all id 1 FAILED 91 :0 41925 .batch batch id 1 FAILED 91 :0 Always check application-specifc output files for error messages.","title":"Investigating a Job Failure"},{"location":"slurm/investigating-job-failure.html#investigating-a-job-failure","text":"","title":"Investigating a Job Failure"},{"location":"slurm/investigating-job-failure.html#description","text":"This page provides some useful information on investigating a job failure. It is important to collect error/output messages either by writing such information to the default location or by specifying specific locations using the \u2013error/\u2013output option. Do not redirect the error/output stream to /dev/null unless you know what you are doing. Error and output messages are the starting point for investigating a job failure. A job may fail due to a hardware failure on a node involved in the computation, a failure of a Slurm daemon, exceeding a resource limit, or a software specific error. The most common causes are exceeding resource limits and software-specific errors which we will discuss here.","title":"Description"},{"location":"slurm/investigating-job-failure.html#exceeding-resource-limits","text":"Each partition limits the maximal allowed runtime of a job and provides default values for the estimated job runtime and memory usage per core. A job should request appropriate values for those resources using the \u2013time and \u2013mem-per-core options. A job is killed if one of these limits is exceeded. In both cases, the error file provides appropriate information: Time limit: ( ... ) slurmstepd: error: *** JOB 41239 ON fnode01 CANCELLED AT 2016 -11-30T11:22:57 DUE TO TIME LIMIT *** ( ... ) `````` Memory limit: ``` Bash ( ... ) slurmstepd: error: Job 41176 exceeded memory limit ( 3940736 > 2068480 ) , being killed slurmstepd: error: Exceeded job memory limit slurmstepd: error: *** JOB 41176 ON fnode01 CANCELLED AT 2016 -11-30T10:21:37 *** ( ... )","title":"Exceeding Resource Limits"},{"location":"slurm/investigating-job-failure.html#software-errors","text":"The exit code of a job is captured by Slurm and saved as part of the job record. For sbatch jobs the exit code of the batch script is captured. For srun, the exit code will be the return value of the executed command. Any non-zero exit code is considered a job failure, and results in job state of FAILED. When a signal was responsible for a job/step termination, the signal number will also be captured, and displayed after the exit code (separated by a colon). Depending on the execution order of the commands in the batch script, it is possible that a specific command fails but the batch script will return zero indicating success. Consider the following simplified example: fail.R var<-sq ( 1 ,1000000000 ) job.sbatch #!/bin/bash # Slurm options #SBATCH --mail-user=nico.faerber@id.unibe.ch #SBATCH --mail-type=begin,end,fail #SBATCH --job-name=\"Simple Example\" #SBATCH --time=00:05:00 #SBATCH --mem-per-cpu=2G # Put your code below this line R CMD BATCH --vanilla fail.R echo \"Script finished\" The exit code and state wrongly indicates that the job finished successfully: $ sbatch job_slurm.sh Submitted batch job 41585 $ sacct -j 41585 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 41585 Simple E + all id 1 COMPLETED 0 :0 41585 .batch batch id 1 COMPLETED 0 :0 Only the R-specific output file shows the error: fail.Rout ( ... ) > var<-sq ( 1 ,1000000000 ) Error: could not find function \"sq\" Execution halted You can bypass this problem by exiting with a proper exit code as soon as the command failed: jobsbatch #!/bin/bash # Slurm options #SBATCH --mail-user=nico.faerber@id.unibe.ch #SBATCH --mail-type=begin,end,fail #SBATCH --job-name=\"Simple Example\" #SBATCH --time=00:05:00 #SBATCH --mem-per-cpu=2G # Put your code below this line R CMD BATCH --vanilla fail.R || exit 91 echo \"Script finished\" Now, the exit code and state matches the true outcome: $ sbatch job_slurm.sh Submitted batch job 41925 $ sacct -j 41925 JobID JobName Partition Account AllocCPUS State ExitCode ------------ ---------- ---------- ---------- ---------- ---------- -------- 41925 Simple E + all id 1 FAILED 91 :0 41925 .batch batch id 1 FAILED 91 :0 Always check application-specifc output files for error messages.","title":"Software Errors"},{"location":"slurm/monitoring-jobs.html","text":"Monitoring Jobs Description This page provides information about monitoring user jobs. Different Slurm commands provide information about jobs/job steps on different levels. The command squeue provides high-level information about jobs in the Slurm scheduling queue (state information, allocated resources, runtime, \u2026). The command sstat provides detailed usage information about running jobs, and sacct provides accounting information about active and completed (past) jobs. The command scontrol provides even more detailed information about jobs and job steps. The output format of most commands is highly configurable to your needs. Look for the \u2013format or \u2013Format options. Most command options support a short form as well as a long form (e.g. -u , and \u2013user= ). Because few options only support the long form, we will consistently use the long form throughout this documentation. squeue Use the squeue command to get a high-level overview of all active (running and pending) jobs in the cluster. Syntax squeue [ options ] Common options --user = <user [ ,user [ ,... ]] > Request jobs from a comma separated list of users. --jobs = <job_id [ ,job_id [ ,... ]] > Request specific jobs to be displayed --partition = <part [ ,part [ ,... ]] > Request jobs to be displayed from a comma separated list of partitions --states = <state [ ,state [ ,... ]] > Display jobs in specific states. Comma separated list or \"all\" . Default: \"PD,R,CG\" The default output format is as follows: JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) where JOBID Job or step ID. For array jobs, the job ID format will be of the form <job_id>_<index> PARTITION Partition of the job/step NAME Name of the job/step USER Owner of the job/step ST State of the job/step. See below for a description of the most common states TIME Time used by the job/step. Format is days-hours:minutes:seconds ( days,hours only printed as needed ) NODES Number of nodes allocated to the job or the minimum amount of nodes required by a pending job NODELIST ( REASON ) For pending jobs: Reason why pending. For failed jobs: Reason why failed. For all other job states: List of allocated nodes. See below for a list of the most common reason codes You can easily tailor the output format of squeue to your own needs. Use the \u2013format (-o) or \u2013Format (-O) options to request a comma separated list of job information to be displayed. See the man page for more information: man squeue Job States During its lifetime, a job passes through several states. The most common states are PENDING, RUNNING, SUSPENDED, COMPLETING, and COMPLETED. PD Pending. Job is waiting for resource allocation R Running. Job has an allocation and is running S Suspended. Execution has been suspended and resources have been released for other jobs CA Cancelled. Job was explicitly cancelled by the user or the system administrator CG Completing. Job is in the process of completing. Some processes on some nodes may still be active CD Completed. Job has terminated all processes on all nodes with an exit code of zero F Failed. Job has terminated with non-zero exit code or other failure condition Why is my job still pending? The REASON column of the squeue output gives you a hint why your job is not running. (Resources) The job is waiting for resources to become available so that the jobs resource request can be fulfilled. (Priority) The job is not allowed to run because at least one higher prioritized job is waiting for resources. (Dependency) The job is waiting for another job to finish first (\u2013dependency=\u2026 option). (DependencyNeverSatisfied) The job is waiting for a dependency that can never be satisfied. Such a job will remain pending forever. Please cancel such jobs. (QOSMaxCpuPerUserLimit) The job is not allowed to start because your currently running jobs consume all allowed CPU resources for your user in a specific partition. Wait for jobs to finish. (AssocGrpCpuLimit) dito. (AssocGrpJobsLimit) The job is not allowed to start because you have reached the maximum of allowed running jobs for your user in a specific partition. Wait for jobs to finish. (ReqNodeNotAvail, UnavailableNodes:\u2026) Some node required by the job is currently not available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Most probably there is an active reservation for all nodes due to an upcoming maintenance downtime and your job is not able to finish before the start of the downtime. Another reason why you should specify the duration of a job (\u2013time) as accurately as possible. Your job will start after the downtime has finished. You can list all active reservations using scontrol show reservation . Why can\u2019t I submit further jobs? sbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user\u2019s size and/or time limits) \u2026 means that you have reached the maximum of allowed jobs to be submitted to a specific partition. Examples List all currently running jobs of user foo: squeue --user = foo --states = PD,R List all currently running jobs of user foo in partition bar : squeue --user = foo --partition = bar --states = R scontrol Use the scontrol command to show more detailed information about a job Syntax scontrol [ options ] [ command ] Examples Show detailed information about job with ID 500: scontrol show jobid 500 Show even more detailed information about job with ID 500 (including the jobscript): scontrol -dd show jobid 500 sacct Use the sacct command to query information about past jobs Syntax sacct [ options ] Common options --endtime = end_time Select jobs in any state before the specified time. --starttime = start_time Select jobs in any state after the specified time. --state = state [ ,state [ ,... ]] Select jobs based on their state during the time period given. By default, the start and end time will be the current time when the --state option is specified, and hence only currently running jobs will be displayed.","title":"Monitoring Jobs"},{"location":"slurm/monitoring-jobs.html#monitoring-jobs","text":"","title":"Monitoring Jobs"},{"location":"slurm/monitoring-jobs.html#description","text":"This page provides information about monitoring user jobs. Different Slurm commands provide information about jobs/job steps on different levels. The command squeue provides high-level information about jobs in the Slurm scheduling queue (state information, allocated resources, runtime, \u2026). The command sstat provides detailed usage information about running jobs, and sacct provides accounting information about active and completed (past) jobs. The command scontrol provides even more detailed information about jobs and job steps. The output format of most commands is highly configurable to your needs. Look for the \u2013format or \u2013Format options. Most command options support a short form as well as a long form (e.g. -u , and \u2013user= ). Because few options only support the long form, we will consistently use the long form throughout this documentation.","title":"Description"},{"location":"slurm/monitoring-jobs.html#squeue","text":"Use the squeue command to get a high-level overview of all active (running and pending) jobs in the cluster. Syntax squeue [ options ] Common options --user = <user [ ,user [ ,... ]] > Request jobs from a comma separated list of users. --jobs = <job_id [ ,job_id [ ,... ]] > Request specific jobs to be displayed --partition = <part [ ,part [ ,... ]] > Request jobs to be displayed from a comma separated list of partitions --states = <state [ ,state [ ,... ]] > Display jobs in specific states. Comma separated list or \"all\" . Default: \"PD,R,CG\" The default output format is as follows: JOBID PARTITION NAME USER ST TIME NODES NODELIST ( REASON ) where JOBID Job or step ID. For array jobs, the job ID format will be of the form <job_id>_<index> PARTITION Partition of the job/step NAME Name of the job/step USER Owner of the job/step ST State of the job/step. See below for a description of the most common states TIME Time used by the job/step. Format is days-hours:minutes:seconds ( days,hours only printed as needed ) NODES Number of nodes allocated to the job or the minimum amount of nodes required by a pending job NODELIST ( REASON ) For pending jobs: Reason why pending. For failed jobs: Reason why failed. For all other job states: List of allocated nodes. See below for a list of the most common reason codes You can easily tailor the output format of squeue to your own needs. Use the \u2013format (-o) or \u2013Format (-O) options to request a comma separated list of job information to be displayed. See the man page for more information: man squeue","title":"squeue"},{"location":"slurm/monitoring-jobs.html#job-states","text":"During its lifetime, a job passes through several states. The most common states are PENDING, RUNNING, SUSPENDED, COMPLETING, and COMPLETED. PD Pending. Job is waiting for resource allocation R Running. Job has an allocation and is running S Suspended. Execution has been suspended and resources have been released for other jobs CA Cancelled. Job was explicitly cancelled by the user or the system administrator CG Completing. Job is in the process of completing. Some processes on some nodes may still be active CD Completed. Job has terminated all processes on all nodes with an exit code of zero F Failed. Job has terminated with non-zero exit code or other failure condition","title":"Job States"},{"location":"slurm/monitoring-jobs.html#why-is-my-job-still-pending","text":"The REASON column of the squeue output gives you a hint why your job is not running. (Resources) The job is waiting for resources to become available so that the jobs resource request can be fulfilled. (Priority) The job is not allowed to run because at least one higher prioritized job is waiting for resources. (Dependency) The job is waiting for another job to finish first (\u2013dependency=\u2026 option). (DependencyNeverSatisfied) The job is waiting for a dependency that can never be satisfied. Such a job will remain pending forever. Please cancel such jobs. (QOSMaxCpuPerUserLimit) The job is not allowed to start because your currently running jobs consume all allowed CPU resources for your user in a specific partition. Wait for jobs to finish. (AssocGrpCpuLimit) dito. (AssocGrpJobsLimit) The job is not allowed to start because you have reached the maximum of allowed running jobs for your user in a specific partition. Wait for jobs to finish. (ReqNodeNotAvail, UnavailableNodes:\u2026) Some node required by the job is currently not available. The node may currently be in use, reserved for another job, in an advanced reservation, DOWN, DRAINED, or not responding. Most probably there is an active reservation for all nodes due to an upcoming maintenance downtime and your job is not able to finish before the start of the downtime. Another reason why you should specify the duration of a job (\u2013time) as accurately as possible. Your job will start after the downtime has finished. You can list all active reservations using scontrol show reservation .","title":"Why is my job still pending?"},{"location":"slurm/monitoring-jobs.html#why-cant-i-submit-further-jobs","text":"sbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user\u2019s size and/or time limits) \u2026 means that you have reached the maximum of allowed jobs to be submitted to a specific partition.","title":"Why can't I submit further jobs?"},{"location":"slurm/monitoring-jobs.html#examples","text":"List all currently running jobs of user foo: squeue --user = foo --states = PD,R List all currently running jobs of user foo in partition bar : squeue --user = foo --partition = bar --states = R","title":"Examples"},{"location":"slurm/monitoring-jobs.html#scontrol","text":"Use the scontrol command to show more detailed information about a job Syntax scontrol [ options ] [ command ]","title":"scontrol"},{"location":"slurm/monitoring-jobs.html#examples_1","text":"Show detailed information about job with ID 500: scontrol show jobid 500 Show even more detailed information about job with ID 500 (including the jobscript): scontrol -dd show jobid 500","title":"Examples"},{"location":"slurm/monitoring-jobs.html#sacct","text":"Use the sacct command to query information about past jobs Syntax sacct [ options ] Common options --endtime = end_time Select jobs in any state before the specified time. --starttime = start_time Select jobs in any state after the specified time. --state = state [ ,state [ ,... ]] Select jobs based on their state during the time period given. By default, the start and end time will be the current time when the --state option is specified, and hence only currently running jobs will be displayed.","title":"sacct"},{"location":"slurm/partitions.html","text":"SLURM partition and QOS Description UBELIX provides different CPU and GPU architectures. Furthermore, we provide job different queues for different job priorities and limitations. restructuring 04.05.2021 With the maintainace 04.05.2021 we restructured the Slurm partition to: more effcient resource usage: All users get access to all resources. Investors have priviledeged access general access to GPUs without preempting resource sharing (fair share) based on research groups using Workspaces, instead of institute based sharing simpler partition design There are three main control mechanisms to specify queues and resources: Partitions and Quality of Service ( QoS ) --gres to select the targeted GPU architecture, see GPUs Partitions There are the current 3 partitions, epyc2 is default: Partition job type CPU / GPU node / GPU memory local Scratch epyc2 single and multi-core AMD Epyc2 2x64 cores 1TB 1TB bdw full nodes only (x*20cores) Intel Broadwell 2x10 cores 156GB 1TB gpu GPU (8 GPUs per node, varying CPUs) Nvidia GTX 1080 Ti Nvidia RTX 2080 Ti Nvidia RTX 3090 Nvidia Tesla P100 11GB 11GB 12GB 24GB 800GB 2x960GB 1.92TB 800GB The current usage can be listed on the UBELIX status page QoS Within these partitions, QoS are used to distinguish different job limits. In each partition there is a default QoS ( bold below). Each QoS has specific limits: Partition QoS time limit cores/node/GPU limit max jobs epyc2 job_epyc2 4 days 512 cores array jobs up to 10000 tasks job_epyc2_debug 20 min 20 cores 1 job_epyc2_long 15 days 64 cores 50 job_epyc2_short 6h 10 nodes 50 bdw job_bdw 24 h 50 nodes 300 bdw_debug 20 min 2 nodes 1 bdw_short 6 h 2 nodes 10 gpu job_gpu 24 h 6x GTX 1080 Ti 2x RTX 2080 Ti 1x RTX 3090 1x Tesla P100 10 job_gpu_debug 20 min 1 GPU 1 job_gpu_preempt 24 h 12x GTX 1080 Ti 4x RTX 2080 Ti 4x RTX 3090 4x Tesla P100 24 The QoS job_epyc2_short and job_gpu_preempt have access extended resources. In case of job_gpu_preempt, these jobs will be pre-empted if not enough resources for the investors. Thus a job can be submitted to the gpu partition using a RTX3090 and allow pre-emption: sbatch --partition = gpu --qos = job_gpu_preempt --gres = gpu:rtx3090 myjob.sh Please see for more details: GPUs Investor QoS Investors get elvated priviledges on additional additional resources. These priviledges are managed using investor QoS in investor partitions . The membership to these is managed on basis of HPC Workspaces, see Workspace Management .","title":"Partitions / QoS"},{"location":"slurm/partitions.html#slurm-partition-and-qos","text":"","title":"SLURM partition and QOS"},{"location":"slurm/partitions.html#description","text":"UBELIX provides different CPU and GPU architectures. Furthermore, we provide job different queues for different job priorities and limitations. restructuring 04.05.2021 With the maintainace 04.05.2021 we restructured the Slurm partition to: more effcient resource usage: All users get access to all resources. Investors have priviledeged access general access to GPUs without preempting resource sharing (fair share) based on research groups using Workspaces, instead of institute based sharing simpler partition design There are three main control mechanisms to specify queues and resources: Partitions and Quality of Service ( QoS ) --gres to select the targeted GPU architecture, see GPUs","title":"Description"},{"location":"slurm/partitions.html#partitions","text":"There are the current 3 partitions, epyc2 is default: Partition job type CPU / GPU node / GPU memory local Scratch epyc2 single and multi-core AMD Epyc2 2x64 cores 1TB 1TB bdw full nodes only (x*20cores) Intel Broadwell 2x10 cores 156GB 1TB gpu GPU (8 GPUs per node, varying CPUs) Nvidia GTX 1080 Ti Nvidia RTX 2080 Ti Nvidia RTX 3090 Nvidia Tesla P100 11GB 11GB 12GB 24GB 800GB 2x960GB 1.92TB 800GB The current usage can be listed on the UBELIX status page","title":"Partitions"},{"location":"slurm/partitions.html#qos","text":"Within these partitions, QoS are used to distinguish different job limits. In each partition there is a default QoS ( bold below). Each QoS has specific limits: Partition QoS time limit cores/node/GPU limit max jobs epyc2 job_epyc2 4 days 512 cores array jobs up to 10000 tasks job_epyc2_debug 20 min 20 cores 1 job_epyc2_long 15 days 64 cores 50 job_epyc2_short 6h 10 nodes 50 bdw job_bdw 24 h 50 nodes 300 bdw_debug 20 min 2 nodes 1 bdw_short 6 h 2 nodes 10 gpu job_gpu 24 h 6x GTX 1080 Ti 2x RTX 2080 Ti 1x RTX 3090 1x Tesla P100 10 job_gpu_debug 20 min 1 GPU 1 job_gpu_preempt 24 h 12x GTX 1080 Ti 4x RTX 2080 Ti 4x RTX 3090 4x Tesla P100 24 The QoS job_epyc2_short and job_gpu_preempt have access extended resources. In case of job_gpu_preempt, these jobs will be pre-empted if not enough resources for the investors. Thus a job can be submitted to the gpu partition using a RTX3090 and allow pre-emption: sbatch --partition = gpu --qos = job_gpu_preempt --gres = gpu:rtx3090 myjob.sh Please see for more details: GPUs","title":"QoS"},{"location":"slurm/partitions.html#investor-qos","text":"Investors get elvated priviledges on additional additional resources. These priviledges are managed using investor QoS in investor partitions . The membership to these is managed on basis of HPC Workspaces, see Workspace Management .","title":"Investor QoS"},{"location":"slurm/submission.html","text":"Job Submission Description This section describes the interaction with the resource manager. The subchapters contain information about submitting jobs to the cluster, monitoring active jobs and retrieving useful information about resource usage. A cluster is a set of connected computers that work together to solve computational tasks (user jobs) and presents itself to the user as a single system. For the resources of a cluster (e.g. CPUs, GPUs, memory) to be used efficiently, a resource manager (also called workload manager or batch-queuing system) is vital. While there are many different resource managers available, the resource manager of choice on UBELIX is SLURM . After submitting a job to the cluster, SLURM will try to fulfill the job\u2019s resource request by allocating resources to the job. If the requested resources are already available, the job can start immediately. Otherwise, the start of the job is delayed (pending) until enough resources are available. SLURM allows you to monitor active (pending, running) jobs and to retrieve statistics about finished jobs e.g. (peak CPU usage). The subchapters describe individual aspects of SLURM. This page describes the job submission process with Slurm. keep output It is important to collect error/output messages either by writing such information to the default location or by specifying specific locations using the --error / -\u2013output option. Do not redirect the error/output stream to /dev/null unless you know what you are doing. Error and output messages are the starting point for investigating a job failure. job series Submit series of jobs (collection of similar jobs) as array jobs instead of one by one. This is crucial for backfilling performance and hence job throughput. instead of submitting the same job repeatedly. See Array jobs Simple Example Batch sbmission script, jobs.sh : #!/bin/bash #SBATCH --job-name=\"First example\" #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=1G # Your code below this line module load Python srun python3 script.py Submit the job script: sbatch job.sh Submitted batch job 30215045 See below for more examples Resource Allocation Every job submission starts with a resources allocation (nodes, cores, memory). An allocation is valid for a specific amount of time, and can be created using the salloc , sbatch or srun commands. Whereas salloc and sbatch only create resource allocations, srun launches parallel tasks within such a resource allocation, or implicitly creates an allocation if not started within one. The usual procedure is to combine resource requests and task execution (job steps) in a single batch script (job script) and then submit the script using the sbatch command. Most command options support a short form as well as a long form (e.g. -u <username> , and --user=<username> ). Because few options only support the long form, we will consistently use the long form throughout this documentation. Some options have default values if not specified: The --time option has partition-specific default values (see scontrol show partition <partname> ). The --mem-per-cpu option has a global default value of 2048MB. The default partition is epyc2 . To select another partition one must use the --partition option, e.g. --partition=gpu . sbatch The sbatch command is used to submit a job script for later execution. It is the most common way to submit a job to the cluster due to its reusability. Slurm options are usually embedded in a job script prefixed by #SBATCH directives. Slurm options specified as command line options overwrite corresponding options embedded in the job script Syntax sbatch [ options ] script [ args... ] Job Script Usually a job script consists of two parts. The first part is optional but highly recommended: Slurm-specific options used by the scheduler to manage the resources (e.g. memory) and configure the job environment Job-specific shell commands The job script acts as a wrapper for your actual job. Command-line options can still be used to overwrite embedded options. Although you can specify all Slurm options on the command-line, we encourage you, for clarity and reusability, to embed Slurm options in the job script Options Option Description Example --job-name Specify a job name --job-name=\"Simple Matlab\" --time Expected runtime of the job. Format: dd-hh:mm:ss --time=12:00:00 --time=2-06:00:00 --ntasks Number of tasks (processes). Used for MPI jobs that may run distributed on multiple compute nodes --ntasks=4 --nodes Request a certain number of nodes --nodes=2 --ntasks-per-node Specifies how many tasks will run on each allocated node. Meant to be used with --nodes . If used with the --ntasks option, the --ntasks option will take precedence and the --ntasks-per-node will be treated as a maximum count of tasks per node. --ntasks-per-node=2 --cpus-per-task Number of CPUs per task (threads). Used for shared memory jobs that run locally on a single compute node. Default is 1 --cpus-per-task=4 --mem-per-cpu Minimum memory required per allocated CPU in megabytes. Different units can be specified using the suffix [K|M|G]. Default 2048 MB --mem-per-cpu=2G --output Redirect standard output . All directories specified in the path must exist before the job starts! By default stderr and stdout are merged into a file slurm-%j.out , where %j is the job allocation number. --output=myCal_%j.out --error Redirect standard error . All directories specified in the path must exist before the job starts! By default stderr and stdout are merged into a file slurm-%j.out , where %j is the job allocation number. --output=myCal_%j.err --partition Select a different partition with different hardware. See Partition/QoS page . Default: epyc2 --partition=bdw --partition=gpu --qos Specify \u201cQuality of Service\u201d. This can be used to change job limits, e.g. for long jobs or short jobs with large resources. See Partition/QoS page --tmp Specify the amount of disk space that must be available on the compute node(s). The local scratch space for the job is referenced by the variable $TMPDIR . Default units are megabytes. Different units can be specified using the suffix [K|M|G|T]. --tmp=8G --mail-user Mail address to contact job owner. Must be a valid email address, if used! --mail-user=foo.bar@unibe.ch --mail-type When to notify a job owner: none , all , begin , end , fail , requeue , array_tasks --mail-type=end,fail --array Submit an array job. Specify the used indices and use \u201c%\u201d to specify the max number of tasks allowed to run concurrently. --array=1,4,16-32:4 --array=1-100%20 --workdir Set the current working directory. All relative paths used in the job script are relative to this directory. Default: The directory from where the sbatch command was executed --dependency Defer the start of this job until the specified dependencies have been satisfied. See man sbatch for a description of all valid dependency types --dependency=afterok:11908 --immediate Only submit the job if all requested resources are immediately available --exclusive Use the compute node(s) exclusively, i.e. do not share nodes with other jobs. CAUTION: Only use this option if you are an experienced user, and you really understand the implications of this feature. If used improperly, the use of this option can lead to a massive waste of computational resources --test-only Validate the batch script and return the estimated start time considering the current cluster state --account Specifies account to charge. Please use Workspace module to select Workspace account. Regular users don\u2019t need to specify this option. salloc The salloc command is used to allocate resources (e.g. nodes), possibly with a set of constraints (e.g. number of processor per node) for later utilization. It is typically used to allocate resources and spawn a shell, in which the srun command is used to launch parallel tasks. Syntax salloc [ options ] [ <command> [ args... ]] Example bash$ salloc -N 2 -t 10 salloc: Granted job allocation 247 bash$ module load foss bash$ srun ./mpi_hello_world Hello, World. I am 1 of 2 running on knlnode03.ubelix.unibe.ch Hello, World. I am 0 of 2 running on knlnode02.ubelix.unibe.ch bash$ exit salloc: Relinquishing job allocation 247 srun The srun command creates job steps. One or multiple srun invocations are usually used from within an existing resource allocation. Thereby, a job step can utilize all resources allocated to the job, or utilize only a subset of the resource allocation. Multiple job steps can run sequentially in the order defined in the batch script or run in parallel, but can together never utilize more resources than provided by the allocation. Do not submit a job script using srun . Embedded Slurm options ( #SBATCH ) are not parsed by srun . Syntax srun [options] executable [args...] When do I use srun in my job script? Use srun in your job script for all main executables, especially if these are: MPI applications multiple job tasks (serial or parallel jobs) simultaneously within an allocation Example Run MPI task: #!/bin/bash #SBATCH --job-name=\"Open MPI example\" #SBATCH --nodes=2 #SBATCH --ntasks-per-node=20 #SBATCH --mem-per-cpu=2G #SBATCH --time=06:00:00 # Your code below this line module load foss srun ./mpi_app.exe Run two jobs simultaneously: #!/bin/bash #SBATCH --job-name=\"Open MPI example\" #SBATCH --ntasks=2 #SBATCH --cpus-per-task=4 # Your code below this line # run 2 threaded applications side-by-side srun --tasks = 1 --cpus-per-task = 2 ./app1 inp1.dat & srun --tasks = 1 --cpus-per-task = 2 ./app2 inp2.dat & wait # wait: Wait for both background commands to finish. This is important when running bash commands in the background (using &)! Otherwise, the job ends immediately. Please run series of similar tasks as job array. See Array Jobs Requesting a Partition / QoS (Queue) Per default jobs are submitted to the epyc2 partition and the default QoS job_epyc2 . The partition option can be used to request different hardware, e.b. gpu partition. And the QoS can be used to run in a specific queue, e.g. job_gpu_debug : #SBATCH --partition=gpu --qos=job_gpu_debug See Partitions / QoS for a list of available partitions and QoS and its specifications. Accounts By default a user has a \u201cprivate\u201d account. When belonging to a Workspace your private account gets deactivated and you can submit with the Workspace account. We strongly suggest to use the Workspace module ( module load Workspace ), which automatically sets the Workspace account for you. If really necessary, the can be selected by the --account option. If a wrong account/partition combination is requested, you will experience the following error message: sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified If you did not specified --account , and belong to a Workspace, please load the Workspace module fist. Parallel Jobs A parallel job requires multiple compute cores. These could be within one node or across the machine in multiple nodes. We distinguish following types: shared memory jobs : SMP, parallel jobs that run on a single compute node. The executable is called once. Within the execution (OMP) threads are spawned and merged. #SBATCH --ntasks=1 # default value #SBATCH --cpus-per-task=4 MPI jobs : parallel jobs that may be distributed over multiple compute nodes. Each task starts the executable. Within the application different workflows need to be defined for the different tasks. The tasks can communicate using Message Passing Interface (MPI). A job with 40 tasks: #SBATCH --nodes=2 #SBATCH --ntasks-per-node=20 hybrid : jobs using a combination of MPI tasks and (OMP) threads. #SBATCH --nodes=2 #SBATCH --ntasks-per-node=5 #SBATCH --cpus-per-task=4 The requested node,task, and CPU resources must match! For example, you cannot request one node ( --nodes=1 ) and more tasks ( --ntasks-per-node ) than CPU cores are available on a single node in the partition. In such a case you will experience an error message: sbatch: error: Batch job submission failed: Requested node configuration is not available. parallel launcher Parallel applications, esp. MPI, need a launcher to setup the environment. We strongly suggest to use srun instead of mpirun. Environment Variables Slurm sets various environment variables available in the context of the job script. Some are set based on the requested resources for the job. Environment Variable Set By Option Description SLURM_JOB_NAME --job-name Name of the job SLURM_ARRAY_JOB_ID ID of your job SLURM_ARRAY_TASK_ID --array ID of the current array task SLURM_ARRAY_TASK_MAX --array Job array\u2019s maximum ID (index) number SLURM_ARRAY_TASK_MIN --array Job array\u2019s minimum ID (index) number SLURM_ARRAY_TASK_STEP --array Job array\u2019s index step size SLURM_NTASKS --ntasks Same as -n , --ntasks SLURM_NTASKS_PER_NODE --ntasks-per-node Number of tasks requested per node. Only set if the --ntasks-per-node option is specified SLURM_CPUS_PER_TASK --cpus-per-task Number of cpus requested per task. Only set if the --cpus-per-task option is specified TMPDIR References the disk space for the job on the local scratch For the full list, see man sbatch Job Examples Sequential Job Running a serial job with email notification in case of error (1 task is default value): #!/bin/bash #SBATCH --mail-user=foo.bar@baz.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --job-name=\"Serial Job\" #SBATCH --time=04:00:00 # Your code below this line echo \"I'm on host: $HOSTNAME \" Parallel Jobs Shared Memory Jobs (e.g. OpenMP) SMP parallelization is based upon dynamically created threads (fork and join) that share memory on a single node. The key request is --cpus-per-task . To run N threads in parallel, we request N CPUs on the node ( --cpus-per-task=N ). #!/bin/bash #SBATCH --mail-user=foo.bar@baz.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --job-name=\"SMP Job\" #SBATCH --mem-per-cpu=2G #SBATCH --cpus-per-task=16 #SBATCH --time=04:00:00 # Your code below this line srun ./my_binary MPI Jobs (e.g. Open MPI) MPI parallelization is based upon processes (local or distributed) that communicate by passing messages. Since they don\u2019t rely on shared memory those processes can be distributed among several compute nodes. Use the option --ntasks to request a certain number of tasks (processes) that can be distributed over multiple nodes: #!/bin/bash #SBATCH --mail-user=foo.bar@baz.unibe.ch #SBATCH --mail-type=end #SBATCH --job-name=\"MPI Job\" #SBATCH --mem-per-cpu=2G #SBATCH --ntasks=8 #SBATCH --time=04:00:00 # Your code below this line # First set the environment for using Open MPI module load foss srun ./my_binary On the \u2018bdw\u2019 partition you must use all CPUs provided by a node (20 CPUs). For example to run an OMPI job on 80 CPUs, do: #!/bin/bash #SBATCH --mail-user=foo.bar@baz.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --job-name=\"MPI Job\" #SBATCH --mem-per-cpu=2G #SBATCH --nodes=4 ## or --ntasks=80 #SBATCH --ntasks-per-node=20 #SBATCH --time=12:00:00 # Your code below this line module load foss srun ./my_binary Performance considerations Job Throughput It is crucial to specify a more or less accurate runtime for your job. Requesting too little will result in job abortion, while requesting too much will have a negative impact on job start time and job throughput: Firstly, jobs with a shorter runtime have a greater chance to benefit from being backfilled between long running jobs and may therefore start earlier if resources are scarce. Secondly, a short running job may still start when a scheduled downtime is getting closer while long running jobs won\u2019t start because they are not guaranteed to finish before the start of the downtime. It is crucial to request the correct amount of cores for your job. Requesting cores that your job cannot utilize is a waste of resources that could otherwise be allocated to other jobs. Hence, jobs that theoretically could run have to wait for the resources to become available. For potential consequences of requesting too less cores on job performance, see below. It is crucial to request the correct amount of memory for your job. Requesting too little memory will result in job abortion. Requesting too much memory is a waste of resources that could otherwise be allocated to other jobs. Job Performance/Runtime It is crucial to request the correct amount of cores for your job. For parallel jobs (shared memory, MPI, hybrid) requesting less cores than processes/threads are spawned by the job will lead to potentially overbooked compute nodes. This is because your job will nevertheless spawn the required number of processes/threads (use a certain number of cores) while to the scheduler it appears that some of the utilized resources are still available, and thus the scheduler will allocate those resources to other jobs. Although under certain circumstances it might make sense to share cores among multiple processes/threads, the above reasoning should be considered as a general guideline, especially for unexperienced user.","title":"Submitting jobs"},{"location":"slurm/submission.html#job-submission","text":"","title":"Job Submission"},{"location":"slurm/submission.html#description","text":"This section describes the interaction with the resource manager. The subchapters contain information about submitting jobs to the cluster, monitoring active jobs and retrieving useful information about resource usage. A cluster is a set of connected computers that work together to solve computational tasks (user jobs) and presents itself to the user as a single system. For the resources of a cluster (e.g. CPUs, GPUs, memory) to be used efficiently, a resource manager (also called workload manager or batch-queuing system) is vital. While there are many different resource managers available, the resource manager of choice on UBELIX is SLURM . After submitting a job to the cluster, SLURM will try to fulfill the job\u2019s resource request by allocating resources to the job. If the requested resources are already available, the job can start immediately. Otherwise, the start of the job is delayed (pending) until enough resources are available. SLURM allows you to monitor active (pending, running) jobs and to retrieve statistics about finished jobs e.g. (peak CPU usage). The subchapters describe individual aspects of SLURM. This page describes the job submission process with Slurm. keep output It is important to collect error/output messages either by writing such information to the default location or by specifying specific locations using the --error / -\u2013output option. Do not redirect the error/output stream to /dev/null unless you know what you are doing. Error and output messages are the starting point for investigating a job failure. job series Submit series of jobs (collection of similar jobs) as array jobs instead of one by one. This is crucial for backfilling performance and hence job throughput. instead of submitting the same job repeatedly. See Array jobs","title":"Description"},{"location":"slurm/submission.html#simple-example","text":"Batch sbmission script, jobs.sh : #!/bin/bash #SBATCH --job-name=\"First example\" #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=1G # Your code below this line module load Python srun python3 script.py Submit the job script: sbatch job.sh Submitted batch job 30215045 See below for more examples","title":"Simple Example"},{"location":"slurm/submission.html#resource-allocation","text":"Every job submission starts with a resources allocation (nodes, cores, memory). An allocation is valid for a specific amount of time, and can be created using the salloc , sbatch or srun commands. Whereas salloc and sbatch only create resource allocations, srun launches parallel tasks within such a resource allocation, or implicitly creates an allocation if not started within one. The usual procedure is to combine resource requests and task execution (job steps) in a single batch script (job script) and then submit the script using the sbatch command. Most command options support a short form as well as a long form (e.g. -u <username> , and --user=<username> ). Because few options only support the long form, we will consistently use the long form throughout this documentation. Some options have default values if not specified: The --time option has partition-specific default values (see scontrol show partition <partname> ). The --mem-per-cpu option has a global default value of 2048MB. The default partition is epyc2 . To select another partition one must use the --partition option, e.g. --partition=gpu .","title":"Resource Allocation"},{"location":"slurm/submission.html#sbatch","text":"The sbatch command is used to submit a job script for later execution. It is the most common way to submit a job to the cluster due to its reusability. Slurm options are usually embedded in a job script prefixed by #SBATCH directives. Slurm options specified as command line options overwrite corresponding options embedded in the job script Syntax sbatch [ options ] script [ args... ]","title":"sbatch"},{"location":"slurm/submission.html#job-script","text":"Usually a job script consists of two parts. The first part is optional but highly recommended: Slurm-specific options used by the scheduler to manage the resources (e.g. memory) and configure the job environment Job-specific shell commands The job script acts as a wrapper for your actual job. Command-line options can still be used to overwrite embedded options. Although you can specify all Slurm options on the command-line, we encourage you, for clarity and reusability, to embed Slurm options in the job script","title":"Job Script"},{"location":"slurm/submission.html#options","text":"Option Description Example --job-name Specify a job name --job-name=\"Simple Matlab\" --time Expected runtime of the job. Format: dd-hh:mm:ss --time=12:00:00 --time=2-06:00:00 --ntasks Number of tasks (processes). Used for MPI jobs that may run distributed on multiple compute nodes --ntasks=4 --nodes Request a certain number of nodes --nodes=2 --ntasks-per-node Specifies how many tasks will run on each allocated node. Meant to be used with --nodes . If used with the --ntasks option, the --ntasks option will take precedence and the --ntasks-per-node will be treated as a maximum count of tasks per node. --ntasks-per-node=2 --cpus-per-task Number of CPUs per task (threads). Used for shared memory jobs that run locally on a single compute node. Default is 1 --cpus-per-task=4 --mem-per-cpu Minimum memory required per allocated CPU in megabytes. Different units can be specified using the suffix [K|M|G]. Default 2048 MB --mem-per-cpu=2G --output Redirect standard output . All directories specified in the path must exist before the job starts! By default stderr and stdout are merged into a file slurm-%j.out , where %j is the job allocation number. --output=myCal_%j.out --error Redirect standard error . All directories specified in the path must exist before the job starts! By default stderr and stdout are merged into a file slurm-%j.out , where %j is the job allocation number. --output=myCal_%j.err --partition Select a different partition with different hardware. See Partition/QoS page . Default: epyc2 --partition=bdw --partition=gpu --qos Specify \u201cQuality of Service\u201d. This can be used to change job limits, e.g. for long jobs or short jobs with large resources. See Partition/QoS page --tmp Specify the amount of disk space that must be available on the compute node(s). The local scratch space for the job is referenced by the variable $TMPDIR . Default units are megabytes. Different units can be specified using the suffix [K|M|G|T]. --tmp=8G --mail-user Mail address to contact job owner. Must be a valid email address, if used! --mail-user=foo.bar@unibe.ch --mail-type When to notify a job owner: none , all , begin , end , fail , requeue , array_tasks --mail-type=end,fail --array Submit an array job. Specify the used indices and use \u201c%\u201d to specify the max number of tasks allowed to run concurrently. --array=1,4,16-32:4 --array=1-100%20 --workdir Set the current working directory. All relative paths used in the job script are relative to this directory. Default: The directory from where the sbatch command was executed --dependency Defer the start of this job until the specified dependencies have been satisfied. See man sbatch for a description of all valid dependency types --dependency=afterok:11908 --immediate Only submit the job if all requested resources are immediately available --exclusive Use the compute node(s) exclusively, i.e. do not share nodes with other jobs. CAUTION: Only use this option if you are an experienced user, and you really understand the implications of this feature. If used improperly, the use of this option can lead to a massive waste of computational resources --test-only Validate the batch script and return the estimated start time considering the current cluster state --account Specifies account to charge. Please use Workspace module to select Workspace account. Regular users don\u2019t need to specify this option.","title":"Options"},{"location":"slurm/submission.html#salloc","text":"The salloc command is used to allocate resources (e.g. nodes), possibly with a set of constraints (e.g. number of processor per node) for later utilization. It is typically used to allocate resources and spawn a shell, in which the srun command is used to launch parallel tasks. Syntax salloc [ options ] [ <command> [ args... ]] Example bash$ salloc -N 2 -t 10 salloc: Granted job allocation 247 bash$ module load foss bash$ srun ./mpi_hello_world Hello, World. I am 1 of 2 running on knlnode03.ubelix.unibe.ch Hello, World. I am 0 of 2 running on knlnode02.ubelix.unibe.ch bash$ exit salloc: Relinquishing job allocation 247","title":"salloc"},{"location":"slurm/submission.html#srun","text":"The srun command creates job steps. One or multiple srun invocations are usually used from within an existing resource allocation. Thereby, a job step can utilize all resources allocated to the job, or utilize only a subset of the resource allocation. Multiple job steps can run sequentially in the order defined in the batch script or run in parallel, but can together never utilize more resources than provided by the allocation. Do not submit a job script using srun . Embedded Slurm options ( #SBATCH ) are not parsed by srun . Syntax srun [options] executable [args...]","title":"srun"},{"location":"slurm/submission.html#when-do-i-use-srun-in-my-job-script","text":"Use srun in your job script for all main executables, especially if these are: MPI applications multiple job tasks (serial or parallel jobs) simultaneously within an allocation Example Run MPI task: #!/bin/bash #SBATCH --job-name=\"Open MPI example\" #SBATCH --nodes=2 #SBATCH --ntasks-per-node=20 #SBATCH --mem-per-cpu=2G #SBATCH --time=06:00:00 # Your code below this line module load foss srun ./mpi_app.exe Run two jobs simultaneously: #!/bin/bash #SBATCH --job-name=\"Open MPI example\" #SBATCH --ntasks=2 #SBATCH --cpus-per-task=4 # Your code below this line # run 2 threaded applications side-by-side srun --tasks = 1 --cpus-per-task = 2 ./app1 inp1.dat & srun --tasks = 1 --cpus-per-task = 2 ./app2 inp2.dat & wait # wait: Wait for both background commands to finish. This is important when running bash commands in the background (using &)! Otherwise, the job ends immediately. Please run series of similar tasks as job array. See Array Jobs","title":"When do I use srun in my job script?"},{"location":"slurm/submission.html#requesting-a-partition-qos-queue","text":"Per default jobs are submitted to the epyc2 partition and the default QoS job_epyc2 . The partition option can be used to request different hardware, e.b. gpu partition. And the QoS can be used to run in a specific queue, e.g. job_gpu_debug : #SBATCH --partition=gpu --qos=job_gpu_debug See Partitions / QoS for a list of available partitions and QoS and its specifications.","title":"Requesting a Partition / QoS (Queue)"},{"location":"slurm/submission.html#accounts","text":"By default a user has a \u201cprivate\u201d account. When belonging to a Workspace your private account gets deactivated and you can submit with the Workspace account. We strongly suggest to use the Workspace module ( module load Workspace ), which automatically sets the Workspace account for you. If really necessary, the can be selected by the --account option. If a wrong account/partition combination is requested, you will experience the following error message: sbatch: error: Batch job submission failed: Invalid account or account/partition combination specified If you did not specified --account , and belong to a Workspace, please load the Workspace module fist.","title":"Accounts"},{"location":"slurm/submission.html#parallel-jobs","text":"A parallel job requires multiple compute cores. These could be within one node or across the machine in multiple nodes. We distinguish following types: shared memory jobs : SMP, parallel jobs that run on a single compute node. The executable is called once. Within the execution (OMP) threads are spawned and merged. #SBATCH --ntasks=1 # default value #SBATCH --cpus-per-task=4 MPI jobs : parallel jobs that may be distributed over multiple compute nodes. Each task starts the executable. Within the application different workflows need to be defined for the different tasks. The tasks can communicate using Message Passing Interface (MPI). A job with 40 tasks: #SBATCH --nodes=2 #SBATCH --ntasks-per-node=20 hybrid : jobs using a combination of MPI tasks and (OMP) threads. #SBATCH --nodes=2 #SBATCH --ntasks-per-node=5 #SBATCH --cpus-per-task=4 The requested node,task, and CPU resources must match! For example, you cannot request one node ( --nodes=1 ) and more tasks ( --ntasks-per-node ) than CPU cores are available on a single node in the partition. In such a case you will experience an error message: sbatch: error: Batch job submission failed: Requested node configuration is not available. parallel launcher Parallel applications, esp. MPI, need a launcher to setup the environment. We strongly suggest to use srun instead of mpirun.","title":"Parallel Jobs"},{"location":"slurm/submission.html#environment-variables","text":"Slurm sets various environment variables available in the context of the job script. Some are set based on the requested resources for the job. Environment Variable Set By Option Description SLURM_JOB_NAME --job-name Name of the job SLURM_ARRAY_JOB_ID ID of your job SLURM_ARRAY_TASK_ID --array ID of the current array task SLURM_ARRAY_TASK_MAX --array Job array\u2019s maximum ID (index) number SLURM_ARRAY_TASK_MIN --array Job array\u2019s minimum ID (index) number SLURM_ARRAY_TASK_STEP --array Job array\u2019s index step size SLURM_NTASKS --ntasks Same as -n , --ntasks SLURM_NTASKS_PER_NODE --ntasks-per-node Number of tasks requested per node. Only set if the --ntasks-per-node option is specified SLURM_CPUS_PER_TASK --cpus-per-task Number of cpus requested per task. Only set if the --cpus-per-task option is specified TMPDIR References the disk space for the job on the local scratch For the full list, see man sbatch","title":"Environment Variables"},{"location":"slurm/submission.html#job-examples","text":"","title":"Job Examples"},{"location":"slurm/submission.html#sequential-job","text":"Running a serial job with email notification in case of error (1 task is default value): #!/bin/bash #SBATCH --mail-user=foo.bar@baz.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --job-name=\"Serial Job\" #SBATCH --time=04:00:00 # Your code below this line echo \"I'm on host: $HOSTNAME \"","title":"Sequential Job"},{"location":"slurm/submission.html#parallel-jobs_1","text":"Shared Memory Jobs (e.g. OpenMP) SMP parallelization is based upon dynamically created threads (fork and join) that share memory on a single node. The key request is --cpus-per-task . To run N threads in parallel, we request N CPUs on the node ( --cpus-per-task=N ). #!/bin/bash #SBATCH --mail-user=foo.bar@baz.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --job-name=\"SMP Job\" #SBATCH --mem-per-cpu=2G #SBATCH --cpus-per-task=16 #SBATCH --time=04:00:00 # Your code below this line srun ./my_binary MPI Jobs (e.g. Open MPI) MPI parallelization is based upon processes (local or distributed) that communicate by passing messages. Since they don\u2019t rely on shared memory those processes can be distributed among several compute nodes. Use the option --ntasks to request a certain number of tasks (processes) that can be distributed over multiple nodes: #!/bin/bash #SBATCH --mail-user=foo.bar@baz.unibe.ch #SBATCH --mail-type=end #SBATCH --job-name=\"MPI Job\" #SBATCH --mem-per-cpu=2G #SBATCH --ntasks=8 #SBATCH --time=04:00:00 # Your code below this line # First set the environment for using Open MPI module load foss srun ./my_binary On the \u2018bdw\u2019 partition you must use all CPUs provided by a node (20 CPUs). For example to run an OMPI job on 80 CPUs, do: #!/bin/bash #SBATCH --mail-user=foo.bar@baz.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --job-name=\"MPI Job\" #SBATCH --mem-per-cpu=2G #SBATCH --nodes=4 ## or --ntasks=80 #SBATCH --ntasks-per-node=20 #SBATCH --time=12:00:00 # Your code below this line module load foss srun ./my_binary","title":"Parallel Jobs"},{"location":"slurm/submission.html#performance-considerations","text":"","title":"Performance considerations"},{"location":"slurm/submission.html#job-throughput","text":"It is crucial to specify a more or less accurate runtime for your job. Requesting too little will result in job abortion, while requesting too much will have a negative impact on job start time and job throughput: Firstly, jobs with a shorter runtime have a greater chance to benefit from being backfilled between long running jobs and may therefore start earlier if resources are scarce. Secondly, a short running job may still start when a scheduled downtime is getting closer while long running jobs won\u2019t start because they are not guaranteed to finish before the start of the downtime. It is crucial to request the correct amount of cores for your job. Requesting cores that your job cannot utilize is a waste of resources that could otherwise be allocated to other jobs. Hence, jobs that theoretically could run have to wait for the resources to become available. For potential consequences of requesting too less cores on job performance, see below. It is crucial to request the correct amount of memory for your job. Requesting too little memory will result in job abortion. Requesting too much memory is a waste of resources that could otherwise be allocated to other jobs.","title":"Job Throughput"},{"location":"slurm/submission.html#job-performanceruntime","text":"It is crucial to request the correct amount of cores for your job. For parallel jobs (shared memory, MPI, hybrid) requesting less cores than processes/threads are spawned by the job will lead to potentially overbooked compute nodes. This is because your job will nevertheless spawn the required number of processes/threads (use a certain number of cores) while to the scheduler it appears that some of the utilized resources are still available, and thus the scheduler will allocate those resources to other jobs. Although under certain circumstances it might make sense to share cores among multiple processes/threads, the above reasoning should be considered as a general guideline, especially for unexperienced user.","title":"Job Performance/Runtime"},{"location":"software/Anaconda.html","text":"Anaconda Description Anaconda provides Python and a long list of packages as well as Jupyter and environment and package manager conda and pip. Anaconda brings a long list of Python packages. You can list them using conda list Availability On our systems we provide Anaconda3, which gets update regularly. Usage module load Anaconda3 Additional Packages to install additional packages please see Additional Packages Jupyter For Jupyter information please see JupyterLab Managing Virtual Environments, Versions with Anaconda Anaconda is a high performance distribution of Python that includes the most popular packages for data science (numpy, scipy,\u2026). It also features conda, a package, dependency and environment manager. With Anaconda you can run multiple versions of Python in isolated environments. conda when using conda the system may complain about: CommandNotFoundError: Your shell has not been properly configured to use 'conda activate' . To initialize your shell, run $ conda init <SHELL_NAME> Currently supported shells are: - bash - fish - tcsh - xonsh - zsh - powershell See 'conda init --help' for more information and options. IMPORTANT: You may need to close and restart your shell after running 'conda init' . Please do not run conda init . This would add hard coded environment changes in your $HOME/.bashrc . Instead initialise the conda environment using: module load Anaconda3 eval \" $( conda shell.bash hook ) \" This should also be used in your batch submission scripts when working with conda environments. conda environments By default conda environments are located into the $HOME/.conda directory. This can be changed using $CONDA_ENVS_PATH . This variable is set in the Workspace module. Which enables you to share conda environments. Move / Migration of conda environments If conda environments need to be transfered on the system, e.g. from $HOME to $WORKSPACE you can use the conda pack (see official conda pack documentation ). If your environment is already moved file system locations, you can recreate a new environment with the specification of the old environment. Therefore, we specify the location of the old environment, load the Anaconda module, initialize conda, and get the specification of the old environment. Then importantly unset the CONDA_ENVS_PATH to install the new conda environment in the default location and create it. export CONDA_ENVS_PATH=${HOME}/anaconda3/envs ## or where you had your old envs module load Anaconda3 eval \"$(conda shell.bash hook)\" conda info --envs conda activate oldEnvName ## choose your old environment name conda list --explicit > spec-list.txt unset CONDA_ENVS_PATH conda create --name myEnvName --file spec-list.txt # select a name","title":"Anaconda"},{"location":"software/Anaconda.html#anaconda","text":"","title":"Anaconda"},{"location":"software/Anaconda.html#description","text":"Anaconda provides Python and a long list of packages as well as Jupyter and environment and package manager conda and pip. Anaconda brings a long list of Python packages. You can list them using conda list","title":"Description"},{"location":"software/Anaconda.html#availability","text":"On our systems we provide Anaconda3, which gets update regularly.","title":"Availability"},{"location":"software/Anaconda.html#usage","text":"module load Anaconda3","title":"Usage"},{"location":"software/Anaconda.html#additional-packages","text":"to install additional packages please see Additional Packages","title":"Additional Packages"},{"location":"software/Anaconda.html#jupyter","text":"For Jupyter information please see JupyterLab","title":"Jupyter"},{"location":"software/Anaconda.html#managing-virtual-environments-versions-with-anaconda","text":"Anaconda is a high performance distribution of Python that includes the most popular packages for data science (numpy, scipy,\u2026). It also features conda, a package, dependency and environment manager. With Anaconda you can run multiple versions of Python in isolated environments.","title":"Managing Virtual Environments, Versions with Anaconda"},{"location":"software/Anaconda.html#conda","text":"when using conda the system may complain about: CommandNotFoundError: Your shell has not been properly configured to use 'conda activate' . To initialize your shell, run $ conda init <SHELL_NAME> Currently supported shells are: - bash - fish - tcsh - xonsh - zsh - powershell See 'conda init --help' for more information and options. IMPORTANT: You may need to close and restart your shell after running 'conda init' . Please do not run conda init . This would add hard coded environment changes in your $HOME/.bashrc . Instead initialise the conda environment using: module load Anaconda3 eval \" $( conda shell.bash hook ) \" This should also be used in your batch submission scripts when working with conda environments.","title":"conda"},{"location":"software/Anaconda.html#conda-environments","text":"By default conda environments are located into the $HOME/.conda directory. This can be changed using $CONDA_ENVS_PATH . This variable is set in the Workspace module. Which enables you to share conda environments.","title":"conda environments"},{"location":"software/Anaconda.html#move-migration-of-conda-environments","text":"If conda environments need to be transfered on the system, e.g. from $HOME to $WORKSPACE you can use the conda pack (see official conda pack documentation ). If your environment is already moved file system locations, you can recreate a new environment with the specification of the old environment. Therefore, we specify the location of the old environment, load the Anaconda module, initialize conda, and get the specification of the old environment. Then importantly unset the CONDA_ENVS_PATH to install the new conda environment in the default location and create it. export CONDA_ENVS_PATH=${HOME}/anaconda3/envs ## or where you had your old envs module load Anaconda3 eval \"$(conda shell.bash hook)\" conda info --envs conda activate oldEnvName ## choose your old environment name conda list --explicit > spec-list.txt unset CONDA_ENVS_PATH conda create --name myEnvName --file spec-list.txt # select a name","title":"Move / Migration of conda environments"},{"location":"software/CUDA.html","text":"CUDA Description Libraries for parallel programm on NVIDIA GPUs. run only If you only want to use the CUDA with your already compiled application you only need to load: module load CUDA compile applications CUDA has restriction in supported compilers. E.g. CUDA/10.1.243 is not compatible with GCC/9.3.9 We suggest to load complete toolchains, e.g. module load fosscuda This provides beside CUDA and GCC also OpenMPI, OpenBLAS, FFTW, ScaLAPACK and others.","title":"CUDA"},{"location":"software/CUDA.html#cuda","text":"","title":"CUDA"},{"location":"software/CUDA.html#description","text":"Libraries for parallel programm on NVIDIA GPUs.","title":"Description"},{"location":"software/CUDA.html#run-only","text":"If you only want to use the CUDA with your already compiled application you only need to load: module load CUDA","title":"run only"},{"location":"software/CUDA.html#compile-applications","text":"CUDA has restriction in supported compilers. E.g. CUDA/10.1.243 is not compatible with GCC/9.3.9 We suggest to load complete toolchains, e.g. module load fosscuda This provides beside CUDA and GCC also OpenMPI, OpenBLAS, FFTW, ScaLAPACK and others.","title":"compile applications"},{"location":"software/EasyBuild.html","text":"EasyBuild Description EasyBuild can install software packages including the related modules. The location will be controlled using our modules, e.g. the Workspace module, see Installing Custom Software . On top of the usual EasyBuild framework we added some extensions which allows you to build for specific architectures or a generic software stack in your user/group space. Therefore, use the eb command to search and try and the eb-install-all or eb-install-generic command to install the package. The following steps need are necessary: load modules find the package specification decide the desired software stack run EasyBuild installation using eb-install-all or eb-install-generic Modules Depending if you want to install the package in user or a group space you need to load the related module and the EasyBuild module, e.g.: module load Workspace ### if you want to install into your HOME use Workspace/home module load EasyBuild Therewith, our EasyBuild tools and EasyBuild itself are available. Note Specify the WorkspaceID when loading the Workspace module. See module instructions Package Specification EasyBuild has a large repository of available packages in different versions. You can use these specifications as is or copy/download and modify the EasyConfigs (see below). Available packages can be searched using the following command, here for the gatk package eb --search gatk [ ... ] eb --search gatk == found valid index for /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs, so using it... * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-1.0.5083.eb ... * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.0.1.2-Java-1.8.eb ... * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.0.12.0-foss-2018b-Python-3.6.6.eb ... * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.0.4.0-intel-2018a-Python-3.6.4.eb * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.0.5.1-foss-2018a-Python-3.6.4.eb ... * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.0.8.1-foss-2018b-Python-2.7.15.eb ... * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.1.8.1-GCCcore-9.3.0-Java-1.8.eb As shown above there are different versions of GATK and for different toolchains available ( foss , intel , GCCcore ). Select one You can list all dependencies using: eb -Dr /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.1.8.1-GCCcore-9.3.0-Java-1.8.eb ... Dry run: printing build status of easyconfigs and dependencies CFGS = /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs * [ ] $CFGS /j/Java/Java-1.8.0_281.eb ( module: Java/1.8.0_281 ) ... * [ x ] $CFGS /p/Python/Python-2.7.18-GCCcore-9.3.0.eb ( module: Python/2.7.18-GCCcore-9.3.0 ) * [ ] $CFGS /g/GATK/GATK-4.1.8.1-GCCcore-9.3.0-Java-1.8.eb ( module: GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8 ) Dependencies marked with x are already installed, the other dependencies will be installed if using the robot option. Additional options, e.g. for selecting a specific software version can be found using eb --help . Using EasyConfig files You can use the directy selected easyconfig or if necessary copy and adapt it. easyconfig files are text files specifying the software version, toolchain version, dependencies, compile arguments and more. If you need more information see EasyBuild documentation , and if necessary ask our support team for assistance. Selecting a software stack Depending on the package and its target usage one or more software stacks should be selected. Therefore, the installation command starts with one for the following command: all architectural software stacks: eb-install-all a specific architectural software stack (e.g. only targeting Broadwell nodes): eb-install-all --archs='broadwell' OR generic software stack: eb-install-generic , CPU architecture independent, like git Installation After selecting the package installation recipe and the target software stack, the installation process can be submitted. With the following commands, SLURM job files will be created, and submitted to the desired compute nodes. There the packages are build and module files created. The general syntax is: eb_install_ { all,generic } [ options ] [ easybuild options ] <easyconfig>.eb Additional SLURM arguments can be selected using the --slurm-args option, e.g. --slurm-args='--account=xyz --time=00:10:00 --cpus-per-task' . If specific architectures should be selected use e.g. --arch='broadwell ivy' . After this options, EasyBuild arguments can be provided without prefix, e.g. --robot . Few examples: for FFTW in all architectural software stacks: eb-install-all --robot --software-name = git --toolchain-name = GCC for git in the generic software stack: eb-install-generic --robot --software-name = git --toolchain-name = GCC for a custom EasyConfig myApp.eb only in the Broadwell and Ivybridge software stack: eb-install-all --archs = 'ivy broadwell' --robot myApp.eb This will need time to get scheduled and processed. The job output is presented in the eb_out.* files, one for each architecture. If the build could not be finished in the default time of 1h, the walltime can be extended using: eb-install-all --robot --slurm-args = '--time=05:00:00' ... Note Please check the end of the out file for the COMPLETED: Installation ended successfully statement. When finished you (and your collaborators) should be able to use use the software, by just loading the user/workspace related module and the module for the installed package.","title":"EasyBuild"},{"location":"software/EasyBuild.html#easybuild","text":"","title":"EasyBuild"},{"location":"software/EasyBuild.html#description","text":"EasyBuild can install software packages including the related modules. The location will be controlled using our modules, e.g. the Workspace module, see Installing Custom Software . On top of the usual EasyBuild framework we added some extensions which allows you to build for specific architectures or a generic software stack in your user/group space. Therefore, use the eb command to search and try and the eb-install-all or eb-install-generic command to install the package. The following steps need are necessary: load modules find the package specification decide the desired software stack run EasyBuild installation using eb-install-all or eb-install-generic","title":"Description"},{"location":"software/EasyBuild.html#modules","text":"Depending if you want to install the package in user or a group space you need to load the related module and the EasyBuild module, e.g.: module load Workspace ### if you want to install into your HOME use Workspace/home module load EasyBuild Therewith, our EasyBuild tools and EasyBuild itself are available. Note Specify the WorkspaceID when loading the Workspace module. See module instructions","title":"Modules"},{"location":"software/EasyBuild.html#package-specification","text":"EasyBuild has a large repository of available packages in different versions. You can use these specifications as is or copy/download and modify the EasyConfigs (see below). Available packages can be searched using the following command, here for the gatk package eb --search gatk [ ... ] eb --search gatk == found valid index for /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs, so using it... * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-1.0.5083.eb ... * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.0.1.2-Java-1.8.eb ... * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.0.12.0-foss-2018b-Python-3.6.6.eb ... * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.0.4.0-intel-2018a-Python-3.6.4.eb * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.0.5.1-foss-2018a-Python-3.6.4.eb ... * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.0.8.1-foss-2018b-Python-2.7.15.eb ... * /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.1.8.1-GCCcore-9.3.0-Java-1.8.eb As shown above there are different versions of GATK and for different toolchains available ( foss , intel , GCCcore ). Select one You can list all dependencies using: eb -Dr /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs/g/GATK/GATK-4.1.8.1-GCCcore-9.3.0-Java-1.8.eb ... Dry run: printing build status of easyconfigs and dependencies CFGS = /storage/software/generic.el7/software/EasyBuild/4.3.3/easybuild/easyconfigs * [ ] $CFGS /j/Java/Java-1.8.0_281.eb ( module: Java/1.8.0_281 ) ... * [ x ] $CFGS /p/Python/Python-2.7.18-GCCcore-9.3.0.eb ( module: Python/2.7.18-GCCcore-9.3.0 ) * [ ] $CFGS /g/GATK/GATK-4.1.8.1-GCCcore-9.3.0-Java-1.8.eb ( module: GATK/4.1.8.1-GCCcore-9.3.0-Java-1.8 ) Dependencies marked with x are already installed, the other dependencies will be installed if using the robot option. Additional options, e.g. for selecting a specific software version can be found using eb --help .","title":"Package Specification"},{"location":"software/EasyBuild.html#using-easyconfig-files","text":"You can use the directy selected easyconfig or if necessary copy and adapt it. easyconfig files are text files specifying the software version, toolchain version, dependencies, compile arguments and more. If you need more information see EasyBuild documentation , and if necessary ask our support team for assistance.","title":"Using EasyConfig files"},{"location":"software/EasyBuild.html#selecting-a-software-stack","text":"Depending on the package and its target usage one or more software stacks should be selected. Therefore, the installation command starts with one for the following command: all architectural software stacks: eb-install-all a specific architectural software stack (e.g. only targeting Broadwell nodes): eb-install-all --archs='broadwell' OR generic software stack: eb-install-generic , CPU architecture independent, like git","title":"Selecting a software stack"},{"location":"software/EasyBuild.html#installation","text":"After selecting the package installation recipe and the target software stack, the installation process can be submitted. With the following commands, SLURM job files will be created, and submitted to the desired compute nodes. There the packages are build and module files created. The general syntax is: eb_install_ { all,generic } [ options ] [ easybuild options ] <easyconfig>.eb Additional SLURM arguments can be selected using the --slurm-args option, e.g. --slurm-args='--account=xyz --time=00:10:00 --cpus-per-task' . If specific architectures should be selected use e.g. --arch='broadwell ivy' . After this options, EasyBuild arguments can be provided without prefix, e.g. --robot . Few examples: for FFTW in all architectural software stacks: eb-install-all --robot --software-name = git --toolchain-name = GCC for git in the generic software stack: eb-install-generic --robot --software-name = git --toolchain-name = GCC for a custom EasyConfig myApp.eb only in the Broadwell and Ivybridge software stack: eb-install-all --archs = 'ivy broadwell' --robot myApp.eb This will need time to get scheduled and processed. The job output is presented in the eb_out.* files, one for each architecture. If the build could not be finished in the default time of 1h, the walltime can be extended using: eb-install-all --robot --slurm-args = '--time=05:00:00' ... Note Please check the end of the out file for the COMPLETED: Installation ended successfully statement. When finished you (and your collaborators) should be able to use use the software, by just loading the user/workspace related module and the module for the installed package.","title":"Installation"},{"location":"software/JupyterLab.html","text":"Jupyter Lab Description Some useful information on using Jupyter Lab on UBELIX compute nodes. IMPORTANT: in the following we show how to start the server on a compute node. Please keep in mind that these resources will be dedicated for you, thus and idle session will waste resources. Please quit your session as soon as you don\u2019t use it anymore , even for a lunch break. Your notebook will maintain all you input/output. Overview On UBELIX we provide Jupyter Lab for working with Jupyter Notebooks. JupyterLab is a single-user web-based Notebook server, running in the user space. JupyterLab servers should be started preferably on a compute node, especially for compute intensive or memory intensive workloads. After starting the Jupyter Lab server your local browser can be connected using port forwarding. Therefore port forwarding needs to be enabled properly. On this page we describe: Launch JupyterLab Connect to UBELIX and establishing SSH port forwarding SSH with port forwarding Launch the JupyterLab server Launch JupyterLab in your local browser Kernels Packages Launch JupyterLab Since JupyterLab is a web based application, a port needs to be forwarded to your local machine, where your browser can connect to. This port numbers need to be between 2000 and 65000 and need to be unique on the present machine. The default port for JupyterLab is 8888, but only one user can use this at a time. To avoid the need for modifying the following procedure again and again, we suggest to (once) select a unique number (between 2000 and 65000). And then following commands can be hopefully reused without modification. The port needs to be specified while establishing the connection to UBELIX and while launching JupyterLab. In the following we use the port number 15051 ( please select another number ). Passwordless SSH within the HPCs Please verify that you created and registered a SSH key within UBLEIX. If you can perform the following command without entering your password your are ready to go: ssh localhost otherwise create and register a new key on a login node. ssh-keygen -t rsa -b 4096 cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys chmod 640 .ssh/authorized_keys Setup SSH with port forwarding First, the port forwarding needs to be enabled between your local machine and UBELIX. Therewith a local port will be connected to the remote port on UBELIX. This ports are numbers between 2000 and 65000, which needs to be unique on the both sides. The default port for JupyterLab is 8888, but only one user can use this at a time. For simplicity, we kept both numbers the same (here 15051). This can be specified on the command line in the terminal. The ssh command from your local machine to the ubelix login node needs to be called with following arguments: ssh -Y -L 15051:localhost:15051 <username>@submit.unibe.ch If configured in your .ssh/config , you can also use the alias instead of the full name for UBELIX. Where <username> is you campus account name. Note: MobaXterm has an internal terminal which acts like a linux terminal and can be configured as described in the Standard Terminal Setup. Therewith, the SSH command line approach above can be used. Launch the JupyterLab server On UBELIX, the required Anaconda3 module needs to be loaded. If you want to use additional kernels (R) you need to load additional modules, e.g. IRkernel (for R kernels): module load Anaconda3 A script is provided, taking care of enabling the port forwarding to the compute node and launching JupyterLab. jupyter-compute 15051 --time=00:45:00 # please change port number This tool will lauch the server on a compute node, and establish the port forwarding. After general output, JupyterLab prints a URL with a unique key and the network port number where the web-server is listening, this should look similar to: ... [C 21:43:35.291 LabApp] To access the notebook, open this file in a browser: file:///gpfs/homefs/id/ms20e149/.local/share/jupyter/runtime/nbserver-30194-open.html Or copy and paste one of these URLs: http://anode001:15051/?token=69ba5d24acab5915f2520c008a57df51f3cc38b7050ea073 or http://127.0.0.1:15051/?token=69ba5d24acab5915f2520c008a57df51f3cc38b7050ea073 The last line needs to be copied in your local browser. Attention do not use Ctrl + C for copying the link, this will abort the server process and kill your job. QOS the jupyter-compute tool uses an special Slurm Quality of Service (QOS), which should reduce queuing times for interactive jobs. Since interactive jobs are considered to be finished within less than a working day, the walltime limit cannot exceed 8h. You can disable that qos using the option --no-qos , but please release the resources as soon as you are not actively working with the resources anymore. JupyterLab in your local browser The full address on the last line (starting with the 127.0.0.1) of the Jupyter Server statement including the token needs to be copied into your browser on your local machine. After initializing Jupyter Lab you should see a page similar to: Therewith the Notebook and its containing tasks are performed on a compute node, which can double check e.g. using using the following in Python: import socket print(socket.gethostname()) IMPORTANT: Please remember to stop your Jupyter Lab server and therewith your slurm job, when you do not need it anymore. Thus, the resource get available to other users again. Note: After stopping the JupyterLab server some sessions may get corrupted and do not take input correctly anymore. In this case just quit and re-establish your ssh session. JupyterLab with multiple CPU cores More resources can be requested, e.g. by using: jupyter-compute 15051 --ntasks 1 -t 60 --cpus-per-task 5 --mem 512MB Where 5 cores are requested for threading and a total memory of 3GB. Please do not use multiprocessing.cpu_count() since this is returning the total amount of cores on the node. Furthermore, if you use libraries, which implement threading: align the numbers of threads (often called jobs) to the selected number of cores (otherwise the performance will be affected). OR requesting GPU resources on a node with a NVIDIA graphics card: jupyter-compute 15051 --ntasks 1 -t 60 --partition=gpu --gres=gpu:gtx1080ti:1 Note: This tool can only be used in the all and gpu partition. Kernels The following JupyterLab kernel are installed: Python3 R R verify that the module IRkernel is loaded module load IRkernel Packages There are a long list of default packages provided by Anaconda3 (list all using !pip list ) and R (list using installed.packages(.Library) , note the list is shortened). Furthermore, you can install additional packages in Python using pip install --user or in R using install.packages(\"sampling\") .","title":"JupyterLab"},{"location":"software/JupyterLab.html#jupyter-lab","text":"","title":"Jupyter Lab"},{"location":"software/JupyterLab.html#description","text":"Some useful information on using Jupyter Lab on UBELIX compute nodes. IMPORTANT: in the following we show how to start the server on a compute node. Please keep in mind that these resources will be dedicated for you, thus and idle session will waste resources. Please quit your session as soon as you don\u2019t use it anymore , even for a lunch break. Your notebook will maintain all you input/output.","title":"Description"},{"location":"software/JupyterLab.html#overview","text":"On UBELIX we provide Jupyter Lab for working with Jupyter Notebooks. JupyterLab is a single-user web-based Notebook server, running in the user space. JupyterLab servers should be started preferably on a compute node, especially for compute intensive or memory intensive workloads. After starting the Jupyter Lab server your local browser can be connected using port forwarding. Therefore port forwarding needs to be enabled properly. On this page we describe: Launch JupyterLab Connect to UBELIX and establishing SSH port forwarding SSH with port forwarding Launch the JupyterLab server Launch JupyterLab in your local browser Kernels Packages","title":"Overview"},{"location":"software/JupyterLab.html#launch-jupyterlab","text":"Since JupyterLab is a web based application, a port needs to be forwarded to your local machine, where your browser can connect to. This port numbers need to be between 2000 and 65000 and need to be unique on the present machine. The default port for JupyterLab is 8888, but only one user can use this at a time. To avoid the need for modifying the following procedure again and again, we suggest to (once) select a unique number (between 2000 and 65000). And then following commands can be hopefully reused without modification. The port needs to be specified while establishing the connection to UBELIX and while launching JupyterLab. In the following we use the port number 15051 ( please select another number ).","title":"Launch JupyterLab"},{"location":"software/JupyterLab.html#passwordless-ssh-within-the-hpcs","text":"Please verify that you created and registered a SSH key within UBLEIX. If you can perform the following command without entering your password your are ready to go: ssh localhost otherwise create and register a new key on a login node. ssh-keygen -t rsa -b 4096 cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys chmod 640 .ssh/authorized_keys","title":"Passwordless SSH within the HPCs"},{"location":"software/JupyterLab.html#setup-ssh-with-port-forwarding","text":"First, the port forwarding needs to be enabled between your local machine and UBELIX. Therewith a local port will be connected to the remote port on UBELIX. This ports are numbers between 2000 and 65000, which needs to be unique on the both sides. The default port for JupyterLab is 8888, but only one user can use this at a time. For simplicity, we kept both numbers the same (here 15051). This can be specified on the command line in the terminal. The ssh command from your local machine to the ubelix login node needs to be called with following arguments: ssh -Y -L 15051:localhost:15051 <username>@submit.unibe.ch If configured in your .ssh/config , you can also use the alias instead of the full name for UBELIX. Where <username> is you campus account name. Note: MobaXterm has an internal terminal which acts like a linux terminal and can be configured as described in the Standard Terminal Setup. Therewith, the SSH command line approach above can be used.","title":"Setup SSH with port forwarding"},{"location":"software/JupyterLab.html#launch-the-jupyterlab-server","text":"On UBELIX, the required Anaconda3 module needs to be loaded. If you want to use additional kernels (R) you need to load additional modules, e.g. IRkernel (for R kernels): module load Anaconda3 A script is provided, taking care of enabling the port forwarding to the compute node and launching JupyterLab. jupyter-compute 15051 --time=00:45:00 # please change port number This tool will lauch the server on a compute node, and establish the port forwarding. After general output, JupyterLab prints a URL with a unique key and the network port number where the web-server is listening, this should look similar to: ... [C 21:43:35.291 LabApp] To access the notebook, open this file in a browser: file:///gpfs/homefs/id/ms20e149/.local/share/jupyter/runtime/nbserver-30194-open.html Or copy and paste one of these URLs: http://anode001:15051/?token=69ba5d24acab5915f2520c008a57df51f3cc38b7050ea073 or http://127.0.0.1:15051/?token=69ba5d24acab5915f2520c008a57df51f3cc38b7050ea073 The last line needs to be copied in your local browser. Attention do not use Ctrl + C for copying the link, this will abort the server process and kill your job. QOS the jupyter-compute tool uses an special Slurm Quality of Service (QOS), which should reduce queuing times for interactive jobs. Since interactive jobs are considered to be finished within less than a working day, the walltime limit cannot exceed 8h. You can disable that qos using the option --no-qos , but please release the resources as soon as you are not actively working with the resources anymore.","title":"Launch the JupyterLab server"},{"location":"software/JupyterLab.html#jupyterlab-in-your-local-browser","text":"The full address on the last line (starting with the 127.0.0.1) of the Jupyter Server statement including the token needs to be copied into your browser on your local machine. After initializing Jupyter Lab you should see a page similar to: Therewith the Notebook and its containing tasks are performed on a compute node, which can double check e.g. using using the following in Python: import socket print(socket.gethostname()) IMPORTANT: Please remember to stop your Jupyter Lab server and therewith your slurm job, when you do not need it anymore. Thus, the resource get available to other users again. Note: After stopping the JupyterLab server some sessions may get corrupted and do not take input correctly anymore. In this case just quit and re-establish your ssh session.","title":"JupyterLab in your local browser"},{"location":"software/JupyterLab.html#jupyterlab-with-multiple-cpu-cores","text":"More resources can be requested, e.g. by using: jupyter-compute 15051 --ntasks 1 -t 60 --cpus-per-task 5 --mem 512MB Where 5 cores are requested for threading and a total memory of 3GB. Please do not use multiprocessing.cpu_count() since this is returning the total amount of cores on the node. Furthermore, if you use libraries, which implement threading: align the numbers of threads (often called jobs) to the selected number of cores (otherwise the performance will be affected). OR requesting GPU resources on a node with a NVIDIA graphics card: jupyter-compute 15051 --ntasks 1 -t 60 --partition=gpu --gres=gpu:gtx1080ti:1 Note: This tool can only be used in the all and gpu partition.","title":"JupyterLab with multiple CPU cores"},{"location":"software/JupyterLab.html#kernels","text":"The following JupyterLab kernel are installed: Python3 R","title":"Kernels"},{"location":"software/JupyterLab.html#r","text":"verify that the module IRkernel is loaded module load IRkernel","title":"R"},{"location":"software/JupyterLab.html#packages","text":"There are a long list of default packages provided by Anaconda3 (list all using !pip list ) and R (list using installed.packages(.Library) , note the list is shortened). Furthermore, you can install additional packages in Python using pip install --user or in R using install.packages(\"sampling\") .","title":"Packages"},{"location":"software/hpc-modules.html","text":"HPC software environment Description On our HPCs we provide a Linux bash environment. Many basic Linux commands are available immediately. Other especially high level user applications and libraries need to be loaded using modules. The present Lmod module system provides the possibility to have various packages side-by-side and even in different versions, while preventing unwanted influences. Furthermore, various software versions are build for the different CPU architectures. They are organized in different software stacks, which are loaded automatically on the related architecture. Additionally to our software stack, VITAL-IT provides a software stack targeting mainly bioinformatics users, see Bioinformatics Software . Basic commands in the following you find commands for listing, loading and unloading modules. Later a more advanced view is presented. List available Modules There are various ways to search for a software package. You can list all currently available packages using: module avail You can search for an packages starting with a specific string, e.g. all version of GCC: module avail GCC Furthermore, the following command lists you all the modules containing a certain sub-string in the name, even in other software stacks, e.g.: module spider Assambler In the example above all modules with the sub-string Assambler will be listed, in this case the ones from the Vital-It software stack. Load/Add a Modules module load OpenMPI/3.1.3-GCC-8.2.0-2.31.1 or equivalently: $ module add OpenMPI/3.1.3-GCC-8.2.0-2.31.1 This may also load modules of dependencies, e.g. by loading OpenMPI, other modules like GCC and libraries are additionally loaded. List all Loaded Modules $ module list Currently Loaded Modules: 1 ) GCCcore/8.2.0 3 ) binutils/.2.31.1-GCCcore-8.2.0 ( H ) 5 ) numactl/2.0.12-GCCcore-8.2.0 7 ) libxml2/.2.9.8-GCCcore-8.2.0 ( H ) 9 ) hwloc/1.11.11-GCCcore-8.2.0 2 ) zlib/.1.2.11-GCCcore-8.2.0 ( H ) 4 ) GCC/8.2.0-2.31.1 6 ) XZ/.5.2.4-GCCcore-8.2.0 ( H ) 8 ) libpciaccess/.0.14-GCCcore-8.2.0 ( H ) 10 ) OpenMPI/3.1.3-GCC-8.2.0-2.31.1 Where: H: Hidden Module Unload/remove a Modules This will only unload the specified modulefile, but not the dependencies that where automatically loaded when loading the specified modulefile (see purge below). $ module unload OpenMPI/3.1.3-GCC-8.2.0-2.31.1 or equivalently: $ module rm OpenMPI/3.1.3-GCC-8.2.0-2.31.1 Purge all Modules This will unload all previously loaded modulefiles. $ module purge Show information Most modules provide a short description which software package they contain and a link to the homepage, as well as information about the changes of environment undertaken. From short to full detail: $ module whatis OpenMPI $ module help OpenMPI $ module show OpenMPI Modules background Architectural software stacks On our HPCs we use LMOD (Lua modules) to provide access to different software packages and different versions. Beside different packages and versions, we provide software stacks build for the different CPU architectures. This enables us to have the packages build e.g. with AVX2 for Broadwell CPUs, but also a version with only SSE4 for Sandy bridge CPUs. These software stacks are completely transparent to the user and will be used automatically when loading a module on the related architecture. If you want to build your own software build for specific Hardware, we provide tools which help you, see Installing Custom Software Scientific Software Management Our scientific software stack, available via module files are mainly build with EasyBuild. This tool helps us to install and maintain software packages and recycle existing installation procedures. There are plenty of install instructions available EasyBuild/Easyconfigs , which can be installed also in the user space with low effort, see Installing Custom Software","title":"HPC software environment"},{"location":"software/hpc-modules.html#hpc-software-environment","text":"","title":"HPC software environment"},{"location":"software/hpc-modules.html#description","text":"On our HPCs we provide a Linux bash environment. Many basic Linux commands are available immediately. Other especially high level user applications and libraries need to be loaded using modules. The present Lmod module system provides the possibility to have various packages side-by-side and even in different versions, while preventing unwanted influences. Furthermore, various software versions are build for the different CPU architectures. They are organized in different software stacks, which are loaded automatically on the related architecture. Additionally to our software stack, VITAL-IT provides a software stack targeting mainly bioinformatics users, see Bioinformatics Software .","title":"Description"},{"location":"software/hpc-modules.html#basic-commands","text":"in the following you find commands for listing, loading and unloading modules. Later a more advanced view is presented.","title":"Basic commands"},{"location":"software/hpc-modules.html#list-available-modules","text":"There are various ways to search for a software package. You can list all currently available packages using: module avail You can search for an packages starting with a specific string, e.g. all version of GCC: module avail GCC Furthermore, the following command lists you all the modules containing a certain sub-string in the name, even in other software stacks, e.g.: module spider Assambler In the example above all modules with the sub-string Assambler will be listed, in this case the ones from the Vital-It software stack.","title":"List available Modules"},{"location":"software/hpc-modules.html#loadadd-a-modules","text":"module load OpenMPI/3.1.3-GCC-8.2.0-2.31.1 or equivalently: $ module add OpenMPI/3.1.3-GCC-8.2.0-2.31.1 This may also load modules of dependencies, e.g. by loading OpenMPI, other modules like GCC and libraries are additionally loaded.","title":"Load/Add a Modules"},{"location":"software/hpc-modules.html#list-all-loaded-modules","text":"$ module list Currently Loaded Modules: 1 ) GCCcore/8.2.0 3 ) binutils/.2.31.1-GCCcore-8.2.0 ( H ) 5 ) numactl/2.0.12-GCCcore-8.2.0 7 ) libxml2/.2.9.8-GCCcore-8.2.0 ( H ) 9 ) hwloc/1.11.11-GCCcore-8.2.0 2 ) zlib/.1.2.11-GCCcore-8.2.0 ( H ) 4 ) GCC/8.2.0-2.31.1 6 ) XZ/.5.2.4-GCCcore-8.2.0 ( H ) 8 ) libpciaccess/.0.14-GCCcore-8.2.0 ( H ) 10 ) OpenMPI/3.1.3-GCC-8.2.0-2.31.1 Where: H: Hidden Module","title":"List all Loaded Modules"},{"location":"software/hpc-modules.html#unloadremove-a-modules","text":"This will only unload the specified modulefile, but not the dependencies that where automatically loaded when loading the specified modulefile (see purge below). $ module unload OpenMPI/3.1.3-GCC-8.2.0-2.31.1 or equivalently: $ module rm OpenMPI/3.1.3-GCC-8.2.0-2.31.1","title":"Unload/remove a Modules"},{"location":"software/hpc-modules.html#purge-all-modules","text":"This will unload all previously loaded modulefiles. $ module purge","title":"Purge all Modules"},{"location":"software/hpc-modules.html#show-information","text":"Most modules provide a short description which software package they contain and a link to the homepage, as well as information about the changes of environment undertaken. From short to full detail: $ module whatis OpenMPI $ module help OpenMPI $ module show OpenMPI","title":"Show information"},{"location":"software/hpc-modules.html#modules-background","text":"","title":"Modules background"},{"location":"software/hpc-modules.html#architectural-software-stacks","text":"On our HPCs we use LMOD (Lua modules) to provide access to different software packages and different versions. Beside different packages and versions, we provide software stacks build for the different CPU architectures. This enables us to have the packages build e.g. with AVX2 for Broadwell CPUs, but also a version with only SSE4 for Sandy bridge CPUs. These software stacks are completely transparent to the user and will be used automatically when loading a module on the related architecture. If you want to build your own software build for specific Hardware, we provide tools which help you, see Installing Custom Software","title":"Architectural software stacks"},{"location":"software/hpc-modules.html#scientific-software-management","text":"Our scientific software stack, available via module files are mainly build with EasyBuild. This tool helps us to install and maintain software packages and recycle existing installation procedures. There are plenty of install instructions available EasyBuild/Easyconfigs , which can be installed also in the user space with low effort, see Installing Custom Software","title":"Scientific Software Management"},{"location":"software/installing-custom-software.html","text":"Installing Custom Software Description UBELIX comes with a plethora of software pre-installed. And there are tools provided installing additional packages in the user/group space. The present CustomRepo and Workspace modules provide easy access even for multiple versions of the same Software package. In general, the module environment is described at HPC software environment . The command module avail lists the available packages and module spider FFTW searches for all modules which have FFTW in their name. This article describes a procedure for installing custom software stacks in your user/group space. An EasyBuild and a manual approach is presented. Note You cannot use the packet management utility yum for this, since this command requires root privileges to install software system wide. Instead you have to compile and install the software yourself. Note If you know that some missing software could be of general interest for a wide community on our machines, you can ask us to install the software system wide. Bioinformatic packages are managed by Vital-IT Note If you need further assistance in installing your software packages or optimizing for our machine architecture, get in touch with our support team. The LMOD module system allows to enable software package by package. Thus, influences between different packages can be minimized. It also allows to have multiple versions of the same software product installed side by side. See HPC software environment . There are modules providing access to your user/group software stacks and assisting you with building packages into them. When possible we use EasyBuild to provision software packages. EasyBuild is a software installation framework. Installation procedures are defined in so called EasyConfigs, including the location of the sources, dependencies, its versions, used environments, compile arguments, and more. These EasyConfigs are publicly available on EasyBuild github repository and can be downloaded used and if necessary adapted, e.g. for new versions. Building Software packages There are mainly two options to build a software package and its module: using EasyBuild , with an existing, an modified or a newly created EasyConfig performing manual installation , and creating a module file In the following, both methods are described in more details. In general we consider: Is there already and EasyBuild recipe available, which can be used or modified? Which software stack we want to build into, a specific CPU architecture, OR all CPU architectures, OR should it be a generic one (see Software stacks ) Software stacks We provide for all CPU architecture a software stack, where applications are build on and for this specific architecture. Therewith, application can use different optimizations, like instruction sets (e.g. using AVX2 on Broadwell) on the different architectures. Furthermore, we provide a generic software stack, where architecture independent applications are installed, e.g. Python packages. Our generic software stack is build on Sandybridge nodes, which have the smallest instruction set, and thus we aim preventing conflicts. For EasyBuild we provide tools which can automatically install in all architectural or the generic software stack, see below. Additionally, we provide modules which provide access and assist building such software stacks in your user/group space. EasyBuild For detailed instructions read the EasyBuild article . If you are installing your own application you may want to consider to create an EasyConfig for it. Have a look in the EasyBuild documentation , examples on the EasyBuild github . And if necessary ask our support team for assistance. Manually compiling There are very different ways of manually installing software packages, starting from just using a compiler, having a makefile, up to complex build systems. A few considerations need to kept in mind while building for our systems: Compilers: different compilers are available and can be loaded by modules. Toolchains bundle compiler with additionally libraries and tools, like MPI, FFTW, MKL, see Toolchains . Furthermore, complex algorithms are optimised differently in the compilers. It is worthwhile to try and compare multiple compilers. CPU architectures: since there are different CPU architectures available, applications should be build for the targeted architecture. Often significant performance improvements can be obtained compiling for the correct instruction sets. Therefore, launch your build processes on the targeted architecture. Accessibility: On the one hand probably different versions, e.g. for compiler and CPU architecture should be provided. On the other hand the access to it should be as easy as possible for all users of that package. Therefore, modules provide a user-friendly. These modules can be organized e.g. in software stacks, one for each architecture. One and probably the most used procedure is the GNU configure / make: tar xzvf some-software-0.1.tar.gz cd some-software-0.1 ./configure --prefix = /path/to/target/some-software/0.1 make make install make clean Note Consider creating an Easyconfig if you already have a well tested procedure, see EasyBuild . configure is usually a complex shell script that gathers information about the system and prepares the compile process. With the --prefix option you can specify a base installation directory, where make install will install the files into subdirectories like bin , lib , include , etc. The make utility is what does the actual compiling and linking. If, for example, some additional libraries are missing on the system or not found in the expected location, the command will exit immediately. Detailed documentation can be found on the GNU make documentation page Software Stacks with Modules The Workspace module and the CustomRepo module provide a pre-defined setup where software stacks for the different CPU architectures as well as a generic one is accessible by default. After loading the module you will always see all the generic software stack and the software stack for the CPU architecture you are located on. If you install your packages into this structure, your modules (for the correct architecture) can be accessed without additional effort. In general you find a structure like: /path/to/workspace/Software +-- sandybridge.el7 | +-- ... +-- ivybridge.el7 | +-- ... +-- broadwell.el7 | +-- modulefiles | +-- all # place modulefiles here with structure Name/version | +-- ProdXY | +-- 0.1 # a modulefile example | +-- easybuild # the EasyBuild software directory, could be used OR | +-- software # you could install under this directory +-- generic.el7 | +-- modulefiles | +-- all # place modulefiles here with structure Name/version | +-- ProdXY | +-- 0.1 # a modulefile example | +-- easybuild # the EasyBuild software directory, could be used OR +-- sources The example shows detailed structure for Broadwell and generic software stack. But the same structure can be found also for the other stacks. The most important is the location of the modulefiles. These should be located in: EASYBUILD_PREFIX/../modulefiles/all for the architectural software stack AND CUSTOM_GENERIC_EASYBUILD_PREFIX/../modulefiles/all for the generic software stack The prefix (set in the Workspace and CustomRepo module) contains architectural information, here for example for Broadwell: EASYBUILD_PREFIX=/storage/workspaces/hpc-group/project1/Software/broadwell.el7/easybuild Modulefiles A modulefile describes location and environment setting for the targeted application, e.g. setting the PATH , LD_LIBRARY_PATH and other variables. The present Lmod system searches these Modulefiles in subdirectories of all directories registered in MODULEPATH . The above described architectural software stacks as well as the generic one are registered in the Workspace and CustomRepo module by default. There are, two types of modules, the default Linux modules, written in TCL (described below) and Lua modules (created by our EasyBuild). Lua modules are more powerful, but for simplicity we present TCL modules here. Assuming we want to provide an application ProdXY . We could create a TCL module file $WORKSPACE/Software/generic.el7/modulefiles/ProdXY as the following: #%module conflict ProdABC # conflicts with another application ProdABC module load Python/3.8.2-GCCcore-9.3.0 # load other additional modules # provide a description whatis \"The ProdXY for doing clever things.\" proc ModulesHelp { } { puts stderr \"This module loads the ProdXY tool.\" puts stderr \"\\t the executable prodXY is provided.\" } # set the path to the software product (can be used later in the module) set PKG_PREFIX /path/to/software/package/ProdXY # add the location of binaries to PATH, such they are immediately accessible prepend-path PATH $PKG_PREFIX/bin # add to library path for dynamically linked applications prepend-path LD_LIBRARY_PATH $PKG_PREFIX/lib # add a location for Python packages prepend-path PYTHONPATH $PKG_PREFIX/lib/python3.8/site-packages/ # for example, you can set environment variables for compiling setenv CFLAGS \"-DNDEBUG\" In the first lines, we can set conflicts with other modules (here named ProdABC). Then we load some dependency modules and provide some description. The additional lines depend on your requirements for the application. With set you can define internal variables (within this modulefile). The command setenv defines a environment variable, set in your environment after loading the module. And prepend-path and append-path extend an environment variable at the front or end. There are common environment variables like: PATH for providing executables, LD_LIBRARY_PATH location of libraries, e.g. prodXY.so , PYTHONPATH providing Python modules, CONDA_ENVS_PATH for providing Conda environments, etc. And others which are very application specific. If the module is in a registered location, it can be loaded by: module load ProdXY","title":"Installing Custom Software"},{"location":"software/installing-custom-software.html#installing-custom-software","text":"","title":"Installing Custom Software"},{"location":"software/installing-custom-software.html#description","text":"UBELIX comes with a plethora of software pre-installed. And there are tools provided installing additional packages in the user/group space. The present CustomRepo and Workspace modules provide easy access even for multiple versions of the same Software package. In general, the module environment is described at HPC software environment . The command module avail lists the available packages and module spider FFTW searches for all modules which have FFTW in their name. This article describes a procedure for installing custom software stacks in your user/group space. An EasyBuild and a manual approach is presented. Note You cannot use the packet management utility yum for this, since this command requires root privileges to install software system wide. Instead you have to compile and install the software yourself. Note If you know that some missing software could be of general interest for a wide community on our machines, you can ask us to install the software system wide. Bioinformatic packages are managed by Vital-IT Note If you need further assistance in installing your software packages or optimizing for our machine architecture, get in touch with our support team. The LMOD module system allows to enable software package by package. Thus, influences between different packages can be minimized. It also allows to have multiple versions of the same software product installed side by side. See HPC software environment . There are modules providing access to your user/group software stacks and assisting you with building packages into them. When possible we use EasyBuild to provision software packages. EasyBuild is a software installation framework. Installation procedures are defined in so called EasyConfigs, including the location of the sources, dependencies, its versions, used environments, compile arguments, and more. These EasyConfigs are publicly available on EasyBuild github repository and can be downloaded used and if necessary adapted, e.g. for new versions.","title":"Description"},{"location":"software/installing-custom-software.html#building-software-packages","text":"There are mainly two options to build a software package and its module: using EasyBuild , with an existing, an modified or a newly created EasyConfig performing manual installation , and creating a module file In the following, both methods are described in more details. In general we consider: Is there already and EasyBuild recipe available, which can be used or modified? Which software stack we want to build into, a specific CPU architecture, OR all CPU architectures, OR should it be a generic one (see Software stacks )","title":"Building Software packages"},{"location":"software/installing-custom-software.html#software-stacks","text":"We provide for all CPU architecture a software stack, where applications are build on and for this specific architecture. Therewith, application can use different optimizations, like instruction sets (e.g. using AVX2 on Broadwell) on the different architectures. Furthermore, we provide a generic software stack, where architecture independent applications are installed, e.g. Python packages. Our generic software stack is build on Sandybridge nodes, which have the smallest instruction set, and thus we aim preventing conflicts. For EasyBuild we provide tools which can automatically install in all architectural or the generic software stack, see below. Additionally, we provide modules which provide access and assist building such software stacks in your user/group space.","title":"Software stacks"},{"location":"software/installing-custom-software.html#easybuild","text":"For detailed instructions read the EasyBuild article . If you are installing your own application you may want to consider to create an EasyConfig for it. Have a look in the EasyBuild documentation , examples on the EasyBuild github . And if necessary ask our support team for assistance.","title":"EasyBuild"},{"location":"software/installing-custom-software.html#manually-compiling","text":"There are very different ways of manually installing software packages, starting from just using a compiler, having a makefile, up to complex build systems. A few considerations need to kept in mind while building for our systems: Compilers: different compilers are available and can be loaded by modules. Toolchains bundle compiler with additionally libraries and tools, like MPI, FFTW, MKL, see Toolchains . Furthermore, complex algorithms are optimised differently in the compilers. It is worthwhile to try and compare multiple compilers. CPU architectures: since there are different CPU architectures available, applications should be build for the targeted architecture. Often significant performance improvements can be obtained compiling for the correct instruction sets. Therefore, launch your build processes on the targeted architecture. Accessibility: On the one hand probably different versions, e.g. for compiler and CPU architecture should be provided. On the other hand the access to it should be as easy as possible for all users of that package. Therefore, modules provide a user-friendly. These modules can be organized e.g. in software stacks, one for each architecture. One and probably the most used procedure is the GNU configure / make: tar xzvf some-software-0.1.tar.gz cd some-software-0.1 ./configure --prefix = /path/to/target/some-software/0.1 make make install make clean Note Consider creating an Easyconfig if you already have a well tested procedure, see EasyBuild . configure is usually a complex shell script that gathers information about the system and prepares the compile process. With the --prefix option you can specify a base installation directory, where make install will install the files into subdirectories like bin , lib , include , etc. The make utility is what does the actual compiling and linking. If, for example, some additional libraries are missing on the system or not found in the expected location, the command will exit immediately. Detailed documentation can be found on the GNU make documentation page","title":"Manually compiling"},{"location":"software/installing-custom-software.html#software-stacks-with-modules","text":"The Workspace module and the CustomRepo module provide a pre-defined setup where software stacks for the different CPU architectures as well as a generic one is accessible by default. After loading the module you will always see all the generic software stack and the software stack for the CPU architecture you are located on. If you install your packages into this structure, your modules (for the correct architecture) can be accessed without additional effort. In general you find a structure like: /path/to/workspace/Software +-- sandybridge.el7 | +-- ... +-- ivybridge.el7 | +-- ... +-- broadwell.el7 | +-- modulefiles | +-- all # place modulefiles here with structure Name/version | +-- ProdXY | +-- 0.1 # a modulefile example | +-- easybuild # the EasyBuild software directory, could be used OR | +-- software # you could install under this directory +-- generic.el7 | +-- modulefiles | +-- all # place modulefiles here with structure Name/version | +-- ProdXY | +-- 0.1 # a modulefile example | +-- easybuild # the EasyBuild software directory, could be used OR +-- sources The example shows detailed structure for Broadwell and generic software stack. But the same structure can be found also for the other stacks. The most important is the location of the modulefiles. These should be located in: EASYBUILD_PREFIX/../modulefiles/all for the architectural software stack AND CUSTOM_GENERIC_EASYBUILD_PREFIX/../modulefiles/all for the generic software stack The prefix (set in the Workspace and CustomRepo module) contains architectural information, here for example for Broadwell: EASYBUILD_PREFIX=/storage/workspaces/hpc-group/project1/Software/broadwell.el7/easybuild","title":"Software Stacks with Modules"},{"location":"software/installing-custom-software.html#modulefiles","text":"A modulefile describes location and environment setting for the targeted application, e.g. setting the PATH , LD_LIBRARY_PATH and other variables. The present Lmod system searches these Modulefiles in subdirectories of all directories registered in MODULEPATH . The above described architectural software stacks as well as the generic one are registered in the Workspace and CustomRepo module by default. There are, two types of modules, the default Linux modules, written in TCL (described below) and Lua modules (created by our EasyBuild). Lua modules are more powerful, but for simplicity we present TCL modules here. Assuming we want to provide an application ProdXY . We could create a TCL module file $WORKSPACE/Software/generic.el7/modulefiles/ProdXY as the following: #%module conflict ProdABC # conflicts with another application ProdABC module load Python/3.8.2-GCCcore-9.3.0 # load other additional modules # provide a description whatis \"The ProdXY for doing clever things.\" proc ModulesHelp { } { puts stderr \"This module loads the ProdXY tool.\" puts stderr \"\\t the executable prodXY is provided.\" } # set the path to the software product (can be used later in the module) set PKG_PREFIX /path/to/software/package/ProdXY # add the location of binaries to PATH, such they are immediately accessible prepend-path PATH $PKG_PREFIX/bin # add to library path for dynamically linked applications prepend-path LD_LIBRARY_PATH $PKG_PREFIX/lib # add a location for Python packages prepend-path PYTHONPATH $PKG_PREFIX/lib/python3.8/site-packages/ # for example, you can set environment variables for compiling setenv CFLAGS \"-DNDEBUG\" In the first lines, we can set conflicts with other modules (here named ProdABC). Then we load some dependency modules and provide some description. The additional lines depend on your requirements for the application. With set you can define internal variables (within this modulefile). The command setenv defines a environment variable, set in your environment after loading the module. And prepend-path and append-path extend an environment variable at the front or end. There are common environment variables like: PATH for providing executables, LD_LIBRARY_PATH location of libraries, e.g. prodXY.so , PYTHONPATH providing Python modules, CONDA_ENVS_PATH for providing Conda environments, etc. And others which are very application specific. If the module is in a registered location, it can be loaded by: module load ProdXY","title":"Modulefiles"},{"location":"software/matlab.html","text":"Matlab Description UBELIX is always featuring the latest two (b)-releases of Matlab. Facts about Matlab on UBELIX It can run in parallel on one node , thanks to the Parallel Computing ToolBox It can take advantage of GPUs It cannot run on more than one node as we do not have the Distributed Computing Toolbox. Matlab is NOT FREE to use! Every user using Matlab on UBELIX must have at least one valid license. You can buy licenses at our software shop . MATLAB Version: 9.3.0.713579 (R2017b) contains: MATLAB Version: 9.1.0.441655 (R2016b) contains: Simulink Version 9.0 (R2017b) Simulink Version 8.8 (R2016b) Bioinformatics Toolbox Version 4.9 (R2017b) Communications System Toolbox Version 6.3 (R2016b) Communications System Toolbox Version 6.5 (R2017b) Computer Vision System Toolbox Version 7.2 (R2016b) Computer Vision System Toolbox Version 8.0 (R2017b) Computer Vision System Toolbox Version 7.2 (R2016b) Control System Toolbox Version 10.3 (R2017b) Curve Fitting Toolbox Version 3.5.4 (R2016b) Curve Fitting Toolbox Version 3.5.6 (R2017b) DSP System Toolbox Version 9.3 (R2016b) DSP System Toolbox Version 9.5 (R2017b) Database Toolbox Version 7.0 (R2016b) Database Toolbox Version 8.0 (R2017b) Financial Toolbox Version 5.8 (R2016b) Financial Toolbox Version 5.10 (R2017b) Fixed-Point Designer Version 5.3 (R2016b) Fixed-Point Designer Version 6.0 (R2017b) Fuzzy Logic Toolbox Version 2.2.24 (R2016b) Fuzzy Logic Toolbox Version 2.3 (R2017b) Image Acquisition Toolbox Version 5.1 (R2016b) Global Optimization Toolbox Version 3.4.3 (R2017b) Image Processing Toolbox Version 9.5 (R2016b) Image Acquisition Toolbox Version 5.3 (R2017b) MATLAB Coder Version 3.2 (R2016b) Image Processing Toolbox Version 10.1 (R2017b) MATLAB Compiler Version 6.3 (R2016b) Instrument Control Toolbox Version 3.12 (R2017b) MATLAB Compiler SDK Version 6.3 (R2016b) MATLAB Coder Version 3.4 (R2017b) Mapping Toolbox Version 4.4 (R2016b) MATLAB Compiler Version 6.5 (R2017b) Neural Network Toolbox Version 9.1 (R2016b) MATLAB Compiler SDK Version 6.4 (R2017b) Optimization Toolbox Version 7.5 (R2016b) Mapping Toolbox Version 4.5.1 (R2017b) Parallel Computing Toolbox Version 6.9 (R2016b) Model Predictive Control Toolbox Version 6.0 (R2017b) Partial Differential Equation Toolbox Version 2.3 (R2016b) Neural Network Toolbox Version 11.0 (R2017b) Robust Control Toolbox Version 6.2 (R2016b) Optimization Toolbox Version 8.0 (R2017b) Signal Processing Toolbox Version 7.3 (R2016b) Parallel Computing Toolbox Version 6.11 (R2017b) Simscape Version 4.1 (R2016b) Partial Differential Equation Toolbox Version 2.5 (R2017b) Simscape Multibody Version 4.9 (R2016b) Robust Control Toolbox Version 6.4 (R2017b) Simscape Power Systems Version 6.6 (R2016b) Signal Processing Toolbox Version 7.5 (R2017b) Simulink Coder Version 8.11 (R2016b) Simscape Version 4.3 (R2017b) Simulink Control Design Version 4.4 (R2016b) Simscape Multibody Version 5.1 (R2017b) Simulink Design Optimization Version 3.1 (R2016b) Simscape Power Systems Version 6.8 (R2017b) Simulink Verification and Validation Version 3.12 (R2016b) Simulink Check Version 4.0 (R2017b) Stateflow Version 8.8 (R2016b) Simulink Coder Version 8.13 (R2017b) Statistics and Machine Learning Toolbox Version 11.0 (R2016b) Simulink Control Design Version 5.0 (R2017b) Symbolic Math Toolbox Version 7.1 (R2016b) Simulink Coverage Version 4.0 (R2017b) System Identification Toolbox Version 9.5 (R2016b) Simulink Design Optimization Version 3.3 (R2017b) Wavelet Toolbox Version 4.17 (R2016b) Simulink Requirements Version 1.0 (R2017b) Stateflow Version 9.0 (R2017b) Statistics and Machine Learning Toolbox Version 11.2 (R2017b) Symbolic Math Toolbox Version 8.0 (R2017b) System Identification Toolbox Version 9.7 (R2017b) Wavelet Toolbox Version 4.19 (R2017b) Running Matlab on the Compute Nodes Submitting a Matlab job to the cluster is very similar to submitting any other serial job. Lets try to run a simple Matlab script which we will put in a file boxfilter.m boxfilter.m % Compute a local mean filter over a neighborhood of 11x11 pixels % Read image into workspace: original = imread ( 'girlface.png' ) ; % Perform the mean filtering: filtered = imboxfilt ( original, 11 ) ; % Save the original and the filtered image side-by-side: imwrite ([ original, filtered ] , 'comparison.png' ) ; Now we need a submission script boxfilter.qsub !#/bin/bash #SBATCH -mail-user=foo@bar.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --job-name=boxfilter #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=2G # Load Matlab form the environment modules module load matlab/R2015b # Tell Matlab to run our box filter.m file and exit matlab -nodisplay -r \"boxfilter, exit\" Passing Arguments to a m-File There are several ways to provide input arguments in Matlab. Define the Variables Before Running the Script Lets take the box filter.m example from above. The script is not universal because the name of the input image and the box size is hardcoded in the script. We make the script more generally usable by: boxfilter.m % Compute a local mean filter over a neighborhood of 11x11 pixels % Read image into workspace: original = imread ( inputImg ) ; % Perform the mean filtering: filtered = imboxfilt ( original, x ) ; % Save the original and the filtered image side-by-side: imwrite ([ original, filtered ] , 'comparison.png' ) ; and then: boxfilter.qsub !#/bin/bash ( ... ) # Load Matlab form the environment modules module load matlab/R2015b # Tell Matlab to run our box filter.m file and exit matlab -nodisplay -r \"inputImg='girlface.png'; x=11; boxfilter, exit\" Advanced Topics Multithreading By default, MATLAB makes use of the multithreading capabilities of the node on which it is running. It is crucial that you allocate the same number of slots for your job as your job utilizes cores. Disable Computational Multithreading If you do not need multithreading for your application consider to disable computational multithreading by setting the -singleCompThread option when starting MATLAB: matlab -nodisplay -singleCompThread -r \"boxfilter('girlface.png', 'comparison.png', 11); exit\" Disable Computational Multithreading If you do not need multithreading for your application consider to disable computational multithreading by setting the -singleCompThread option when starting MATLAB: matlab -nodisplay -singleCompThread -r \"boxfilter('girlface.png', 'comparison.png', 11); exit\" Running MATLAB in Multithreaded Mode Most of the time, running MATLAB in single-threaded mode will meet your needs. If you have mathematically intense computations that might benefit from multi-threading capabilities provided by MATLAB\u2019s BLAS implementation, then you should limit MATLAB to a well defined number of threads, so that you can allocate the correct number of slots for your job. Use the maxNumCompThreads(N) function to control the number of computational threads:","title":"Matlab"},{"location":"software/matlab.html#matlab","text":"","title":"Matlab"},{"location":"software/matlab.html#description","text":"UBELIX is always featuring the latest two (b)-releases of Matlab.","title":"Description"},{"location":"software/matlab.html#facts-about-matlab-on-ubelix","text":"It can run in parallel on one node , thanks to the Parallel Computing ToolBox It can take advantage of GPUs It cannot run on more than one node as we do not have the Distributed Computing Toolbox. Matlab is NOT FREE to use! Every user using Matlab on UBELIX must have at least one valid license. You can buy licenses at our software shop . MATLAB Version: 9.3.0.713579 (R2017b) contains: MATLAB Version: 9.1.0.441655 (R2016b) contains: Simulink Version 9.0 (R2017b) Simulink Version 8.8 (R2016b) Bioinformatics Toolbox Version 4.9 (R2017b) Communications System Toolbox Version 6.3 (R2016b) Communications System Toolbox Version 6.5 (R2017b) Computer Vision System Toolbox Version 7.2 (R2016b) Computer Vision System Toolbox Version 8.0 (R2017b) Computer Vision System Toolbox Version 7.2 (R2016b) Control System Toolbox Version 10.3 (R2017b) Curve Fitting Toolbox Version 3.5.4 (R2016b) Curve Fitting Toolbox Version 3.5.6 (R2017b) DSP System Toolbox Version 9.3 (R2016b) DSP System Toolbox Version 9.5 (R2017b) Database Toolbox Version 7.0 (R2016b) Database Toolbox Version 8.0 (R2017b) Financial Toolbox Version 5.8 (R2016b) Financial Toolbox Version 5.10 (R2017b) Fixed-Point Designer Version 5.3 (R2016b) Fixed-Point Designer Version 6.0 (R2017b) Fuzzy Logic Toolbox Version 2.2.24 (R2016b) Fuzzy Logic Toolbox Version 2.3 (R2017b) Image Acquisition Toolbox Version 5.1 (R2016b) Global Optimization Toolbox Version 3.4.3 (R2017b) Image Processing Toolbox Version 9.5 (R2016b) Image Acquisition Toolbox Version 5.3 (R2017b) MATLAB Coder Version 3.2 (R2016b) Image Processing Toolbox Version 10.1 (R2017b) MATLAB Compiler Version 6.3 (R2016b) Instrument Control Toolbox Version 3.12 (R2017b) MATLAB Compiler SDK Version 6.3 (R2016b) MATLAB Coder Version 3.4 (R2017b) Mapping Toolbox Version 4.4 (R2016b) MATLAB Compiler Version 6.5 (R2017b) Neural Network Toolbox Version 9.1 (R2016b) MATLAB Compiler SDK Version 6.4 (R2017b) Optimization Toolbox Version 7.5 (R2016b) Mapping Toolbox Version 4.5.1 (R2017b) Parallel Computing Toolbox Version 6.9 (R2016b) Model Predictive Control Toolbox Version 6.0 (R2017b) Partial Differential Equation Toolbox Version 2.3 (R2016b) Neural Network Toolbox Version 11.0 (R2017b) Robust Control Toolbox Version 6.2 (R2016b) Optimization Toolbox Version 8.0 (R2017b) Signal Processing Toolbox Version 7.3 (R2016b) Parallel Computing Toolbox Version 6.11 (R2017b) Simscape Version 4.1 (R2016b) Partial Differential Equation Toolbox Version 2.5 (R2017b) Simscape Multibody Version 4.9 (R2016b) Robust Control Toolbox Version 6.4 (R2017b) Simscape Power Systems Version 6.6 (R2016b) Signal Processing Toolbox Version 7.5 (R2017b) Simulink Coder Version 8.11 (R2016b) Simscape Version 4.3 (R2017b) Simulink Control Design Version 4.4 (R2016b) Simscape Multibody Version 5.1 (R2017b) Simulink Design Optimization Version 3.1 (R2016b) Simscape Power Systems Version 6.8 (R2017b) Simulink Verification and Validation Version 3.12 (R2016b) Simulink Check Version 4.0 (R2017b) Stateflow Version 8.8 (R2016b) Simulink Coder Version 8.13 (R2017b) Statistics and Machine Learning Toolbox Version 11.0 (R2016b) Simulink Control Design Version 5.0 (R2017b) Symbolic Math Toolbox Version 7.1 (R2016b) Simulink Coverage Version 4.0 (R2017b) System Identification Toolbox Version 9.5 (R2016b) Simulink Design Optimization Version 3.3 (R2017b) Wavelet Toolbox Version 4.17 (R2016b) Simulink Requirements Version 1.0 (R2017b) Stateflow Version 9.0 (R2017b) Statistics and Machine Learning Toolbox Version 11.2 (R2017b) Symbolic Math Toolbox Version 8.0 (R2017b) System Identification Toolbox Version 9.7 (R2017b) Wavelet Toolbox Version 4.19 (R2017b)","title":"Facts about Matlab on UBELIX"},{"location":"software/matlab.html#running-matlab-on-the-compute-nodes","text":"Submitting a Matlab job to the cluster is very similar to submitting any other serial job. Lets try to run a simple Matlab script which we will put in a file boxfilter.m boxfilter.m % Compute a local mean filter over a neighborhood of 11x11 pixels % Read image into workspace: original = imread ( 'girlface.png' ) ; % Perform the mean filtering: filtered = imboxfilt ( original, 11 ) ; % Save the original and the filtered image side-by-side: imwrite ([ original, filtered ] , 'comparison.png' ) ; Now we need a submission script boxfilter.qsub !#/bin/bash #SBATCH -mail-user=foo@bar.unibe.ch #SBATCH --mail-type=end,fail #SBATCH --job-name=boxfilter #SBATCH --time=00:10:00 #SBATCH --mem-per-cpu=2G # Load Matlab form the environment modules module load matlab/R2015b # Tell Matlab to run our box filter.m file and exit matlab -nodisplay -r \"boxfilter, exit\"","title":"Running Matlab on the Compute Nodes"},{"location":"software/matlab.html#passing-arguments-to-a-m-file","text":"There are several ways to provide input arguments in Matlab.","title":"Passing Arguments to a m-File"},{"location":"software/matlab.html#define-the-variables-before-running-the-script","text":"Lets take the box filter.m example from above. The script is not universal because the name of the input image and the box size is hardcoded in the script. We make the script more generally usable by: boxfilter.m % Compute a local mean filter over a neighborhood of 11x11 pixels % Read image into workspace: original = imread ( inputImg ) ; % Perform the mean filtering: filtered = imboxfilt ( original, x ) ; % Save the original and the filtered image side-by-side: imwrite ([ original, filtered ] , 'comparison.png' ) ; and then: boxfilter.qsub !#/bin/bash ( ... ) # Load Matlab form the environment modules module load matlab/R2015b # Tell Matlab to run our box filter.m file and exit matlab -nodisplay -r \"inputImg='girlface.png'; x=11; boxfilter, exit\"","title":"Define the Variables Before Running the Script"},{"location":"software/matlab.html#advanced-topics","text":"","title":"Advanced Topics"},{"location":"software/matlab.html#multithreading","text":"By default, MATLAB makes use of the multithreading capabilities of the node on which it is running. It is crucial that you allocate the same number of slots for your job as your job utilizes cores. Disable Computational Multithreading If you do not need multithreading for your application consider to disable computational multithreading by setting the -singleCompThread option when starting MATLAB: matlab -nodisplay -singleCompThread -r \"boxfilter('girlface.png', 'comparison.png', 11); exit\" Disable Computational Multithreading If you do not need multithreading for your application consider to disable computational multithreading by setting the -singleCompThread option when starting MATLAB: matlab -nodisplay -singleCompThread -r \"boxfilter('girlface.png', 'comparison.png', 11); exit\" Running MATLAB in Multithreaded Mode Most of the time, running MATLAB in single-threaded mode will meet your needs. If you have mathematically intense computations that might benefit from multi-threading capabilities provided by MATLAB\u2019s BLAS implementation, then you should limit MATLAB to a well defined number of threads, so that you can allocate the correct number of slots for your job. Use the maxNumCompThreads(N) function to control the number of computational threads:","title":"Multithreading"},{"location":"software/pre-installed-software.html","text":"Pre-Installed Software Description This page contains a list of pre-installed software that is available for all UBELIX users. If you want to install custom software yourself, take a look here . If you think that some missing software could be of general interest for the UBELIX community, you can ask us to install the software system wide. Since maintaining software is a lot of work, we will select carefully which software we will install globally. Environment Modules To make certain versions of a software available, the user must first \u201cload/add\u201d the corresponding modulefile. Environment modules allow to maintain different versions of the same software by altering the shell environment variables (e.g. $PATH) accordingly. Each modulefile contains all information needed to configure the shell for a specific software. Toolchains If you need to compile any application, we suggest to load a toolchain. There are various toolchains available, to list some: Toolchain packacges GCC GCC compiler gompi GCC, OpenMPI gompic GCC, OpenMPI, CUDA foss GCC, OpenMPI, OpenBLAS, FFTW, ScaLAPACK fosscuda GCC, OpenMPI, OpenBLAS, FFTW, ScaLAPACK, CUDA intel Intel compiler, (GCC required), MKL, Intel MPI iompi Intel compiler, OpenMPI iomkl Intel compiler, OpenMPI, MKL Note: this list is not meant to be complete but to give an idea of high level toolchains. you can list all available toolchains using module avail and see the containing packages using, e.g. module show gompic List all Available Modulefiles $ module avail --------------------------------------------------------------------------------------- /software.el7/modulefiles/all ---------------------------------------------------------------------------------------- Advisor/2018_update3 foss/2017a iccifort/2017.4.196-GCC-6.4.0-2.28 numactl/2.0.11-GCC-5.4.0-2.26 Autotools/20150215-GCC-5.4.0-2.26 foss/2017b iccifort/2018.1.163-GCC-6.4.0-2.28 numactl/2.0.11-GCC-6.3.0-2.27 Autotools/20150215-GCCcore-6.3.0 foss/2018a iccifort/2018.3.222-GCC-7.3.0-2.30 ( D ) numactl/2.0.11-GCCcore-6.4.0 Autotools/20170619-GCCcore-6.4.0 foss/2018b ICU/61.1-GCCcore-6.4.0 numactl/2.0.11-GCCcore-7.3.0 Autotools/20180311-GCCcore-7.3.0 foss/2019a ( D ) IDL/8.6 numactl/2.0.12-GCCcore-8.2.0 ( D ) Autotools/20180311-GCCcore-8.2.0 ( D ) fosscuda/2019a iimpi/2017b OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1 Boost/1.66.0-foss-2018a Gaussian/g09.D01 iimpi/2018a OpenBLAS/0.2.19-GCC-6.3.0-2.27-LAPACK-3.7.0 CMake/3.9.1-GCCcore-6.4.0 Gaussian/g16.A03 ( D ) iimpi/2018b ( D ) OpenBLAS/0.2.20-GCC-6.4.0-2.28 CMake/3.9.5-GCCcore-6.4.0 GCC/5.4.0-2.26 imkl/2017.3.196-iimpi-2017b OpenBLAS/0.3.1-GCC-7.3.0-2.30 CMake/3.10.2-GCCcore-6.4.0 GCC/6.3.0-2.27 imkl/2017.3.196-iompi-2017b OpenBLAS/0.3.5-GCC-8.2.0-2.31.1 ( D ) CMake/3.11.1-GCCcore-6.4.0 GCC/6.4.0-2.28 imkl/2018.1.163-iimpi-2018a OpenMPI/1.10.3-GCC-5.4.0-2.26 CMake/3.11.4-GCCcore-7.3.0 ( D ) GCC/7.3.0-2.30 imkl/2018.1.163-iompi-2018a OpenMPI/2.0.2-GCC-6.3.0-2.27 CUDA/8.0.61 GCC/8.2.0-2.31.1 ( D ) imkl/2018.3.222-iimpi-2018b OpenMPI/2.1.1-GCC-6.4.0-2.28 CUDA/9.0.176 GCCcore/5.4.0 imkl/2018.3.222-iompi-2018b ( D ) OpenMPI/2.1.1-iccifort-2017.4.196-GCC-6.4.0-2.28 CUDA/9.1.85 GCCcore/6.3.0 impi/2017.3.196-iccifort-2017.4.196-GCC-6.4.0-2.28 OpenMPI/2.1.2-GCC-6.4.0-2.28 CUDA/9.2.88 GCCcore/6.4.0 impi/2018.1.163-iccifort-2018.1.163-GCC-6.4.0-2.28 OpenMPI/2.1.2-iccifort-2018.1.163-GCC-6.4.0-2.28 CUDA/10.1.105-GCC-8.2.0-2.31.1 ( D ) GCCcore/7.3.0 impi/2018.3.222-iccifort-2018.3.222-GCC-7.3.0-2.30 ( D ) OpenMPI/3.1.1-GCC-7.3.0-2.30 cuDNN/6.0-CUDA-8.0.61 GCCcore/8.2.0 ( D ) Inspector/2018_update3 OpenMPI/3.1.1-iccifort-2018.3.222-GCC-7.3.0-2.30 cuDNN/7.0.5-CUDA-9.0.176 gcccuda/2019a intel/2017b OpenMPI/3.1.3-GCC-8.2.0-2.31.1 cuDNN/7.0.5-CUDA-9.1.85 GEOS/3.6.2-foss-2018a-Python-3.6.4 intel/2018a OpenMPI/3.1.3-gcccuda-2019a ( D ) cuDNN/7.1.4-CUDA-9.2.88 git-lfs/2.4.2 intel/2018b ( D ) Perl/5.26.1-GCCcore-6.4.0 cuDNN/7.6.0.64-gcccuda-2019a ( D ) GMP/6.1.2-GCCcore-6.4.0 iomkl/2017b PGI/17.10-GCC-6.4.0-2.28 cURL/7.58.0-GCCcore-6.4.0 gompi/2016b iomkl/2018a PGI/18.4-GCC-6.4.0-2.28 cURL/7.60.0-GCCcore-7.3.0 ( D ) gompi/2017a iomkl/2018b ( D ) PGI/19.4-GCC-8.2.0-2.31.1 ( D ) Doxygen/1.8.13-GCCcore-6.4.0 gompi/2017b iompi/2017b Python/2.7.14-foss-2018a Doxygen/1.8.14-GCCcore-7.3.0 ( D ) gompi/2018a iompi/2018a Python/2.7.14-GCCcore-6.4.0-bare EasyBuild/3.6.1 gompi/2018b iompi/2018b ( D ) Python/3.6.4-foss-2018a ( D ) EasyBuild/3.6.2 gompi/2019a ( D ) itac/2018.3.022 R/3.4.4-foss-2018a-X11-20180131 EasyBuild/3.7.1 gompic/2019a Java/1.7.0_60 ScaLAPACK/2.0.2-gompi-2016b-OpenBLAS-0.2.18-LAPACK-3.6.1 EasyBuild/3.9.1 ( D ) GSL/2.4-GCCcore-6.4.0 Java/1.7.0_80 ScaLAPACK/2.0.2-gompi-2017a-OpenBLAS-0.2.19-LAPACK-3.7.0 FFTW/3.3.4-gompi-2016b HDF5/1.10.1-foss-2018a Java/1.8.0_121 ScaLAPACK/2.0.2-gompi-2017b-OpenBLAS-0.2.20 FFTW/3.3.6-gompi-2017a HDF5/1.10.2-foss-2018b ( D ) Java/1.8.0_152 ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20 FFTW/3.3.6-gompi-2017b help2man/1.47.4-GCCcore-6.3.0 Java/1.8.0_162 ( D ) ScaLAPACK/2.0.2-gompi-2018b-OpenBLAS-0.3.1 FFTW/3.3.7-gompi-2018a help2man/1.47.4-GCCcore-6.4.0 libsndfile/1.0.28-GCCcore-6.4.0 ScaLAPACK/2.0.2-gompi-2019a-OpenBLAS-0.3.5 FFTW/3.3.7-intel-2017b help2man/1.47.4-GCCcore-7.3.0 LLVM/5.0.1-GCCcore-6.4.0 ScaLAPACK/2.0.2-gompic-2019a-OpenBLAS-0.3.5 ( D ) FFTW/3.3.7-intel-2018a help2man/1.47.4 Mako/1.0.7-foss-2018a-Python-2.7.14 SQLite/3.20.1-GCCcore-6.4.0 FFTW/3.3.7-iomkl-2018a help2man/1.47.7-GCCcore-8.2.0 ( D ) MATLAB/2016b SQLite/3.21.0-GCCcore-6.4.0 ( D ) FFTW/3.3.8-gompi-2018b hwloc/1.11.3-GCC-5.4.0-2.26 MATLAB/2017b tmux/2.7 FFTW/3.3.8-gompi-2019a hwloc/1.11.5-GCC-6.3.0-2.27 MATLAB/2018b ( D ) UDUNITS/2.2.26-foss-2018a FFTW/3.3.8-gompic-2019a hwloc/1.11.7-GCCcore-6.4.0 netCDF/4.6.0-foss-2018a vital-it/7 FFTW/3.3.8-intel-2018b hwloc/1.11.8-GCCcore-6.4.0 netCDF/4.6.1-foss-2018b ( D ) VTune/2018_update3 FFTW/3.3.8-iomkl-2018b ( D ) hwloc/1.11.10-GCCcore-7.3.0 nettle/3.4-foss-2018a X11/20180131-GCCcore-6.4.0 foss/2016b hwloc/1.11.11-GCCcore-8.2.0 ( D ) NLopt/2.4.2-foss-2018a Where: D: Default Module Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\" . Bioinformatics Software In co-operation with the Vital-IT Group of the SIB Swiss Institute of Bioinformatics , a large set of bioinformatics software tools and databases is available to the life science community. To also list all modulefiles provided by Vital-IT, you have to first load the vital-it modulefile: Loading the vital-it modulefile automatically configures the environment to use specific versions of selected software, e.g. python v2.7.5, and gcc v4.9.1 $ module load vital-it && module avail ---------------------------------------------------------------------------------------------- /software/module ---------------------------------------------------------------------------------------------- Blast/blast/latest SequenceAnalysis/primer3/2.3.7 UHTS/Analysis/mummer/4.0.0beta1 ( D ) Blast/blast/2.2.26 ( D ) SequenceAnalysis/PrimerDesign/iPCR/1.0 UHTS/Analysis/NanoOK/1.2.6 Blast/ncbi-blast/latest SequenceAnalysis/ProtoGene/4.2.2 UHTS/Analysis/nanoraw/0.5 Blast/ncbi-blast/2.2.31+ SequenceAnalysis/readseq/2.1.30 UHTS/Analysis/oncodrivefm/1.0.3 ( ... ) SequenceAnalysis/OrthologyAnalysis/OMA/2.1.1 UHTS/Analysis/msprime/0.7.0 Utility/rpy2/2.9.1 ( D ) SequenceAnalysis/orthomclSoftware/2.0.9 UHTS/Analysis/MultiQC/1.3 Utility/Solver/SoPlex/4.0.0 SequenceAnalysis/patsearch/1 UHTS/Analysis/MultiQC/1.7 ( D ) Utility/Tarql/1.1 SequenceAnalysis/pftools/2.3.4 UHTS/Analysis/mummer/3.9.4alpha Utility/UCSC-utils/359 SequenceAnalysis/pftools/2.3.5.d ( D ) UHTS/Analysis/mummer/4.0.0beta --------------------------------------------------------------------------------------- /software.el7/modulefiles/all ---------------------------------------------------------------------------------------- Advisor/2018_update3 foss/2017a iccifort/2017.4.196-GCC-6.4.0-2.28 numactl/2.0.11-GCC-5.4.0-2.26 Autotools/20150215-GCC-5.4.0-2.26 foss/2017b iccifort/2018.1.163-GCC-6.4.0-2.28 numactl/2.0.11-GCC-6.3.0-2.27 Autotools/20150215-GCCcore-6.3.0 foss/2018a iccifort/2018.3.222-GCC-7.3.0-2.30 ( D ) numactl/2.0.11-GCCcore-6.4.0 Autotools/20170619-GCCcore-6.4.0 foss/2018b ICU/61.1-GCCcore-6.4.0 numactl/2.0.11-GCCcore-7.3.0 Autotools/20180311-GCCcore-7.3.0 foss/2019a ( D ) IDL/8.6 numactl/2.0.12-GCCcore-8.2.0 ( D ) Autotools/20180311-GCCcore-8.2.0 ( D ) fosscuda/2019a iimpi/2017b OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1 Boost/1.66.0-foss-2018a Gaussian/g09.D01 iimpi/2018a OpenBLAS/0.2.19-GCC-6.3.0-2.27-LAPACK-3.7.0 CMake/3.9.1-GCCcore-6.4.0 Gaussian/g16.A03 ( D ) iimpi/2018b ( D ) OpenBLAS/0.2.20-GCC-6.4.0-2.28 CMake/3.9.5-GCCcore-6.4.0 GCC/5.4.0-2.26 imkl/2017.3.196-iimpi-2017b OpenBLAS/0.3.1-GCC-7.3.0-2.30 CMake/3.10.2-GCCcore-6.4.0 GCC/6.3.0-2.27 imkl/2017.3.196-iompi-2017b OpenBLAS/0.3.5-GCC-8.2.0-2.31.1 ( D ) CMake/3.11.1-GCCcore-6.4.0 GCC/6.4.0-2.28 imkl/2018.1.163-iimpi-2018a OpenMPI/1.10.3-GCC-5.4.0-2.26 CMake/3.11.4-GCCcore-7.3.0 ( D ) GCC/7.3.0-2.30 imkl/2018.1.163-iompi-2018a OpenMPI/2.0.2-GCC-6.3.0-2.27 CUDA/8.0.61 GCC/8.2.0-2.31.1 ( D ) imkl/2018.3.222-iimpi-2018b OpenMPI/2.1.1-GCC-6.4.0-2.28 CUDA/9.0.176 GCCcore/5.4.0 imkl/2018.3.222-iompi-2018b ( D ) OpenMPI/2.1.1-iccifort-2017.4.196-GCC-6.4.0-2.28 CUDA/9.1.85 GCCcore/6.3.0 impi/2017.3.196-iccifort-2017.4.196-GCC-6.4.0-2.28 OpenMPI/2.1.2-GCC-6.4.0-2.28 CUDA/9.2.88 GCCcore/6.4.0 impi/2018.1.163-iccifort-2018.1.163-GCC-6.4.0-2.28 OpenMPI/2.1.2-iccifort-2018.1.163-GCC-6.4.0-2.28 CUDA/10.1.105-GCC-8.2.0-2.31.1 ( D ) GCCcore/7.3.0 impi/2018.3.222-iccifort-2018.3.222-GCC-7.3.0-2.30 ( D ) OpenMPI/3.1.1-GCC-7.3.0-2.30 cuDNN/6.0-CUDA-8.0.61 GCCcore/8.2.0 ( D ) Inspector/2018_update3 OpenMPI/3.1.1-iccifort-2018.3.222-GCC-7.3.0-2.30 cuDNN/7.0.5-CUDA-9.0.176 gcccuda/2019a intel/2017b OpenMPI/3.1.3-GCC-8.2.0-2.31.1 cuDNN/7.0.5-CUDA-9.1.85 GEOS/3.6.2-foss-2018a-Python-3.6.4 intel/2018a OpenMPI/3.1.3-gcccuda-2019a ( D ) cuDNN/7.1.4-CUDA-9.2.88 git-lfs/2.4.2 intel/2018b ( D ) Perl/5.26.1-GCCcore-6.4.0 cuDNN/7.6.0.64-gcccuda-2019a ( D ) GMP/6.1.2-GCCcore-6.4.0 iomkl/2017b PGI/17.10-GCC-6.4.0-2.28 cURL/7.58.0-GCCcore-6.4.0 gompi/2016b iomkl/2018a PGI/18.4-GCC-6.4.0-2.28 cURL/7.60.0-GCCcore-7.3.0 ( D ) gompi/2017a iomkl/2018b ( D ) PGI/19.4-GCC-8.2.0-2.31.1 ( D ) Doxygen/1.8.13-GCCcore-6.4.0 gompi/2017b iompi/2017b Python/2.7.14-foss-2018a Doxygen/1.8.14-GCCcore-7.3.0 ( D ) gompi/2018a iompi/2018a Python/2.7.14-GCCcore-6.4.0-bare EasyBuild/3.6.1 gompi/2018b iompi/2018b ( D ) Python/3.6.4-foss-2018a ( D ) EasyBuild/3.6.2 gompi/2019a ( D ) itac/2018.3.022 R/3.4.4-foss-2018a-X11-20180131 EasyBuild/3.7.1 gompic/2019a Java/1.7.0_60 ScaLAPACK/2.0.2-gompi-2016b-OpenBLAS-0.2.18-LAPACK-3.6.1 EasyBuild/3.9.1 ( D ) GSL/2.4-GCCcore-6.4.0 Java/1.7.0_80 ScaLAPACK/2.0.2-gompi-2017a-OpenBLAS-0.2.19-LAPACK-3.7.0 FFTW/3.3.4-gompi-2016b HDF5/1.10.1-foss-2018a Java/1.8.0_121 ScaLAPACK/2.0.2-gompi-2017b-OpenBLAS-0.2.20 FFTW/3.3.6-gompi-2017a HDF5/1.10.2-foss-2018b ( D ) Java/1.8.0_152 ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20 FFTW/3.3.6-gompi-2017b help2man/1.47.4-GCCcore-6.3.0 Java/1.8.0_162 ( D ) ScaLAPACK/2.0.2-gompi-2018b-OpenBLAS-0.3.1 FFTW/3.3.7-gompi-2018a help2man/1.47.4-GCCcore-6.4.0 libsndfile/1.0.28-GCCcore-6.4.0 ScaLAPACK/2.0.2-gompi-2019a-OpenBLAS-0.3.5 FFTW/3.3.7-intel-2017b help2man/1.47.4-GCCcore-7.3.0 LLVM/5.0.1-GCCcore-6.4.0 ScaLAPACK/2.0.2-gompic-2019a-OpenBLAS-0.3.5 ( D ) FFTW/3.3.7-intel-2018a help2man/1.47.4 Mako/1.0.7-foss-2018a-Python-2.7.14 SQLite/3.20.1-GCCcore-6.4.0 FFTW/3.3.7-iomkl-2018a help2man/1.47.7-GCCcore-8.2.0 ( D ) MATLAB/2016b SQLite/3.21.0-GCCcore-6.4.0 ( D ) FFTW/3.3.8-gompi-2018b hwloc/1.11.3-GCC-5.4.0-2.26 MATLAB/2017b tmux/2.7 FFTW/3.3.8-gompi-2019a hwloc/1.11.5-GCC-6.3.0-2.27 MATLAB/2018b ( D ) UDUNITS/2.2.26-foss-2018a FFTW/3.3.8-gompic-2019a hwloc/1.11.7-GCCcore-6.4.0 netCDF/4.6.0-foss-2018a vital-it/7 ( L ) FFTW/3.3.8-intel-2018b hwloc/1.11.8-GCCcore-6.4.0 netCDF/4.6.1-foss-2018b ( D ) VTune/2018_update3 FFTW/3.3.8-iomkl-2018b ( D ) hwloc/1.11.10-GCCcore-7.3.0 nettle/3.4-foss-2018a X11/20180131-GCCcore-6.4.0 foss/2016b hwloc/1.11.11-GCCcore-8.2.0 ( D ) NLopt/2.4.2-foss-2018a Where: L: Module is loaded D: Default Module Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\" .","title":"Pre-Installed Software"},{"location":"software/pre-installed-software.html#pre-installed-software","text":"","title":"Pre-Installed Software"},{"location":"software/pre-installed-software.html#description","text":"This page contains a list of pre-installed software that is available for all UBELIX users. If you want to install custom software yourself, take a look here . If you think that some missing software could be of general interest for the UBELIX community, you can ask us to install the software system wide. Since maintaining software is a lot of work, we will select carefully which software we will install globally.","title":"Description"},{"location":"software/pre-installed-software.html#environment-modules","text":"To make certain versions of a software available, the user must first \u201cload/add\u201d the corresponding modulefile. Environment modules allow to maintain different versions of the same software by altering the shell environment variables (e.g. $PATH) accordingly. Each modulefile contains all information needed to configure the shell for a specific software.","title":"Environment Modules"},{"location":"software/pre-installed-software.html#toolchains","text":"If you need to compile any application, we suggest to load a toolchain. There are various toolchains available, to list some: Toolchain packacges GCC GCC compiler gompi GCC, OpenMPI gompic GCC, OpenMPI, CUDA foss GCC, OpenMPI, OpenBLAS, FFTW, ScaLAPACK fosscuda GCC, OpenMPI, OpenBLAS, FFTW, ScaLAPACK, CUDA intel Intel compiler, (GCC required), MKL, Intel MPI iompi Intel compiler, OpenMPI iomkl Intel compiler, OpenMPI, MKL Note: this list is not meant to be complete but to give an idea of high level toolchains. you can list all available toolchains using module avail and see the containing packages using, e.g. module show gompic","title":"Toolchains"},{"location":"software/pre-installed-software.html#list-all-available-modulefiles","text":"$ module avail --------------------------------------------------------------------------------------- /software.el7/modulefiles/all ---------------------------------------------------------------------------------------- Advisor/2018_update3 foss/2017a iccifort/2017.4.196-GCC-6.4.0-2.28 numactl/2.0.11-GCC-5.4.0-2.26 Autotools/20150215-GCC-5.4.0-2.26 foss/2017b iccifort/2018.1.163-GCC-6.4.0-2.28 numactl/2.0.11-GCC-6.3.0-2.27 Autotools/20150215-GCCcore-6.3.0 foss/2018a iccifort/2018.3.222-GCC-7.3.0-2.30 ( D ) numactl/2.0.11-GCCcore-6.4.0 Autotools/20170619-GCCcore-6.4.0 foss/2018b ICU/61.1-GCCcore-6.4.0 numactl/2.0.11-GCCcore-7.3.0 Autotools/20180311-GCCcore-7.3.0 foss/2019a ( D ) IDL/8.6 numactl/2.0.12-GCCcore-8.2.0 ( D ) Autotools/20180311-GCCcore-8.2.0 ( D ) fosscuda/2019a iimpi/2017b OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1 Boost/1.66.0-foss-2018a Gaussian/g09.D01 iimpi/2018a OpenBLAS/0.2.19-GCC-6.3.0-2.27-LAPACK-3.7.0 CMake/3.9.1-GCCcore-6.4.0 Gaussian/g16.A03 ( D ) iimpi/2018b ( D ) OpenBLAS/0.2.20-GCC-6.4.0-2.28 CMake/3.9.5-GCCcore-6.4.0 GCC/5.4.0-2.26 imkl/2017.3.196-iimpi-2017b OpenBLAS/0.3.1-GCC-7.3.0-2.30 CMake/3.10.2-GCCcore-6.4.0 GCC/6.3.0-2.27 imkl/2017.3.196-iompi-2017b OpenBLAS/0.3.5-GCC-8.2.0-2.31.1 ( D ) CMake/3.11.1-GCCcore-6.4.0 GCC/6.4.0-2.28 imkl/2018.1.163-iimpi-2018a OpenMPI/1.10.3-GCC-5.4.0-2.26 CMake/3.11.4-GCCcore-7.3.0 ( D ) GCC/7.3.0-2.30 imkl/2018.1.163-iompi-2018a OpenMPI/2.0.2-GCC-6.3.0-2.27 CUDA/8.0.61 GCC/8.2.0-2.31.1 ( D ) imkl/2018.3.222-iimpi-2018b OpenMPI/2.1.1-GCC-6.4.0-2.28 CUDA/9.0.176 GCCcore/5.4.0 imkl/2018.3.222-iompi-2018b ( D ) OpenMPI/2.1.1-iccifort-2017.4.196-GCC-6.4.0-2.28 CUDA/9.1.85 GCCcore/6.3.0 impi/2017.3.196-iccifort-2017.4.196-GCC-6.4.0-2.28 OpenMPI/2.1.2-GCC-6.4.0-2.28 CUDA/9.2.88 GCCcore/6.4.0 impi/2018.1.163-iccifort-2018.1.163-GCC-6.4.0-2.28 OpenMPI/2.1.2-iccifort-2018.1.163-GCC-6.4.0-2.28 CUDA/10.1.105-GCC-8.2.0-2.31.1 ( D ) GCCcore/7.3.0 impi/2018.3.222-iccifort-2018.3.222-GCC-7.3.0-2.30 ( D ) OpenMPI/3.1.1-GCC-7.3.0-2.30 cuDNN/6.0-CUDA-8.0.61 GCCcore/8.2.0 ( D ) Inspector/2018_update3 OpenMPI/3.1.1-iccifort-2018.3.222-GCC-7.3.0-2.30 cuDNN/7.0.5-CUDA-9.0.176 gcccuda/2019a intel/2017b OpenMPI/3.1.3-GCC-8.2.0-2.31.1 cuDNN/7.0.5-CUDA-9.1.85 GEOS/3.6.2-foss-2018a-Python-3.6.4 intel/2018a OpenMPI/3.1.3-gcccuda-2019a ( D ) cuDNN/7.1.4-CUDA-9.2.88 git-lfs/2.4.2 intel/2018b ( D ) Perl/5.26.1-GCCcore-6.4.0 cuDNN/7.6.0.64-gcccuda-2019a ( D ) GMP/6.1.2-GCCcore-6.4.0 iomkl/2017b PGI/17.10-GCC-6.4.0-2.28 cURL/7.58.0-GCCcore-6.4.0 gompi/2016b iomkl/2018a PGI/18.4-GCC-6.4.0-2.28 cURL/7.60.0-GCCcore-7.3.0 ( D ) gompi/2017a iomkl/2018b ( D ) PGI/19.4-GCC-8.2.0-2.31.1 ( D ) Doxygen/1.8.13-GCCcore-6.4.0 gompi/2017b iompi/2017b Python/2.7.14-foss-2018a Doxygen/1.8.14-GCCcore-7.3.0 ( D ) gompi/2018a iompi/2018a Python/2.7.14-GCCcore-6.4.0-bare EasyBuild/3.6.1 gompi/2018b iompi/2018b ( D ) Python/3.6.4-foss-2018a ( D ) EasyBuild/3.6.2 gompi/2019a ( D ) itac/2018.3.022 R/3.4.4-foss-2018a-X11-20180131 EasyBuild/3.7.1 gompic/2019a Java/1.7.0_60 ScaLAPACK/2.0.2-gompi-2016b-OpenBLAS-0.2.18-LAPACK-3.6.1 EasyBuild/3.9.1 ( D ) GSL/2.4-GCCcore-6.4.0 Java/1.7.0_80 ScaLAPACK/2.0.2-gompi-2017a-OpenBLAS-0.2.19-LAPACK-3.7.0 FFTW/3.3.4-gompi-2016b HDF5/1.10.1-foss-2018a Java/1.8.0_121 ScaLAPACK/2.0.2-gompi-2017b-OpenBLAS-0.2.20 FFTW/3.3.6-gompi-2017a HDF5/1.10.2-foss-2018b ( D ) Java/1.8.0_152 ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20 FFTW/3.3.6-gompi-2017b help2man/1.47.4-GCCcore-6.3.0 Java/1.8.0_162 ( D ) ScaLAPACK/2.0.2-gompi-2018b-OpenBLAS-0.3.1 FFTW/3.3.7-gompi-2018a help2man/1.47.4-GCCcore-6.4.0 libsndfile/1.0.28-GCCcore-6.4.0 ScaLAPACK/2.0.2-gompi-2019a-OpenBLAS-0.3.5 FFTW/3.3.7-intel-2017b help2man/1.47.4-GCCcore-7.3.0 LLVM/5.0.1-GCCcore-6.4.0 ScaLAPACK/2.0.2-gompic-2019a-OpenBLAS-0.3.5 ( D ) FFTW/3.3.7-intel-2018a help2man/1.47.4 Mako/1.0.7-foss-2018a-Python-2.7.14 SQLite/3.20.1-GCCcore-6.4.0 FFTW/3.3.7-iomkl-2018a help2man/1.47.7-GCCcore-8.2.0 ( D ) MATLAB/2016b SQLite/3.21.0-GCCcore-6.4.0 ( D ) FFTW/3.3.8-gompi-2018b hwloc/1.11.3-GCC-5.4.0-2.26 MATLAB/2017b tmux/2.7 FFTW/3.3.8-gompi-2019a hwloc/1.11.5-GCC-6.3.0-2.27 MATLAB/2018b ( D ) UDUNITS/2.2.26-foss-2018a FFTW/3.3.8-gompic-2019a hwloc/1.11.7-GCCcore-6.4.0 netCDF/4.6.0-foss-2018a vital-it/7 FFTW/3.3.8-intel-2018b hwloc/1.11.8-GCCcore-6.4.0 netCDF/4.6.1-foss-2018b ( D ) VTune/2018_update3 FFTW/3.3.8-iomkl-2018b ( D ) hwloc/1.11.10-GCCcore-7.3.0 nettle/3.4-foss-2018a X11/20180131-GCCcore-6.4.0 foss/2016b hwloc/1.11.11-GCCcore-8.2.0 ( D ) NLopt/2.4.2-foss-2018a Where: D: Default Module Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\" .","title":"List all Available Modulefiles"},{"location":"software/pre-installed-software.html#bioinformatics-software","text":"In co-operation with the Vital-IT Group of the SIB Swiss Institute of Bioinformatics , a large set of bioinformatics software tools and databases is available to the life science community. To also list all modulefiles provided by Vital-IT, you have to first load the vital-it modulefile: Loading the vital-it modulefile automatically configures the environment to use specific versions of selected software, e.g. python v2.7.5, and gcc v4.9.1 $ module load vital-it && module avail ---------------------------------------------------------------------------------------------- /software/module ---------------------------------------------------------------------------------------------- Blast/blast/latest SequenceAnalysis/primer3/2.3.7 UHTS/Analysis/mummer/4.0.0beta1 ( D ) Blast/blast/2.2.26 ( D ) SequenceAnalysis/PrimerDesign/iPCR/1.0 UHTS/Analysis/NanoOK/1.2.6 Blast/ncbi-blast/latest SequenceAnalysis/ProtoGene/4.2.2 UHTS/Analysis/nanoraw/0.5 Blast/ncbi-blast/2.2.31+ SequenceAnalysis/readseq/2.1.30 UHTS/Analysis/oncodrivefm/1.0.3 ( ... ) SequenceAnalysis/OrthologyAnalysis/OMA/2.1.1 UHTS/Analysis/msprime/0.7.0 Utility/rpy2/2.9.1 ( D ) SequenceAnalysis/orthomclSoftware/2.0.9 UHTS/Analysis/MultiQC/1.3 Utility/Solver/SoPlex/4.0.0 SequenceAnalysis/patsearch/1 UHTS/Analysis/MultiQC/1.7 ( D ) Utility/Tarql/1.1 SequenceAnalysis/pftools/2.3.4 UHTS/Analysis/mummer/3.9.4alpha Utility/UCSC-utils/359 SequenceAnalysis/pftools/2.3.5.d ( D ) UHTS/Analysis/mummer/4.0.0beta --------------------------------------------------------------------------------------- /software.el7/modulefiles/all ---------------------------------------------------------------------------------------- Advisor/2018_update3 foss/2017a iccifort/2017.4.196-GCC-6.4.0-2.28 numactl/2.0.11-GCC-5.4.0-2.26 Autotools/20150215-GCC-5.4.0-2.26 foss/2017b iccifort/2018.1.163-GCC-6.4.0-2.28 numactl/2.0.11-GCC-6.3.0-2.27 Autotools/20150215-GCCcore-6.3.0 foss/2018a iccifort/2018.3.222-GCC-7.3.0-2.30 ( D ) numactl/2.0.11-GCCcore-6.4.0 Autotools/20170619-GCCcore-6.4.0 foss/2018b ICU/61.1-GCCcore-6.4.0 numactl/2.0.11-GCCcore-7.3.0 Autotools/20180311-GCCcore-7.3.0 foss/2019a ( D ) IDL/8.6 numactl/2.0.12-GCCcore-8.2.0 ( D ) Autotools/20180311-GCCcore-8.2.0 ( D ) fosscuda/2019a iimpi/2017b OpenBLAS/0.2.18-GCC-5.4.0-2.26-LAPACK-3.6.1 Boost/1.66.0-foss-2018a Gaussian/g09.D01 iimpi/2018a OpenBLAS/0.2.19-GCC-6.3.0-2.27-LAPACK-3.7.0 CMake/3.9.1-GCCcore-6.4.0 Gaussian/g16.A03 ( D ) iimpi/2018b ( D ) OpenBLAS/0.2.20-GCC-6.4.0-2.28 CMake/3.9.5-GCCcore-6.4.0 GCC/5.4.0-2.26 imkl/2017.3.196-iimpi-2017b OpenBLAS/0.3.1-GCC-7.3.0-2.30 CMake/3.10.2-GCCcore-6.4.0 GCC/6.3.0-2.27 imkl/2017.3.196-iompi-2017b OpenBLAS/0.3.5-GCC-8.2.0-2.31.1 ( D ) CMake/3.11.1-GCCcore-6.4.0 GCC/6.4.0-2.28 imkl/2018.1.163-iimpi-2018a OpenMPI/1.10.3-GCC-5.4.0-2.26 CMake/3.11.4-GCCcore-7.3.0 ( D ) GCC/7.3.0-2.30 imkl/2018.1.163-iompi-2018a OpenMPI/2.0.2-GCC-6.3.0-2.27 CUDA/8.0.61 GCC/8.2.0-2.31.1 ( D ) imkl/2018.3.222-iimpi-2018b OpenMPI/2.1.1-GCC-6.4.0-2.28 CUDA/9.0.176 GCCcore/5.4.0 imkl/2018.3.222-iompi-2018b ( D ) OpenMPI/2.1.1-iccifort-2017.4.196-GCC-6.4.0-2.28 CUDA/9.1.85 GCCcore/6.3.0 impi/2017.3.196-iccifort-2017.4.196-GCC-6.4.0-2.28 OpenMPI/2.1.2-GCC-6.4.0-2.28 CUDA/9.2.88 GCCcore/6.4.0 impi/2018.1.163-iccifort-2018.1.163-GCC-6.4.0-2.28 OpenMPI/2.1.2-iccifort-2018.1.163-GCC-6.4.0-2.28 CUDA/10.1.105-GCC-8.2.0-2.31.1 ( D ) GCCcore/7.3.0 impi/2018.3.222-iccifort-2018.3.222-GCC-7.3.0-2.30 ( D ) OpenMPI/3.1.1-GCC-7.3.0-2.30 cuDNN/6.0-CUDA-8.0.61 GCCcore/8.2.0 ( D ) Inspector/2018_update3 OpenMPI/3.1.1-iccifort-2018.3.222-GCC-7.3.0-2.30 cuDNN/7.0.5-CUDA-9.0.176 gcccuda/2019a intel/2017b OpenMPI/3.1.3-GCC-8.2.0-2.31.1 cuDNN/7.0.5-CUDA-9.1.85 GEOS/3.6.2-foss-2018a-Python-3.6.4 intel/2018a OpenMPI/3.1.3-gcccuda-2019a ( D ) cuDNN/7.1.4-CUDA-9.2.88 git-lfs/2.4.2 intel/2018b ( D ) Perl/5.26.1-GCCcore-6.4.0 cuDNN/7.6.0.64-gcccuda-2019a ( D ) GMP/6.1.2-GCCcore-6.4.0 iomkl/2017b PGI/17.10-GCC-6.4.0-2.28 cURL/7.58.0-GCCcore-6.4.0 gompi/2016b iomkl/2018a PGI/18.4-GCC-6.4.0-2.28 cURL/7.60.0-GCCcore-7.3.0 ( D ) gompi/2017a iomkl/2018b ( D ) PGI/19.4-GCC-8.2.0-2.31.1 ( D ) Doxygen/1.8.13-GCCcore-6.4.0 gompi/2017b iompi/2017b Python/2.7.14-foss-2018a Doxygen/1.8.14-GCCcore-7.3.0 ( D ) gompi/2018a iompi/2018a Python/2.7.14-GCCcore-6.4.0-bare EasyBuild/3.6.1 gompi/2018b iompi/2018b ( D ) Python/3.6.4-foss-2018a ( D ) EasyBuild/3.6.2 gompi/2019a ( D ) itac/2018.3.022 R/3.4.4-foss-2018a-X11-20180131 EasyBuild/3.7.1 gompic/2019a Java/1.7.0_60 ScaLAPACK/2.0.2-gompi-2016b-OpenBLAS-0.2.18-LAPACK-3.6.1 EasyBuild/3.9.1 ( D ) GSL/2.4-GCCcore-6.4.0 Java/1.7.0_80 ScaLAPACK/2.0.2-gompi-2017a-OpenBLAS-0.2.19-LAPACK-3.7.0 FFTW/3.3.4-gompi-2016b HDF5/1.10.1-foss-2018a Java/1.8.0_121 ScaLAPACK/2.0.2-gompi-2017b-OpenBLAS-0.2.20 FFTW/3.3.6-gompi-2017a HDF5/1.10.2-foss-2018b ( D ) Java/1.8.0_152 ScaLAPACK/2.0.2-gompi-2018a-OpenBLAS-0.2.20 FFTW/3.3.6-gompi-2017b help2man/1.47.4-GCCcore-6.3.0 Java/1.8.0_162 ( D ) ScaLAPACK/2.0.2-gompi-2018b-OpenBLAS-0.3.1 FFTW/3.3.7-gompi-2018a help2man/1.47.4-GCCcore-6.4.0 libsndfile/1.0.28-GCCcore-6.4.0 ScaLAPACK/2.0.2-gompi-2019a-OpenBLAS-0.3.5 FFTW/3.3.7-intel-2017b help2man/1.47.4-GCCcore-7.3.0 LLVM/5.0.1-GCCcore-6.4.0 ScaLAPACK/2.0.2-gompic-2019a-OpenBLAS-0.3.5 ( D ) FFTW/3.3.7-intel-2018a help2man/1.47.4 Mako/1.0.7-foss-2018a-Python-2.7.14 SQLite/3.20.1-GCCcore-6.4.0 FFTW/3.3.7-iomkl-2018a help2man/1.47.7-GCCcore-8.2.0 ( D ) MATLAB/2016b SQLite/3.21.0-GCCcore-6.4.0 ( D ) FFTW/3.3.8-gompi-2018b hwloc/1.11.3-GCC-5.4.0-2.26 MATLAB/2017b tmux/2.7 FFTW/3.3.8-gompi-2019a hwloc/1.11.5-GCC-6.3.0-2.27 MATLAB/2018b ( D ) UDUNITS/2.2.26-foss-2018a FFTW/3.3.8-gompic-2019a hwloc/1.11.7-GCCcore-6.4.0 netCDF/4.6.0-foss-2018a vital-it/7 ( L ) FFTW/3.3.8-intel-2018b hwloc/1.11.8-GCCcore-6.4.0 netCDF/4.6.1-foss-2018b ( D ) VTune/2018_update3 FFTW/3.3.8-iomkl-2018b ( D ) hwloc/1.11.10-GCCcore-7.3.0 nettle/3.4-foss-2018a X11/20180131-GCCcore-6.4.0 foss/2016b hwloc/1.11.11-GCCcore-8.2.0 ( D ) NLopt/2.4.2-foss-2018a Where: L: Module is loaded D: Default Module Use \"module spider\" to find all possible modules. Use \"module keyword key1 key2 ...\" to search for all possible modules matching any of the \"keys\" .","title":"Bioinformatics Software"},{"location":"software/python.html","text":"Python Description there are multiple versions of Pyhton available on our HPCs. On the one hand there are Python modules for Python 2 and 3. Which already has a longer list of additional packages installed, including pip . On the other hand there is Anaconda installed, which brings an even longer list of packages with it. For Anaconda see Anaconda page Additional Packages If you need additional packages we suggest to install them using pip . You can install into your private HOME using the pip install --user option or share the packages with your colleges using the Workspace module, e.g. for installing a package matplotlib : ## load Python first, this can also be `module load Anaconda3` module load Python ## maybe you need to specify the Workspace name first OR use `Workspace/home` module load Workspace ## pip install in the Workspace or HOME location ## the variable PYTHONPACKAGEPATH is set in the Workspace module pip install --prefix $PYTHONPACKAGEPATH matplotlib Therewith the Python packages are automatically available when module load Python ## OR module load Anaconda3 module load Workspace ## maybe you need to specify the Workspace name first python -c \"import matplotlib\" since $PYTHONPATH and $PATH are set to the above specified location. Type if you get the error: ERROR: You must give at least one requirement to install (see \"pip help install\") you need to reload the Workspace module to properly set the variables. The Workspace module need to have Anaconda3/Python loaded first to read the proper Python Version. module load Python ## OR module load Anaconda3","title":"Python"},{"location":"software/python.html#python","text":"","title":"Python"},{"location":"software/python.html#description","text":"there are multiple versions of Pyhton available on our HPCs. On the one hand there are Python modules for Python 2 and 3. Which already has a longer list of additional packages installed, including pip . On the other hand there is Anaconda installed, which brings an even longer list of packages with it. For Anaconda see Anaconda page","title":"Description"},{"location":"software/python.html#additional-packages","text":"If you need additional packages we suggest to install them using pip . You can install into your private HOME using the pip install --user option or share the packages with your colleges using the Workspace module, e.g. for installing a package matplotlib : ## load Python first, this can also be `module load Anaconda3` module load Python ## maybe you need to specify the Workspace name first OR use `Workspace/home` module load Workspace ## pip install in the Workspace or HOME location ## the variable PYTHONPACKAGEPATH is set in the Workspace module pip install --prefix $PYTHONPACKAGEPATH matplotlib Therewith the Python packages are automatically available when module load Python ## OR module load Anaconda3 module load Workspace ## maybe you need to specify the Workspace name first python -c \"import matplotlib\" since $PYTHONPATH and $PATH are set to the above specified location. Type if you get the error: ERROR: You must give at least one requirement to install (see \"pip help install\") you need to reload the Workspace module to properly set the variables. The Workspace module need to have Anaconda3/Python loaded first to read the proper Python Version. module load Python ## OR module load Anaconda3","title":"Additional Packages"},{"location":"software/r.html","text":"R Description R is provided by an environment module and must be loaded explicitly: module load R ##OR## module load R/3.4.4-foss-2018a-X11-20180131 R --version R version 4 .0.0 ( 2020 -04-24 ) -- \"Arbor Day\" ... The Vital-IT project is also providing some versions. The following commands will list the available versions: module load vital-it module avail 2 > & 1 | grep \" R\\/\" R/3.4.2 R/latest To use one of these version, you have to load the respective module, which then masks the system\u2019s version, i.e. module load vital-it module load R/3.4.2 Do not forget to put those two lines into your job script as well in order to use the same version from within the job later on a compute node! Basic Topics Customizing the R Workspace At startup, unless \u2013no-init-file, or \u2013vanilla was given, R searches for a user profile in the current directory (from where R was started), or in the user\u2019s home directory (in that order). A different path of the user profile file can be specified by the R_PROFILE_USER environment variable. The found user profile is then sourced into the workspace. You can use this file to customize your workspace, i.e., to set specific options, define functions, load libraries, and so on. Consider the following example: .Rprofile # Set some options options ( stringsAsFactors = FALSE ) options ( max.print = 100 ) options ( scipen = 10 ) # Load class library library ( class ) # Don't save workspace by default q <- function ( save = \"no\" , ... ) { quit ( save = save, ... ) } # User-defined function for setting standard seed mySeed <- function () set.seed ( 5450 ) # User-defined function for calculating L1/L2-norm, returns euclidean distance (L2-norm) by default myDistance <- function ( x, y, type = c ( \"Euclidean\" , \"L2\" , \"Manhattan\" , \"L1\" )) { type <- match.arg ( type ) if (( type == \"Manhattan\" ) | ( type == \"L1\" )) { d <- sum ( abs ( x - y ) ) } else { d <- sqrt ( sum ( ( x - y ) ^ 2 ) ) } return ( d ) } Installing Packages R is installed as global Module in various versions. There are already a longer list of pre-installed packages available. If you need additional packages you can install them by yourself. The default location would be the R installation directory, which is not writeable for users. Nevertheless, in the following is shown how to install into a shared HPC Workspace or into your private HOME. A) Into a shared Workspace With the Workspace tools we provide short-cuts to install R packages in the shared Workspace location. Therefore, the environment variable $R_LIBS is set to $WORKSPACE/RPackages . Initially this directory need to be created, using: module load Workspace mkdir $R_LIBS If you want to install into your $HOME Then R packages can be installed using the install.packages() routine in an interactive R shell, e.g. for doParallel : module load R R ... > install.packages(\"doParallel\") Then please follow the procedure as shown below . Then the installed packaged will be available to you and all other Workspace members by simply loading the Workspace module. Please remember to add the Workspace and the R module to your job scripts: module load Workspace module load R B) Into your HOME Note you can also use procedure A) and load Workspace/home to install into your HOME directory. If you are not using a Workspace module and try to install a package, at the first time R tries to install the package into a global/generic location, which is not writeable by users. You can then select to install in a \u201cpersonal library\u201d into your HOME: module load R R > install.packages ( \"doParallel\" ) Installing package into \u2018/usr/lib64/R/library\u2019 ( as \u2018lib\u2019 is unspecified ) Warnung in install.packages ( \"doParallel\" ) ' lib = \"/usr/lib64/R/library\" ist nicht schreibbar Would you like to use a personal library instead? ( y/n ) Next, type \u201cy\u201d to create your personal library at the default location within your HOME directory: Would you like to create a personal library ~/R/x86_64-redhat-linux-gnu-library/4.0 Installation Routine Next, select a CRAN mirror to download from. The mirror list will be not the same as below. The mirror list is constantly changing, but will look like it. Pick any country nearby, i.e. Switzerland. If https makes problems, pick \u201c(HTTP mirrors)\u201d and then select something nearby as shown below --- Bitte einen CRAN Spiegel f\u00fcr diese Sitzung ausw\u00e4hlen --- Error in download.file ( url, destfile = f, quiet = TRUE ) : nicht unterst\u00fctztes URL Schema HTTPS CRAN mirror 1 : 0 -Cloud [ https ] 2 : Austria [ https ] 3 : Chile [ https ] 4 : China ( Beijing 4 ) [ https ] 5 : Colombia ( Cali ) [ https ] 6 : France ( Lyon 2 ) [ https ] 7 : France ( Paris 2 ) [ https ] 8 : Germany ( M\u00fcnster ) [ https ] 9 : Iceland [ https ] 10 : Mexico ( Mexico City ) [ https ] 11 : Russia ( Moscow ) [ https ] 12 : Spain ( A Coru\u00f1a ) [ https ] 13 : Switzerland [ https ] 14 : UK ( Bristol ) [ https ] 15 : UK ( Cambridge ) [ https ] 16 : USA ( CA 1 ) [ https ] 17 : USA ( KS ) [ https ] 18 : USA ( MI 1 ) [ https ] 19 : USA ( TN ) [ https ] 20 : USA ( TX ) [ https ] 21 : USA ( WA ) [ https ] 22 : ( HTTP mirrors ) Selection: 22 HTTP CRAN mirror 1 : 0 -Cloud 2 : Algeria 3 : Argentina ( La Plata ) 4 : Australia ( Canberra ) 5 : Australia ( Melbourne ) 6 : Austria 7 : Belgium ( Antwerp ) 8 : Belgium ( Ghent ) ( ... ) 65 : Slovakia 66 : South Africa ( Cape Town ) 67 : South Africa ( Johannesburg ) 68 : Spain ( A Coru\u00f1a ) 69 : Spain ( Madrid ) 70 : Sweden 71 : Switzerland 72 : Taiwan ( Chungli ) 73 : Taiwan ( Taipei ) 74 : Thailand 75 : Turkey ( Denizli ) 76 : Turkey ( Mersin ) ( ... ) 93 : USA ( OH 2 ) 94 : USA ( OR ) 95 : USA ( PA 2 ) 96 : USA ( TN ) 97 : USA ( TX ) 98 : USA ( WA ) 99 : Venezuela Selection: 71 Finally, the package gets installed. After installing the package you can close the interactive session by typing q(). Do not forget to load the corresponding library (for each R session) before using functions provided by the package: > library ( doParallel ) Batch Execution of R The syntax for running R non-interactively with input read from infile and output send to outfile is: R CMD BATCH [ options ] infile [ outfile ] Suppose you placed your R code in a file called foo.R: foo.R set.seed ( 3000 ) valx<-seq ( -2,2,0.01 ) valy<-2*valx+rnorm ( length ( valx ) ,0,4 ) # Save plot to pdf pdf ( 'histplot.pdf' ) hist ( valy,prob = TRUE,breaks = 20 , main = \"Histogram and PDF\" ,xlab = \"y\" , ylim = c ( 0 ,0.15 )) curve ( dnorm ( x,mean ( valy ) ,sd ( valy )) ,add = T,col = \"red\" ) dev.off () To execute foo.R on the cluster, add the R call to your job script\u2026 Rbatch.sh #! /bin/bash #SBATCH --mail-user=<put your valid email address here!> #SBATCH --mail-type=end,fail #SBATCH --time=01:00:00 #SBATCH --mem-per-cpu=2G # Put your code below this line module load vital-it module load R/3.4.2 R CMD BATCH --no-save --no-restore foo.R \u2026and submit your job script to the cluster: sbatch Rbatch.sh Advanced Topics Parallel R By default, R will not make use of multiple cores available on compute nodes to parallelize computations. Parallel processing functionality is provided by add-on packages. Consider the following contrived example to get you started. To follow the example, you need the following packages installed, and the corresponding libraries loaded: > library ( doParallel ) > library ( foreach ) The foreach package provides a looping construct for executing R statements repeatedly, either sequentially (similar to a for loop) or in parallel. While the binary operator %do% is used for executing the statements sequentially, the %dopar% operator is used to execute code in parallel using the currently registered backend. The getDoParWorkers() function returns the number of execution workers (cores) available in the currently registered doPar backend, by default this corresponds to one worker: > getDoParWorkers () [ 1 ] 1 Hence, the following R code will execute on a single core (even with the %dopar% operator): > start.time <- Sys.time () > foreach ( i = 4 :1, .combine = 'c' , .inorder = FALSE ) %dopar% { + Sys.sleep ( 3 *i ) + i + } end.time <- Sys.time () exec.time <- end.time - start.time [ 1 ] 4 3 2 1 Let\u2019s measure the runtime of the sequentiall execution: > start.time <- Sys.time () ; foreach ( i = 4 :1, .combine = 'c' , .inorder = TRUE ) %dopar% { Sys.sleep ( 3 *i ) ; i } ; end.time <- Sys.time () ; exec.time <- end.time - start.time ; exec.time [ 1 ] 4 3 2 1 Time difference of 30 .04088 secs Now, we will register a parallel backend to allow the %dopar% operator to execute in parallel. The doParallel package provides a parallel backend for the %dopar% operator. Let\u2019s find out the number of cores available on the current node > detectCores () [ 1 ] 24 To register the doPar backend call the function registerDoParallel(). With no arguments provided, the number of cores assigned to the backend matches the value of options(\u201ccores\u201d) , or if not set, to half of the cores detected by the parallel package. registerDoParallel () > getDoParWorkers () [ 1 ] 12 To assign 4 cores to the parallel backend: > registerDoParallel ( cores = 4 ) > getDoParWorkers () [ 1 ] 4 Request the correct number of slots Because it is crucial to request the correct number of slots for a parallel job, we propose to set the number of cores for the doPar backend to the number of slots allocated to your job: registerDoParallel(cores=Sys.getenv(\"SLURM_CPUS_PER_TASK\")) Now, run the example again: > foreach ( i = 4 :1, .combine = 'c' , .inorder = FALSE ) %dopar% { + Sys.sleep ( 3 *i ) + i + } [ 1 ] 4 3 2 1 Well, the output is basically the same (the results are combined in the same order!). Let\u2019s again measure the runtime of the parallel execution on 4 cores: The binary operator %do% will always execute a foreach-loop sequentially even if registerDoParallel was called before! To correctly run a foreach in parallel, two conditions must be met: registerDoParallel() must be called with a certain number of cores The %dopar% operator must be used in the foreach-loop to have it run in parallel! Installing DESeq2 from Bioconductor packages DESeq2 1 installed from Bioconductor 2 has many dependencies. Two odd facts are hindering a succesful build of DESeq2 in first place: data.table is needed by Hmisc, which in turn is needed by DESeq2. While Hmisc is automatically installed prior to DESeq2, data.table is not and has to be installed manually first. https://bioconductor.org/packages/release/bioc/html/DESeq2.html \u21a9 https://bioconductor.org/ \u21a9","title":"R"},{"location":"software/r.html#r","text":"","title":"R"},{"location":"software/r.html#description","text":"R is provided by an environment module and must be loaded explicitly: module load R ##OR## module load R/3.4.4-foss-2018a-X11-20180131 R --version R version 4 .0.0 ( 2020 -04-24 ) -- \"Arbor Day\" ... The Vital-IT project is also providing some versions. The following commands will list the available versions: module load vital-it module avail 2 > & 1 | grep \" R\\/\" R/3.4.2 R/latest To use one of these version, you have to load the respective module, which then masks the system\u2019s version, i.e. module load vital-it module load R/3.4.2 Do not forget to put those two lines into your job script as well in order to use the same version from within the job later on a compute node!","title":"Description"},{"location":"software/r.html#basic-topics","text":"","title":"Basic Topics"},{"location":"software/r.html#customizing-the-r-workspace","text":"At startup, unless \u2013no-init-file, or \u2013vanilla was given, R searches for a user profile in the current directory (from where R was started), or in the user\u2019s home directory (in that order). A different path of the user profile file can be specified by the R_PROFILE_USER environment variable. The found user profile is then sourced into the workspace. You can use this file to customize your workspace, i.e., to set specific options, define functions, load libraries, and so on. Consider the following example: .Rprofile # Set some options options ( stringsAsFactors = FALSE ) options ( max.print = 100 ) options ( scipen = 10 ) # Load class library library ( class ) # Don't save workspace by default q <- function ( save = \"no\" , ... ) { quit ( save = save, ... ) } # User-defined function for setting standard seed mySeed <- function () set.seed ( 5450 ) # User-defined function for calculating L1/L2-norm, returns euclidean distance (L2-norm) by default myDistance <- function ( x, y, type = c ( \"Euclidean\" , \"L2\" , \"Manhattan\" , \"L1\" )) { type <- match.arg ( type ) if (( type == \"Manhattan\" ) | ( type == \"L1\" )) { d <- sum ( abs ( x - y ) ) } else { d <- sqrt ( sum ( ( x - y ) ^ 2 ) ) } return ( d ) }","title":"Customizing the R Workspace"},{"location":"software/r.html#installing-packages","text":"R is installed as global Module in various versions. There are already a longer list of pre-installed packages available. If you need additional packages you can install them by yourself. The default location would be the R installation directory, which is not writeable for users. Nevertheless, in the following is shown how to install into a shared HPC Workspace or into your private HOME.","title":"Installing Packages"},{"location":"software/r.html#a-into-a-shared-workspace","text":"With the Workspace tools we provide short-cuts to install R packages in the shared Workspace location. Therefore, the environment variable $R_LIBS is set to $WORKSPACE/RPackages . Initially this directory need to be created, using: module load Workspace mkdir $R_LIBS If you want to install into your $HOME Then R packages can be installed using the install.packages() routine in an interactive R shell, e.g. for doParallel : module load R R ... > install.packages(\"doParallel\") Then please follow the procedure as shown below . Then the installed packaged will be available to you and all other Workspace members by simply loading the Workspace module. Please remember to add the Workspace and the R module to your job scripts: module load Workspace module load R","title":"A) Into a shared Workspace"},{"location":"software/r.html#b-into-your-home","text":"Note you can also use procedure A) and load Workspace/home to install into your HOME directory. If you are not using a Workspace module and try to install a package, at the first time R tries to install the package into a global/generic location, which is not writeable by users. You can then select to install in a \u201cpersonal library\u201d into your HOME: module load R R > install.packages ( \"doParallel\" ) Installing package into \u2018/usr/lib64/R/library\u2019 ( as \u2018lib\u2019 is unspecified ) Warnung in install.packages ( \"doParallel\" ) ' lib = \"/usr/lib64/R/library\" ist nicht schreibbar Would you like to use a personal library instead? ( y/n ) Next, type \u201cy\u201d to create your personal library at the default location within your HOME directory: Would you like to create a personal library ~/R/x86_64-redhat-linux-gnu-library/4.0","title":"B) Into your HOME"},{"location":"software/r.html#installation-routine","text":"Next, select a CRAN mirror to download from. The mirror list will be not the same as below. The mirror list is constantly changing, but will look like it. Pick any country nearby, i.e. Switzerland. If https makes problems, pick \u201c(HTTP mirrors)\u201d and then select something nearby as shown below --- Bitte einen CRAN Spiegel f\u00fcr diese Sitzung ausw\u00e4hlen --- Error in download.file ( url, destfile = f, quiet = TRUE ) : nicht unterst\u00fctztes URL Schema HTTPS CRAN mirror 1 : 0 -Cloud [ https ] 2 : Austria [ https ] 3 : Chile [ https ] 4 : China ( Beijing 4 ) [ https ] 5 : Colombia ( Cali ) [ https ] 6 : France ( Lyon 2 ) [ https ] 7 : France ( Paris 2 ) [ https ] 8 : Germany ( M\u00fcnster ) [ https ] 9 : Iceland [ https ] 10 : Mexico ( Mexico City ) [ https ] 11 : Russia ( Moscow ) [ https ] 12 : Spain ( A Coru\u00f1a ) [ https ] 13 : Switzerland [ https ] 14 : UK ( Bristol ) [ https ] 15 : UK ( Cambridge ) [ https ] 16 : USA ( CA 1 ) [ https ] 17 : USA ( KS ) [ https ] 18 : USA ( MI 1 ) [ https ] 19 : USA ( TN ) [ https ] 20 : USA ( TX ) [ https ] 21 : USA ( WA ) [ https ] 22 : ( HTTP mirrors ) Selection: 22 HTTP CRAN mirror 1 : 0 -Cloud 2 : Algeria 3 : Argentina ( La Plata ) 4 : Australia ( Canberra ) 5 : Australia ( Melbourne ) 6 : Austria 7 : Belgium ( Antwerp ) 8 : Belgium ( Ghent ) ( ... ) 65 : Slovakia 66 : South Africa ( Cape Town ) 67 : South Africa ( Johannesburg ) 68 : Spain ( A Coru\u00f1a ) 69 : Spain ( Madrid ) 70 : Sweden 71 : Switzerland 72 : Taiwan ( Chungli ) 73 : Taiwan ( Taipei ) 74 : Thailand 75 : Turkey ( Denizli ) 76 : Turkey ( Mersin ) ( ... ) 93 : USA ( OH 2 ) 94 : USA ( OR ) 95 : USA ( PA 2 ) 96 : USA ( TN ) 97 : USA ( TX ) 98 : USA ( WA ) 99 : Venezuela Selection: 71 Finally, the package gets installed. After installing the package you can close the interactive session by typing q(). Do not forget to load the corresponding library (for each R session) before using functions provided by the package: > library ( doParallel )","title":"Installation Routine"},{"location":"software/r.html#batch-execution-of-r","text":"The syntax for running R non-interactively with input read from infile and output send to outfile is: R CMD BATCH [ options ] infile [ outfile ] Suppose you placed your R code in a file called foo.R: foo.R set.seed ( 3000 ) valx<-seq ( -2,2,0.01 ) valy<-2*valx+rnorm ( length ( valx ) ,0,4 ) # Save plot to pdf pdf ( 'histplot.pdf' ) hist ( valy,prob = TRUE,breaks = 20 , main = \"Histogram and PDF\" ,xlab = \"y\" , ylim = c ( 0 ,0.15 )) curve ( dnorm ( x,mean ( valy ) ,sd ( valy )) ,add = T,col = \"red\" ) dev.off () To execute foo.R on the cluster, add the R call to your job script\u2026 Rbatch.sh #! /bin/bash #SBATCH --mail-user=<put your valid email address here!> #SBATCH --mail-type=end,fail #SBATCH --time=01:00:00 #SBATCH --mem-per-cpu=2G # Put your code below this line module load vital-it module load R/3.4.2 R CMD BATCH --no-save --no-restore foo.R \u2026and submit your job script to the cluster: sbatch Rbatch.sh","title":"Batch Execution of R"},{"location":"software/r.html#advanced-topics","text":"","title":"Advanced Topics"},{"location":"software/r.html#parallel-r","text":"By default, R will not make use of multiple cores available on compute nodes to parallelize computations. Parallel processing functionality is provided by add-on packages. Consider the following contrived example to get you started. To follow the example, you need the following packages installed, and the corresponding libraries loaded: > library ( doParallel ) > library ( foreach ) The foreach package provides a looping construct for executing R statements repeatedly, either sequentially (similar to a for loop) or in parallel. While the binary operator %do% is used for executing the statements sequentially, the %dopar% operator is used to execute code in parallel using the currently registered backend. The getDoParWorkers() function returns the number of execution workers (cores) available in the currently registered doPar backend, by default this corresponds to one worker: > getDoParWorkers () [ 1 ] 1 Hence, the following R code will execute on a single core (even with the %dopar% operator): > start.time <- Sys.time () > foreach ( i = 4 :1, .combine = 'c' , .inorder = FALSE ) %dopar% { + Sys.sleep ( 3 *i ) + i + } end.time <- Sys.time () exec.time <- end.time - start.time [ 1 ] 4 3 2 1 Let\u2019s measure the runtime of the sequentiall execution: > start.time <- Sys.time () ; foreach ( i = 4 :1, .combine = 'c' , .inorder = TRUE ) %dopar% { Sys.sleep ( 3 *i ) ; i } ; end.time <- Sys.time () ; exec.time <- end.time - start.time ; exec.time [ 1 ] 4 3 2 1 Time difference of 30 .04088 secs Now, we will register a parallel backend to allow the %dopar% operator to execute in parallel. The doParallel package provides a parallel backend for the %dopar% operator. Let\u2019s find out the number of cores available on the current node > detectCores () [ 1 ] 24 To register the doPar backend call the function registerDoParallel(). With no arguments provided, the number of cores assigned to the backend matches the value of options(\u201ccores\u201d) , or if not set, to half of the cores detected by the parallel package. registerDoParallel () > getDoParWorkers () [ 1 ] 12 To assign 4 cores to the parallel backend: > registerDoParallel ( cores = 4 ) > getDoParWorkers () [ 1 ] 4 Request the correct number of slots Because it is crucial to request the correct number of slots for a parallel job, we propose to set the number of cores for the doPar backend to the number of slots allocated to your job: registerDoParallel(cores=Sys.getenv(\"SLURM_CPUS_PER_TASK\")) Now, run the example again: > foreach ( i = 4 :1, .combine = 'c' , .inorder = FALSE ) %dopar% { + Sys.sleep ( 3 *i ) + i + } [ 1 ] 4 3 2 1 Well, the output is basically the same (the results are combined in the same order!). Let\u2019s again measure the runtime of the parallel execution on 4 cores: The binary operator %do% will always execute a foreach-loop sequentially even if registerDoParallel was called before! To correctly run a foreach in parallel, two conditions must be met: registerDoParallel() must be called with a certain number of cores The %dopar% operator must be used in the foreach-loop to have it run in parallel!","title":"Parallel R"},{"location":"software/r.html#installing-deseq2-from-bioconductor-packages","text":"DESeq2 1 installed from Bioconductor 2 has many dependencies. Two odd facts are hindering a succesful build of DESeq2 in first place: data.table is needed by Hmisc, which in turn is needed by DESeq2. While Hmisc is automatically installed prior to DESeq2, data.table is not and has to be installed manually first. https://bioconductor.org/packages/release/bioc/html/DESeq2.html \u21a9 https://bioconductor.org/ \u21a9","title":"Installing DESeq2 from Bioconductor packages"},{"location":"software/relion.html","text":"Relion Description Some useful information on using Relion. Running Relion A standard submission script serves as a template for your Relion jobs. Create a file with the following content within your home directory: qsub.sh #!/bin/bash #SBATCH --mail-user=<put your valid email address> #SBATCH --mail-type=end,fail #SBATCH --ntasks=XXXmpinodesXXX #SBATCH --time=XXXextra1XXX #SBATCH --mem-per-cpu=XXXextra2XXX #SBATCH --partition=XXXqueueXXX #SBATCH --error=XXXerrfileXXX #SBATCH --output=XXXoutfileXXX module load relion/1.4 mpiexec XXXcommandXXX ####the end Substitute your own email address! Keywords starting and finishing with \u201cXXX\u201d are recognized by Relion and should not be edited. To select a specific processor family you can edit the -pe option and subsitute \u201corte-sandy\u201d, \u201corte-ivy\u201d or \u201corte-broadwell\u201d for \u201corte\u201d. Now, you can set up tasks that will run on the cluster as follows: Start the Relion GUI Click on the \u201cRunning\u201d tab Add appropriate values for each option: Number of MPI procs: The number of processes the job should use. Number of threads: Currently only 1 thread is supported on Ubelix. Available RAM per thread: Relion jobs can be quite eager but it is impossible to precisely predict how much RAM each process will need. 4 is usually a good place to start. This option here is only indicative and puts no limit on the RAM that Relion can use. Nonetheless to prevent stupid mistakes, you should always enter the same amount of RAM here as in the option \u201cMaximum RAM per process\u201d (see below). Submit to queue: Must be set to yes if the aim is to run on Ubelix queuing system. Queue name: In general set it to \u00ab all \u00bb. If you want to use a specific queue, please refer to https://docs.id.unibe.ch/ubelix/advanced-topics/parallel-jobs Queue submit command: Set it to \u201csbatch\u201d. Maximum CPU time: The maximum allowed running time. See https://docs.id.unibe.ch/ubelix/ubelix-101/the-job-script (Mandatory options) for details on the meaning of this option for Ubelix usage. Maximum RAM process: The maximum allowed RAM per process allowed by Ubelix. If you ask for too much RAM your job is less likely to start fast. If you ask for too little RAM your job will crash. The error output by Relion and Ubelix in such case is not always explicit. Nevertheless, too little RAM is the most common cause of crash. Therefore if you experience an unexpected crash, try increasing the available RAM per thread. See https://docs.id.unibe.ch/ubelix/ubelix-101/the-job-script (Mandatory options) for details on the meaning of this option for Ubelix usage. Note that unlike in \u201cAvailable RAM per thread\u201d option, you must append a \u201cG\u201d to the desired number of Gigabytes (for example, 4G). To prevent stupid mistake, you should always enter the same amount of RAM here as in the option \u201cAvailable RAM per thread\u201d. Standard submission script: Path to the standard submission script described above. Minimum dedicated core per node: Set to 1. Further Information Relion wiki: http://www2.mrc-lmb.cam.ac.uk/relion/index.php/Main_Page Tutorial: http://www2.mrc-lmb.cam.ac.uk/groups/scheres/relion13_tutorial.pdf","title":"Relion"},{"location":"software/relion.html#relion","text":"","title":"Relion"},{"location":"software/relion.html#description","text":"Some useful information on using Relion.","title":"Description"},{"location":"software/relion.html#running-relion","text":"A standard submission script serves as a template for your Relion jobs. Create a file with the following content within your home directory: qsub.sh #!/bin/bash #SBATCH --mail-user=<put your valid email address> #SBATCH --mail-type=end,fail #SBATCH --ntasks=XXXmpinodesXXX #SBATCH --time=XXXextra1XXX #SBATCH --mem-per-cpu=XXXextra2XXX #SBATCH --partition=XXXqueueXXX #SBATCH --error=XXXerrfileXXX #SBATCH --output=XXXoutfileXXX module load relion/1.4 mpiexec XXXcommandXXX ####the end Substitute your own email address! Keywords starting and finishing with \u201cXXX\u201d are recognized by Relion and should not be edited. To select a specific processor family you can edit the -pe option and subsitute \u201corte-sandy\u201d, \u201corte-ivy\u201d or \u201corte-broadwell\u201d for \u201corte\u201d. Now, you can set up tasks that will run on the cluster as follows: Start the Relion GUI Click on the \u201cRunning\u201d tab Add appropriate values for each option: Number of MPI procs: The number of processes the job should use. Number of threads: Currently only 1 thread is supported on Ubelix. Available RAM per thread: Relion jobs can be quite eager but it is impossible to precisely predict how much RAM each process will need. 4 is usually a good place to start. This option here is only indicative and puts no limit on the RAM that Relion can use. Nonetheless to prevent stupid mistakes, you should always enter the same amount of RAM here as in the option \u201cMaximum RAM per process\u201d (see below). Submit to queue: Must be set to yes if the aim is to run on Ubelix queuing system. Queue name: In general set it to \u00ab all \u00bb. If you want to use a specific queue, please refer to https://docs.id.unibe.ch/ubelix/advanced-topics/parallel-jobs Queue submit command: Set it to \u201csbatch\u201d. Maximum CPU time: The maximum allowed running time. See https://docs.id.unibe.ch/ubelix/ubelix-101/the-job-script (Mandatory options) for details on the meaning of this option for Ubelix usage. Maximum RAM process: The maximum allowed RAM per process allowed by Ubelix. If you ask for too much RAM your job is less likely to start fast. If you ask for too little RAM your job will crash. The error output by Relion and Ubelix in such case is not always explicit. Nevertheless, too little RAM is the most common cause of crash. Therefore if you experience an unexpected crash, try increasing the available RAM per thread. See https://docs.id.unibe.ch/ubelix/ubelix-101/the-job-script (Mandatory options) for details on the meaning of this option for Ubelix usage. Note that unlike in \u201cAvailable RAM per thread\u201d option, you must append a \u201cG\u201d to the desired number of Gigabytes (for example, 4G). To prevent stupid mistake, you should always enter the same amount of RAM here as in the option \u201cAvailable RAM per thread\u201d. Standard submission script: Path to the standard submission script described above. Minimum dedicated core per node: Set to 1.","title":"Running Relion"},{"location":"software/relion.html#further-information","text":"Relion wiki: http://www2.mrc-lmb.cam.ac.uk/relion/index.php/Main_Page Tutorial: http://www2.mrc-lmb.cam.ac.uk/groups/scheres/relion13_tutorial.pdf","title":"Further Information"},{"location":"software/singularity.html","text":"Singularity Description Put your scientific workflows, software and libraries in a Singularity container and run it on UBELIX Examples Work interactively Submit an interactive SLURM job and then use the shell command to spawn an interactive shell within the Singularity container: srun --time = 01 :00:00 --mem-per-cpu = 2G --pty bash singularity shell <image> Execute the containers \u201crunscript\u201d #!/bin/bash #SBATCH --partition=all #SBATCH --mem-per-cpu=2G singularity run <image> #or ./<image> Run a command within your container image singularity exec <image> <command> e.g: singularity exec container.img cat /etc/os-release Bind directories Per default the started application (e.g. cat in the last example) runs withing the container. The container works like a seperate machine with own operation system etc. Thus, per default you have no access to files and directories outside the container. This can be changed using binding paths. If files are needed outside the container, e.g. in your HOME you can add the a path to SINGULARITY_BINDPATH=\"src1[:dest1],src2[:dest2] . All subdirectories and files will be accessible. Thus you could bind your HOME directory as: export SINGULARITY_BINDPATH = \" $HOME /: $HOME /\" # or simply export SINGULARITY_BINDPATH = \" $HOME \" Further Information Official Singularity Documentation can be found at https://sylabs.io/docs/","title":"Singularity"},{"location":"software/singularity.html#singularity","text":"","title":"Singularity"},{"location":"software/singularity.html#description","text":"Put your scientific workflows, software and libraries in a Singularity container and run it on UBELIX","title":"Description"},{"location":"software/singularity.html#examples","text":"","title":"Examples"},{"location":"software/singularity.html#work-interactively","text":"Submit an interactive SLURM job and then use the shell command to spawn an interactive shell within the Singularity container: srun --time = 01 :00:00 --mem-per-cpu = 2G --pty bash singularity shell <image>","title":"Work interactively"},{"location":"software/singularity.html#execute-the-containers-runscript","text":"#!/bin/bash #SBATCH --partition=all #SBATCH --mem-per-cpu=2G singularity run <image> #or ./<image>","title":"Execute the containers \"runscript\""},{"location":"software/singularity.html#run-a-command-within-your-container-image","text":"singularity exec <image> <command> e.g: singularity exec container.img cat /etc/os-release","title":"Run a command within your container image"},{"location":"software/singularity.html#bind-directories","text":"Per default the started application (e.g. cat in the last example) runs withing the container. The container works like a seperate machine with own operation system etc. Thus, per default you have no access to files and directories outside the container. This can be changed using binding paths. If files are needed outside the container, e.g. in your HOME you can add the a path to SINGULARITY_BINDPATH=\"src1[:dest1],src2[:dest2] . All subdirectories and files will be accessible. Thus you could bind your HOME directory as: export SINGULARITY_BINDPATH = \" $HOME /: $HOME /\" # or simply export SINGULARITY_BINDPATH = \" $HOME \"","title":"Bind directories"},{"location":"software/singularity.html#further-information","text":"Official Singularity Documentation can be found at https://sylabs.io/docs/","title":"Further Information"},{"location":"software/terminal-multiplexer-tmux.html","text":"Terminal Multiplexer (tmux) Description Frequently, people want to run programs on the submit host independently from an SSH session. Besides allowing a user to access multiple terminal sessions inside a single terminal window, tmux also lets you separate a program from the Unix shell that started the program. Tmux allows you detach from your running tmux session (the session will keep running in the background) and attach to the same session later on. Because the tmux session is running on the remote server, your session persists even on logout. Working Example Start a new tmux session on the submit host: $ tmux new -s first_session This will automatically attach you to a tmux session named first_session. Do your work within your tmux session. Detach from the session: Ctrl-b d Now you cloud disconnect from the server and reconnect later on. List all your existing tmux session: $ tmux ls first_session: 1 windows ( created Wed Jan 14 15 :23:11 2016 ) [ 80x85 ] ``` Bash Reattach to an existing tmux session: ``` Bash $ tumb attach -t first_session Further Information A tmux primer: https://danielmiessler.com/study/tmux","title":"Terminal Multiplexer (tmux)"},{"location":"software/terminal-multiplexer-tmux.html#terminal-multiplexer-tmux","text":"","title":"Terminal Multiplexer (tmux)"},{"location":"software/terminal-multiplexer-tmux.html#description","text":"Frequently, people want to run programs on the submit host independently from an SSH session. Besides allowing a user to access multiple terminal sessions inside a single terminal window, tmux also lets you separate a program from the Unix shell that started the program. Tmux allows you detach from your running tmux session (the session will keep running in the background) and attach to the same session later on. Because the tmux session is running on the remote server, your session persists even on logout.","title":"Description"},{"location":"software/terminal-multiplexer-tmux.html#working-example","text":"Start a new tmux session on the submit host: $ tmux new -s first_session This will automatically attach you to a tmux session named first_session. Do your work within your tmux session. Detach from the session: Ctrl-b d Now you cloud disconnect from the server and reconnect later on. List all your existing tmux session: $ tmux ls first_session: 1 windows ( created Wed Jan 14 15 :23:11 2016 ) [ 80x85 ] ``` Bash Reattach to an existing tmux session: ``` Bash $ tumb attach -t first_session","title":"Working Example"},{"location":"software/terminal-multiplexer-tmux.html#further-information","text":"A tmux primer: https://danielmiessler.com/study/tmux","title":"Further Information"}]}